{"meta":{"title":"流沙","subtitle":"Artificial Intelligence","description":"人工智能、机器学习、深度学习、数据挖掘、深度聚类","author":" liusha","url":"https://jpccc.github.io","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2021-10-26T04:13:28.865Z","updated":"2021-10-23T05:36:56.709Z","comments":false,"path":"/404.html","permalink":"https://jpccc.github.io/404.html","excerpt":"","text":".article-header { padding: 0; padding-top: 26px; border-left: none; text-align: center; } .article-header:hover { border-left: none; } .article-title { font-size: 2.1em; } strong a { color: #747474; } .article-meta { display: none; } .share { display: none; } .ds-meta { display: none; } .player { margin-left: -10px; } .sign { text-align: right; font-style: italic; } #page-visit { display: none; } .center { text-align: center; height: 2.5em; font-weight: bold; } .article-entry hr { margin: 0; } .pic { text-align: center; margin: 0; } .pic br { display: none; } #container .article-info-post.article-info { display: none; } #container .article .article-title { padding: 0; }"},{"title":"about","date":"2021-10-21T07:45:26.000Z","updated":"2021-10-21T07:45:26.601Z","comments":true,"path":"about/index.html","permalink":"https://jpccc.github.io/about/index.html","excerpt":"","text":""},{"title":"tags","date":"2021-10-21T07:42:08.000Z","updated":"2021-10-21T07:44:49.569Z","comments":true,"path":"tags/index.html","permalink":"https://jpccc.github.io/tags/index.html","excerpt":"","text":""},{"title":"resource","date":"2021-10-22T02:51:41.000Z","updated":"2021-10-22T02:51:41.764Z","comments":true,"path":"resource/index.html","permalink":"https://jpccc.github.io/resource/index.html","excerpt":"","text":""}],"posts":[{"title":"Understanding Graphs, Automatic Differentiation and Autograd","slug":"pytorch计算图","date":"2022-04-05T16:08:00.000Z","updated":"2022-04-06T01:32:53.828Z","comments":true,"path":"2022/04/06/pytorch计算图/","link":"","permalink":"https://jpccc.github.io/2022/04/06/pytorch%E8%AE%A1%E7%AE%97%E5%9B%BE/","excerpt":"IntroductionPyTorch is one of the foremost python deep learning libraries out there. It’s the go to choice for deep learning research, and as each days passes by, more and more companies and research labs are adopting this library. In this series of tutorials, we will be introducing you to PyTorch, and how to make the best use of the libraries as well the ecosystem of tools built around it. We’ll first cover the basic building blocks, and then move onto how you can quickly prototype custom architectures. We will finally conclude with a couple of posts on how to scale your code, and how to debug your code if things go awry. This is Part 1 of our PyTorch 101 series.","text":"IntroductionPyTorch is one of the foremost python deep learning libraries out there. It’s the go to choice for deep learning research, and as each days passes by, more and more companies and research labs are adopting this library. In this series of tutorials, we will be introducing you to PyTorch, and how to make the best use of the libraries as well the ecosystem of tools built around it. We’ll first cover the basic building blocks, and then move onto how you can quickly prototype custom architectures. We will finally conclude with a couple of posts on how to scale your code, and how to debug your code if things go awry. This is Part 1 of our PyTorch 101 series. Understanding Graphs, Automatic Differentiation and Autograd Building Your First Neural Network Going Deep with PyTorch Memory Management and Using Multiple GPUs Understanding Hooks You can get all the code in this post, (and other posts as well) in the Github repo here. Prerequisites Chain rule Basic Understanding of Deep Learning PyTorch 1.0 You can get all the code in this post, (and other posts as well) in the Github repo here. Automatic DifferentiationA lot of tutorial series on PyTorch would start begin with a rudimentary discussion of what the basic structures are. However, I’d like to instead start by discussing automatic differentiation first. Automatic Differentiation is a building block of not only PyTorch, but every DL library out there. In my opinion, PyTorch’s automatic differentiation engine, called Autograd is a brilliant tool to understand how automatic differentiation works. This will not only help you understand PyTorch better, but also other DL libraries. Modern neural network architectures can have millions of learnable parameters. From a computational point of view, training a neural network consists of two phases: A forward pass to compute the value of the loss function. A backward pass to compute the gradients of the learnable parameters. The forward pass is pretty straight forward. The output of one layer is the input to the next and so forth. Backward pass is a bit more complicated since it requires us to use the chain rule to compute the gradients of weights w.r.t to the loss function. A toy exampleLet us take an very simple neural network consisting of just 5 neurons. Our neural network looks like the following. The following equations describe our neural network. $$b=w1∗a\\\\c=w2∗a\\\\d=w3∗b+w4∗c\\\\L=10−d\\\\$$ Let us compute the gradients for each of the learnable parameters ww. $$\\frac{\\partial{L}}{\\partial{w_4}} = \\frac{\\partial{L}}{\\partial{d}}\\frac{\\partial{d}}{\\partial{w_4}} \\\\\\frac{\\partial{L}}{\\partial{w_3}} = \\frac{\\partial{L}}{\\partial{d}}\\frac{\\partial{d}}{\\partial{w_3}}\\\\\\frac{\\partial{L}}{\\partial{w_2}} = \\frac{\\partial{L}}{\\partial{d}} * \\frac{\\partial{d}}{\\partial{c}} * \\frac{\\partial{c}}{\\partial{w_2}}\\\\\\frac{\\partial{L}}{\\partial{w_1}} = \\frac{\\partial{L}}{\\partial{d}}* \\frac{\\partial{d}}{\\partial{b}} * \\frac{\\partial{b}}{\\partial{w_1}}\\\\$$ All these gradients have been computed by applying the chain rule. Note that all the individual gradients on the right hand side of the equations mentioned above can be computed directly since the numerators of the gradients are explicit functions of the denominators. Computation GraphsWe could manually compute the gradients of our network as it was very simple. Imagine, what if you had a network with 152 layers. Or, if the network had multiple branches. When we design software to implement neural networks, we want to come up with a way that can allow us to seamlessly compute the gradients, regardless of the architecture type so that the programmer doesn’t have to manually compute gradients when changes are made to the network. We galvanize(激励,启发) this idea in form of a data structure called a Computation graph. A computation graph looks very similar to the diagram of the graph that we made in the image above. However, the nodes in a computation graph are basically operators. These operators are basically the mathematical operators except for one case, where we need to represent creation of a user-defined variable. Notice that we have also denoted(表示…) the leaf variables a,w1,w2,w3,w4 in the graph for sake of clarity. However, it should noted that they are not a part of the computation graph. What they represent in our graph is the special case for user-defined variables which we just covered as an exception. The variables, b,c and d are created as a result of mathematical operations, whereas variables a, w1, w2, w3 and w4 are initialised by the user itself. Since, they are not created by any mathematical operator, nodes corresponding to their creation is represented by their name itself. This is true for all the leaf nodes in the graph. Computing the gradientsNow, we are ready to describe how we will compute gradients using a computation graph. Each node of the computation graph, with the exception of(除了…外) leaf nodes, can be considered as a function which takes some inputs and produces an output. Consider the node of the graph which produces variable d from w4cand w3b. Therefore we can write, $$d=f(w_3b,w_4c)$$ ​ d is output of function f(x,y) = x + y Now, we can easily compute the gradient of the ff with respect to it’s inputs, $\\frac{\\partial{f}}{\\partial{w_3b}}$ and $\\frac{\\partial{f}}{\\partial{w_4b}}$ (which are both 1). Now, label the edges coming into the nodes with their respective gradients like the following image. ​ Local Gradients We do it for the entire graph. The graph looks like this. Back propagation in an Computational Graph Following we describe the algorithm for computing derivative(微分) of any node in this graph with respect to the loss, LL. Let’s say we want to compute the derivative, $\\frac{\\partial{f}}{\\partial{w_4}}$ We first trace down all possible paths from d to $w_4$. There is only one such path. We multiply all the edges long this path. If you see, the product is precisely the same expression we derived using chain rule. If there is more than one path to a variable from L then, we multiply the edges along each path and then add them together. For example,$\\frac{\\partial{L}}{\\partial{a}}$ is computed as $$\\frac{\\partial{L}}{\\partial{w_4}} = \\frac{\\partial{L}}{\\partial{d}}*\\frac{\\partial{d}}{\\partial{b}}*\\frac{\\partial{b}}{\\partial{a}} + \\frac{\\partial{L}}{\\partial{d}}*\\frac{\\partial{d}}{\\partial{c}}*\\frac{\\partial{c}}{\\partial{a}}$$ PyTorch AutogradNow we get what a computational graph is, let’s get back to PyTorch and understand how the above is implemented in PyTorch. (注意：对谁求导，对应的导数就保存在对应变量中) TensorTensor is a data structure which is a fundamental building block of PyTorch. Tensors are pretty much like numpy arrays, except that unlike numpy, tensors are designed to take advantage of parallel computation capabilities of a GPU. A lot of Tensor syntax(语法) is similar to that of numpy arrays. 123456789In [1]: import torchIn [2]: tsr = torch.Tensor(3,5)In [3]: tsrOut[3]: tensor([[ 0.0000e+00, 0.0000e+00, 8.4452e-29, -1.0842e-19, 1.2413e-35], [ 1.4013e-45, 1.2416e-35, 1.4013e-45, 2.3331e-35, 1.4013e-45], [ 1.0108e-36, 1.4013e-45, 8.3641e-37, 1.4013e-45, 1.0040e-36]]) One it’s own, Tensor is just like a numpy ndarray. A data structure that can let you do fast linear algebra options. If you want PyTorch to create a graph corresponding to these operations, you will have to set the requires_grad attribute of the Tensor to True. The API can be a bit confusing here. There are multiple ways to initialise tensors in PyTorch. While some ways can let you explicitly define that the requires_grad in the constructor itself, others require you to set it manually after creation of the Tensor. 1234&gt;&gt; t1 = torch.randn((3,3), requires_grad = True) &gt;&gt; t2 = torch.FloatTensor(3,3) # No way to specify requires_grad while initiating &gt;&gt; t2.requires_grad = True requires_grad is contagious. It means that when a Tensor is created by operating on other Tensors, the requires_grad of the resultant Tensor would be set True given at least one of the tensors used for creation has it’s requires_grad set to True. Each Tensor has a something an attribute called grad_fn, which refers to the mathematical operator that create the variable. If requires_grad is set to False, grad_fn would be None. In our example where, $d=f(w_3b,w_4c)$, d‘s grad function would be the addition operator, since f adds it’s to input together. Notice, addition operator is also the node in our graph that output’s d. If our Tensor is a leaf node (initialised by the user), then the grad_fn is also None. 123456789101112131415161718import torch a = torch.randn((3,3), requires_grad = True)w1 = torch.randn((3,3), requires_grad = True)w2 = torch.randn((3,3), requires_grad = True)w3 = torch.randn((3,3), requires_grad = True)w4 = torch.randn((3,3), requires_grad = True)b = w1*a c = w2*ad = w3*b + w4*c L = 10 - dprint(&quot;The grad fn for a is&quot;, a.grad_fn)print(&quot;The grad fn for d is&quot;, d.grad_fn) If you run the code above, you get the following output. 12The grad fn for a is NoneThe grad fn for d is &lt;AddBackward0 object at 0x1033afe48&gt; One can use the member function is_leaf to determine whether a variable is a leaf Tensor or not. FunctionAll mathematical operations in PyTorch are implemented by the torch.nn.Autograd.Function class. This class has two important member functions we need to look at. The first is it’s forward function, which simply computes the output using it’s inputs. The backward function takes the incoming gradient coming from the the part of the network in front of it. As you can see, the gradient to be backpropagated from a function f is basically the gradient that is backpropagated to f from the layers in front of it multiplied by the local gradient of the output of f with respect to it’s inputs（链式规则）. This is exactly what the backward function does. Let’s again understand with our example of$$d=f(w_3b,w_4c)$$ d is our Tensor here. It’s grad_fn is &lt;ThAddBackward&gt;. This is basically the addition operation since the function that creates d adds inputs. The forward function of the it’s grad_fn receives the inputs $w_3b$ and $w_4c$ and adds them. This value is basically stored in the d. The backward function of the &lt;ThAddBackward&gt; basically takes the the incoming gradient from the further layers as the input. This is basically $\\frac{\\partial{L}}{\\partial{d}}$ coming along the edge leading from L to d. This gradient is also the gradient of L w.r.t to d and is stored in grad attribute of the d. It can be accessed by calling d.grad. It then takes computes the local gradients $\\frac{\\partial{d}}{\\partial{w_4c}}$and$\\frac{\\partial{d}}{\\partial{w_3b}}$. Then the backward function multiplies the incoming gradient with the locally computed gradients respectively and “sends“ the gradients to it’s inputs by invoking the backward method of the grad_fn of their inputs. For example, the backward function of &lt;ThAddBackward&gt; associated with d invokes(援引，调用) backward function of the grad_fn of the $w_4∗c$∗(Here, $w_4∗c$ is a intermediate Tensor, and it’s grad_fn is &lt;ThMulBackward&gt;. At time of invocation of the backward function, the gradient $\\frac{\\partial{L}}{\\partial{d}}*\\frac{\\partial{d}}{\\partial{w_4c}}$ is passed as the input. Now, for the variable $w_4∗c$, $\\frac{\\partial{L}}{\\partial{d}}*\\frac{\\partial{d}}{\\partial{w_4c}}$ becomes the incoming gradient, $\\frac{\\partial{L}}{\\partial{d}}$ was for $d$ in step 3 and the process repeats. Algorithmically, here’s how back propagation happens with a computation graph. (Not the actual implementation, only representative) 1234567891011def backward (incoming_gradients): self.Tensor.grad = incoming_gradients for inp in self.inputs: if inp.grad_fn is not None: new_incoming_gradients = // incoming_gradient * local_grad(self.Tensor, inp) inp.grad_fn.backward(new_incoming_gradients) else: pass Here, self.Tensor is basically the Tensor created by Autograd.Function, which was d in our example. Incoming gradients and local gradients have been described above. In order to compute derivatives in our neural network, we generally call backward on the Tensor representing our loss. Then, we backtrack through the graph starting from node representing the grad_fn of our loss. As described above, the backward function is recursively called through out the graph as we backtrack. Once, we reach a leaf node, since the grad_fn is None, but stop backtracking through that path. One thing to note here is that PyTorch gives an error if you call backward() on vector-valued Tensor. This means you can only call backward on a scalar valued Tensor. In our example, if we assume a to be a vector valued Tensor, and call backward on L, it will throw up an error. 1234567891011121314151617import torch a = torch.randn((3,3), requires_grad = True)w1 = torch.randn((3,3), requires_grad = True)w2 = torch.randn((3,3), requires_grad = True)w3 = torch.randn((3,3), requires_grad = True)w4 = torch.randn((3,3), requires_grad = True)b = w1*a c = w2*ad = w3*b + w4*c L = (10 - d)L.backward() Running the above snippet results in the following error. 1RuntimeError: grad can be implicitly created only for scalar outputs This is because gradients can be computed with respect to scalar values by definition. You can’t exactly differentiate a vector with respect to another vector. The mathematical entity used for such cases is called a Jacobian, the discussion of which is beyond the scope of this article. There are two ways to overcome this. If you just make a small change in the above code setting L to be the sum of all the errors, our problem will be solved. 123456789101112131415161718import torch a = torch.randn((3,3), requires_grad = True)w1 = torch.randn((3,3), requires_grad = True)w2 = torch.randn((3,3), requires_grad = True)w3 = torch.randn((3,3), requires_grad = True)w4 = torch.randn((3,3), requires_grad = True)b = w1*a c = w2*ad = w3*b + w4*c # Replace L = (10 - d) by L = (10 -d).sum()L.backward() Once that’s done, you can access the gradients by calling the grad attribute of Tensor. Second way is, for some reason have to absolutely call backward on a vector function, you can pass a torch.ones of size of shape of the tensor you are trying to call backward with. 12# Replace L.backward() with L.backward(torch.ones(L.shape)) Notice how backward used to take incoming gradients as it’s input. Doing the above makes the backward think that incoming gradient are just Tensor of ones of same size as L, and it’s able to back propagate. In this way, we can have gradients for every Tensor , and we can update them using Optimisation algorithm of our choice. 1w1 = w1 - learning_rate * w1.grad And so on. How are PyTorch’s graphs different from TensorFlow graphsPyTorch creates something called a Dynamic Computation Graph, which means that the graph is generated on the fly. Until the forward function of a Variable is called, there exists no node for the Tensor (it’s grad_fn) in the graph. 12345a = torch.randn((3,3), requires_grad = True) #No graph yet, as a is a leafw1 = torch.randn((3,3), requires_grad = True) #Same logic as aboveb = w1*a #Graph with node `mulBackward` is created. The graph is created as a result of forward function of many Tensors being invoked. Only then, the buffers for the non-leaf nodes allocated for the graph and intermediate values (used for computing gradients later. When you call backward, as the gradients are computed, these buffers (for non-leaf variables) are essentially freed, and the graph is destroyed ( In a sense, you can’t backpropagate through it since the buffers holding values to compute the gradients are gone). Next time, you will call forward on the same set of tensors, the leaf node buffers from the previous run will be shared, while the non-leaf nodes buffers will be created again. If you call backward more than once on a graph with non-leaf nodes, you’ll be met with the following error. 1RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time. This is because the non-leaf buffers gets destroyed the first time backward() is called and hence, there’s no path to navigate to the leaves when backward is invoked the second time. You can undo this non-leaf buffer destroying behaviour by adding retain_graph = True argument to the backward function. 1loss.backward(retain_graph = True) If you do the above, you will be able to backpropagate again through the same graph and the gradients will be accumulated, i.e. the next you backpropagate, the gradients will be added to those already stored in the previous back pass. This is in contrast to the Static Computation Graphs, used by TensorFlow where the graph is declared before running the program. Then the graph is “run” by feeding values to the predefined graph. The dynamic graph paradigm allows you to make changes to your network architecture during runtime, as a graph is created only when a piece of code is run. This means a graph may be redefined during the lifetime for a program since you don’t have to define it beforehand. This, however, is not possible with static graphs where graphs are created before running the program, and merely executed later. Dynamic graphs also make debugging way easier since it’s easier to locate the source of your error. Some Tricks of Traderequires_gradThis is an attribute of the Tensor class. By default, it’s False. It comes handy when you have to freeze some layers, and stop them from updating parameters while training. You can simply set the requires_grad to False, and these Tensors won’t participate in the computation graph. Thus, no gradient would be propagated to them, or to those layers which depend upon these layers for gradient flow requires_grad. When set to True, requires_grad is contagious meaning even if one operand of an operation has requires_grad set to True, so will the result. torch.no_grad()When we are computing gradients, we need to cache input values, and intermediate features as they maybe required to compute the gradient later. The gradient of $ b=w_1∗a$ w.r.t it’s inputs w1w1 and aa is aa and w1w1 respectively. We need to store these values for gradient computation during the backward pass. This affects the memory footprint of the network. While, we are performing inference, we don’t compute gradients, and thus, don’t need to store these values. Infact, no graph needs to be create during inference as it will lead to useless consumption of memory. PyTorch offers a context manager, called torch.no_grad for this purpose. 12with torch.no_grad: inference code goes here No graph is defined for operations executed under this context manager. ConclusionUnderstanding how Autograd and computation graphs works can make life with PyTorch a whole lot easier. With our foundations rock solid, the next posts will detail how to create custom complex architectures, how to create custom data pipelines and more interesting stuff. Further Reading Chain Rule Backpropagation","categories":[],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"https://jpccc.github.io/tags/pytorch/"}]},{"title":"Jensen_inequality","slug":"Jensen-inequality","date":"2021-10-28T08:10:44.000Z","updated":"2022-04-05T16:03:21.412Z","comments":true,"path":"2021/10/28/Jensen-inequality/","link":"","permalink":"https://jpccc.github.io/2021/10/28/Jensen-inequality/","excerpt":"Jensen不等式Jensen不等式（Jensen’s inequality）是以丹麦数学家Johan Jensen命名的，它在概率论、机器学习、测度论、统计物理等领域都有相关应用。 在机器学习领域，我目前接触到的是用Jensen不等式用来证明KL散度大于等于0。 Jensen不等式是和凸函数的定义是息息相关的，首先介绍什么是凸函数(convec function)。","text":"Jensen不等式Jensen不等式（Jensen’s inequality）是以丹麦数学家Johan Jensen命名的，它在概率论、机器学习、测度论、统计物理等领域都有相关应用。 在机器学习领域，我目前接触到的是用Jensen不等式用来证明KL散度大于等于0。 Jensen不等式是和凸函数的定义是息息相关的，首先介绍什么是凸函数(convec function)。 凸函数凸函数是一个定义在某个向量空间的凸子集 C（区间）上的实值函数 f，如果在其定义域 C 上的任意两点 $x_1$,$x_2$ 有: $$tf(x_1)+(1-t)f(x_2)\\geq f(tx_1+(1-t)x_2) \\tag{1}$$ 也就是说凸函数任意两点的割线位于函数图形上方， 这也是Jensen不等式的两点形式。 Jensen不等式2134123若对于任意点集${x_i}$，若 $\\lambda_i\\geq 0$ 且 $\\underset {i}\\sum\\lambda_i=1$ ，使用数学归纳法，可以证明凸函数 f (x) 满足：$$f(\\sum_{i=1}^M\\lambda_ix_i)\\leq \\sum_{i=1}^M\\lambda_if(x_i) \\tag{2} $$ 公式(2)被称为 Jensen 不等式，它是公式(1)的泛化形式。 证明如下： 当i=1或2时，由凸函数的定义成立 假设当i=M时，公式(2)成立 现在证明则i=M+1时，Jensen不等式也成立：证明 在概率论中，如果把$\\lambda_i$看成取值为$x_i$的离散变量x的概率分布，那么公式(2)就可以写成： $$f(E(X))\\leq E[f(x)]$$ 其中, E[·] 表示期望。 对于连续变量，Jensen不等式给出了积分的凸函数值和凸函数的积分值间的关系。","categories":[],"tags":[{"name":"math","slug":"math","permalink":"https://jpccc.github.io/tags/math/"}]},{"title":"统计学习数学基础-1","slug":"统计学习part1","date":"2021-10-28T04:44:13.000Z","updated":"2022-04-06T01:40:36.657Z","comments":true,"path":"2021/10/28/统计学习part1/","link":"","permalink":"https://jpccc.github.io/2021/10/28/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0part1/","excerpt":"Introduction对概率的诠释有两大学派，一种是频率派另一种是贝叶斯派。后面我们对观测集采用下面记号：$$X_{N\\times p}=(x_{1},x_{2},\\cdots,x_{N})^{T},x_{i}=(x_{i1},x_{i2},\\cdots,x_{ip})^{T}$$这个记号表示有 $N$ 个样本，每个样本都是 $p$ 维向量。其中每个观测都是由 $p(x|\\theta)$ 生成的。","text":"Introduction对概率的诠释有两大学派，一种是频率派另一种是贝叶斯派。后面我们对观测集采用下面记号：$$X_{N\\times p}=(x_{1},x_{2},\\cdots,x_{N})^{T},x_{i}=(x_{i1},x_{i2},\\cdots,x_{ip})^{T}$$这个记号表示有 $N$ 个样本，每个样本都是 $p$ 维向量。其中每个观测都是由 $p(x|\\theta)$ 生成的。 频率派的观点$p(x|\\theta)$中的 $\\theta$ 是一个未知常量，数据是随机变量。对于 $N$ 个观测来说观测集的概率为$p(X|\\theta)\\mathop{=}\\limits _{iid}\\prod\\limits {i=1}^{N}p(x{i}|\\theta))$.为了求 $\\theta$ 的大小，我们采用最大对数似然MLE的方法： $$\\theta_{MLE}=\\mathop{argmax}\\limits_{\\theta}\\log p(X|\\theta)\\mathop{=}\\limits_{iid}\\mathop{argmax}\\limits_{\\theta}\\sum\\limits {i=1}^{N}\\log p(x{i}|\\theta)$$ $x_i$服从独立同分布的条件，所以$P(X|\\theta)=\\prod_{i=0}^n p(x_i|\\theta)$,为了方便计算，在前面加上log,将连乘变成连加。 贝叶斯派的观点贝叶斯派认为 $p(x|\\theta)$ 中的 $\\theta$ 不是一个常量。这个 $\\theta$ 满足一个预设的先验的分布 $\\theta\\sim p(\\theta)$ （比喻可以假设为高斯分布）,并借助贝叶斯定理，用似然将参数的先验和后验连接起来，于是根据贝叶斯定理依赖观测集参数的后验可以写成： $$p(\\theta|X)=\\frac{p(X|\\theta)\\cdot p(\\theta)}{p(X)}=\\frac{p(X|\\theta)\\cdot p(\\theta)}{\\int\\limits _{\\theta}p(X|\\theta)\\cdot p(\\theta)d\\theta}$$为了求 $\\theta$ 的值，我们要最大化这个参数后验MAP： 最大化后验的解释：在给定观测X的情况下，找出$\\theta$的概率最大时所对应的值，即这时候的$\\theta$更可能为我们要找的参数。 $$\\theta_{MAP}=\\mathop{argmax}\\limits _{\\theta}p(\\theta|X)=\\mathop{argmax}\\limits _{\\theta}p(X|\\theta)\\cdot p(\\theta)$$其中第二个等号是由于分母和 $\\theta$ 没有关系。求解这个 $\\theta$ 值后计算$\\frac{p(X|\\theta)\\cdot p(\\theta)}{\\int\\limits _{\\theta}p(X|\\theta)\\cdot p(\\theta)d\\theta}$ ，就得到了参数的后验概率。其中 $p(X|\\theta)$ 叫似然，是我们的模型分布。得到了参数的后验分布后，我们可以将这个分布用于贝叶斯预测： $$p(x_{new}|X)=\\int\\limits {\\theta}p(x{new},\\theta|X)=\\int\\limits {\\theta}p(x{new}|\\theta)\\cdot p(\\theta|X)d\\theta$$ 其中积分中的被乘数是模型，乘数是后验分布。 $p(x|\\theta)$是似然，$p(\\theta|x)$是后验。注意第一个等式中，$\\theta$不论放分子还是分母都可以这样积分掉。 小结频率派和贝叶斯派分别给出了一系列的机器学习算法。频率派的观点导出了一系列的统计机器学习算法而贝叶斯派导出了概率图理论。在应用频率派的 MLE 方法时最优化理论占有重要地位。而贝叶斯派的算法无论是后验概率的建模还是应用这个后验进行推断时积分占有重要地位。因此采样积分方法如 MCMC 有很多应用。(即频率派需要设计损失函数并进行优化，而贝叶斯派需要积分后验中分母。) 附录 频率派认为参数是客观存在不会改变的，虽然未知，但却是固定值（故可用最优化方法去找那一个唯一的值）；贝叶斯派则认为参数是随机值，因为不可能做完整的实验去确定，因此参数也可以有分布。往小处说，频率派最常关心的是似然函数，他们认为直接用样本去计算出的概率就是真实的，而贝叶斯派最常关心的是后验分布，他们认为样本只是用来修正经验观点。 贝叶斯派因为所有的参数都是随机变量，都有分布，因此可以使用一些基于采样的方法 （如MCMC）使得我们更容易构建复杂模型。频率派的优点则是没有假设一个先验分布，因此更加客观，也更加无偏，在一些保守的领域（比如制药业、法律）比贝叶斯方法更受到信任。 MathBasics高斯分布一维情况 MLE高斯分布在机器学习中占有举足轻重的作用。在 MLE 方法中： $$\\theta=(\\mu,\\Sigma)=(\\mu,\\sigma^{2}),\\theta_{MLE}=\\mathop{argmax}\\limits _{\\theta}\\log p(X|\\theta)\\mathop{=}\\limits _{iid}\\mathop{argmax}\\limits _{\\theta}\\sum\\limits {i=1}^{N}\\log p(x{i}|\\theta)$$ 一般地，高斯分布的概率密度函数PDF写为： $$p(x|\\mu,\\Sigma)=\\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}}e^{-\\frac{1}{2}(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)}$$ 带入 MLE 中我们考虑一维的情况 $$\\log p(X|\\theta)=\\sum\\limits {i=1}^{N}\\log p(x{i}|\\theta)=\\sum\\limits {i=1}^{N}\\log\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp(-(x{i}-\\mu)^{2}/2\\sigma^{2})$$ 首先对 $\\mu$ 的极值可以得到 ：$$\\mu_{MLE}=\\mathop{argmax}\\limits _{\\mu}\\log p(X|\\theta)=\\mathop{argmax}\\limits _{\\mu}\\sum\\limits {i=1}^{N}(x{i}-\\mu)^{2}$$ 于是：$$\\frac{\\partial}{\\partial\\mu}\\sum\\limits {i=1}^{N}(x{i}-\\mu)^{2}=0\\longrightarrow\\mu_{MLE}=\\frac{1}{N}\\sum\\limits {i=1}^{N}x{i}$$ 其次对 $\\theta$ 中的另一个参数 $\\sigma$ ，有： $$\\begin{align}\\sigma_{MLE}=\\mathop{argmax}\\limits _{\\sigma}\\log p(X|\\theta)&amp;=\\mathop{argmax}\\limits _{\\sigma}\\sum\\limits {i=1}^{N}[-\\log\\sigma-\\frac{1}{2\\sigma^{2}}(x{i}-\\mu)^{2}]\\&amp;=\\mathop{argmin}\\limits _{\\sigma}\\sum\\limits {i=1}^{N}[\\log\\sigma+\\frac{1}{2\\sigma^{2}}(x{i}-\\mu)^{2}]\\end{align}$$ 于是： $$\\frac{\\partial}{\\partial\\sigma}\\sum\\limits {i=1}^{N}[\\log\\sigma+\\frac{1}{2\\sigma^{2}}(x{i}-\\mu)^{2}]=0\\longrightarrow\\sigma_{MLE}^{2}=\\frac{1}{N}\\sum\\limits {i=1}^{N}(x{i}-\\mu)^{2}$$ 值得注意的是，上面的推导中，首先对 $\\mu$ 求 MLE， 然后利用这个结果求 $\\sigma_{MLE}$ ，因此可以预期的是对数据集求期望时 $\\mathbb{E}{\\mathcal{D}}[\\mu{MLE}]$ 是无偏差的： $$\\mathbb{E}{\\mathcal{D}}[\\mu{MLE}]=\\mathbb{E}{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits {i=1}^{N}x{i}]=\\frac{1}{N}\\sum\\limits {i=1}^{N}\\mathbb{E}{\\mathcal{D}}[x{i}]=\\mu$$ 但是当对 $\\sigma_{MLE}$ 求 期望的时候由于使用了单个数据集的 $\\mu_{MLE}$，因此对所有数据集求期望的时候我们会发现 $\\sigma_{MLE}$ 是 有偏的： $$\\begin{align}\\mathbb{E}{\\mathcal{D}}[\\sigma{MLE}^{2}]&amp;=\\mathbb{E}{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits {i=1}^{N}(x{i}-\\mu{MLE})^{2}]=\\mathbb{E}{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits {i=1}^{N}(x{i}^{2}-2x{i}\\mu_{MLE}+\\mu_{MLE}^{2})\\&amp;=\\mathbb{E}{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits {i=1}^{N}x{i}^{2}-\\mu{MLE}^{2}]=\\mathbb{E}{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits {i=1}^{N}x{i}^{2}-\\mu^{2}+\\mu^{2}-\\mu{MLE}^{2}]\\&amp;= \\mathbb{E}{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits {i=1}^{N}x{i}^{2}-\\mu^{2}]-\\mathbb{E}{\\mathcal{D}}[\\mu_{MLE}^{2}-\\mu^{2}]=\\sigma^{2}-(\\mathbb{E}{\\mathcal{D}}[\\mu{MLE}^{2}]-\\mu^{2})\\&amp;=\\sigma^{2}-(\\mathbb{E}{\\mathcal{D}}[\\mu{MLE}^{2}]-\\mathbb{E}{\\mathcal{D}}^{2}[\\mu{MLE}])=\\sigma^{2}-Var[\\mu_{MLE}]\\&amp;=\\sigma^{2}-Var[\\frac{1}{N}\\sum\\limits {i=1}^{N}x{i}]=\\sigma^{2}-\\frac{1}{N^{2}}\\sum\\limits {i=1}^{N}Var[x{i}]=\\frac{N-1}{N}\\sigma^{2}\\end{align}$$ 所以：${\\sigma}^{2}$的无偏估计应该为： $$\\hat{\\sigma}^{2}=\\frac{1}{N-1}\\sum\\limits {i=1}^{N}(x{i}-\\mu)^{2}$$ 多维情况多维高斯分布表达式为： $$p(x|\\mu,\\Sigma)=\\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}}e^{-\\frac{1}{2}(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)}$$ 其中 $x,\\mu\\in\\mathbb{R}^{p},\\Sigma\\in\\mathbb{R}^{p\\times p}$ ，$\\Sigma$ 为协方差矩阵，一般而言也是半正定矩阵。这里我们只考虑正定矩阵。首先我们处理指数上的数字，指数上的数字可以记为 $x$ 和 $\\mu$ 之间的马氏距离。对于对称的协方差矩阵可进行特征值分解，$\\Sigma=U\\Lambda U^{T}=(u_{1},u_{2},\\cdots,u_{p})diag(\\lambda_{i})(u_{1},u_{2},\\cdots,u_{p})^{T}=\\sum\\limits {i=1}^{p}u{i}\\lambda_{i}u_{i}^{T}$ ，于是： $$\\Sigma^{-1}=\\sum\\limits {i=1}^{p}u{i}\\frac{1}{\\lambda_{i}}u_{i}^{T}$$ $$\\Delta=(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)=\\sum\\limits {i=1}^{p}(x-\\mu)^{T}u{i}\\frac{1}{\\lambda_{i}}u_{i}^{T}(x-\\mu)=\\sum\\limits {i=1}^{p}\\frac{y{i}^{2}}{\\lambda_{i}}$$ 我们注意到 $y_{i}$ 是 $x-\\mu$ 在特征向量 $u_{i}$ 上的投影长度，因此上式子就是 $\\Delta$ 取不同值时的同心椭圆。 下面我们看多维高斯模型在实际应用时的两个问题 参数 $\\Sigma,\\mu$ 的自由度为 $O(p^{2})$ 对于维度很高的数据其自由度太高。解决方案：高自由度的来源是 $\\Sigma$ 有 $\\frac{p(p+1)}{2}$ 个自由参数，可以假设其是对角矩阵，甚至在各向同性假设中假设其对角线上的元素都相同。前一种的算法有 Factor Analysis，后一种有概率 PCA(p-PCA) 。 第二个问题是单个高斯分布是单峰的，对有多个峰的数据分布不能得到好的结果。解决方案：高斯混合GMM 模型。 下面对多维高斯分布的常用定理进行介绍。 我们记 $x=(x_1, x_2,\\cdots,x_p)^T=(x_{a,m\\times 1}, x_{b,n\\times1})^T,\\mu=(\\mu_{a,m\\times1}, \\mu_{b,n\\times1}),\\Sigma=\\begin{pmatrix}\\Sigma_{aa}&amp;\\Sigma_{ab}\\\\Sigma_{ba}&amp;\\Sigma_{bb}\\end{pmatrix}$，已知 $x\\sim\\mathcal{N}(\\mu,\\Sigma)$。 首先是一个高斯分布的定理： 定理：已知 $x\\sim\\mathcal{N}(\\mu,\\Sigma), y\\sim Ax+b$，那么 $y\\sim\\mathcal{N}(A\\mu+b, A\\Sigma A^T)$。 证明：$\\mathbb{E}[y]=\\mathbb{E}[Ax+b]=A\\mathbb{E}[x]+b=A\\mu+b$，$Var[y]=Var[Ax+b]=Var[Ax]=A\\cdot Var[x]\\cdot A^T$。 下面利用这个定理得到 $p(x_a),p(x_b),p(x_a|x_b),p(x_b|x_a)$ 这四个量。 $x_a=\\begin{pmatrix}\\mathbb{I}{m\\times m}&amp;\\mathbb{O}{m\\times n})\\end{pmatrix}\\begin{pmatrix}x_a\\x_b\\end{pmatrix}$，代入定理中得到： $$ \\mathbb{E}[x_a]=\\begin{pmatrix}\\mathbb{I}&amp;\\mathbb{O}\\end{pmatrix}\\begin{pmatrix}\\mu_a\\\\mu_b\\end{pmatrix}=\\mu_a\\ Var[x_a]=\\begin{pmatrix}\\mathbb{I}&amp;\\mathbb{O}\\end{pmatrix}\\begin{pmatrix}\\Sigma_{aa}&amp;\\Sigma_{ab}\\\\Sigma_{ba}&amp;\\Sigma_{bb}\\end{pmatrix}\\begin{pmatrix}\\mathbb{I}\\\\mathbb{O}\\end{pmatrix}=\\Sigma_{aa} $$ 所以 $x_a\\sim\\mathcal{N}(\\mu_a,\\Sigma_{aa})$。 同样的，$x_b\\sim\\mathcal{N}(\\mu_b,\\Sigma_{bb})$。 对于两个条件概率，我们引入三个量： $$ x_{b\\cdot a}=x_b-\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a\\ \\mu_{b\\cdot a}=\\mu_b-\\Sigma_{ba}\\Sigma_{aa}^{-1}\\mu_a\\ \\Sigma_{bb\\cdot a}=\\Sigma_{bb}-\\Sigma_{ba}\\Sigma_{aa}^{-1}\\Sigma_{ab} $$ 特别的，最后一个式子叫做 $\\Sigma_{bb}$ 的 Schur Complementary。可以看到： $$ x_{b\\cdot a}=\\begin{pmatrix}-\\Sigma_{ba}\\Sigma_{aa}^{-1}&amp;\\mathbb{I}_{n\\times n}\\end{pmatrix}\\begin{pmatrix}x_a\\x_b\\end{pmatrix} $$ 所以： $$ \\mathbb{E}[x_{b\\cdot a}]=\\begin{pmatrix}-\\Sigma_{ba}\\Sigma_{aa}^{-1}&amp;\\mathbb{I}{n\\times n}\\end{pmatrix}\\begin{pmatrix}\\mu_a\\\\mu_b\\end{pmatrix}=\\mu{b\\cdot a}\\ Var[x_{b\\cdot a}]=\\begin{pmatrix}-\\Sigma_{ba}\\Sigma_{aa}^{-1}&amp;\\mathbb{I}{n\\times n}\\end{pmatrix}\\begin{pmatrix}\\Sigma{aa}&amp;\\Sigma_{ab}\\\\Sigma_{ba}&amp;\\Sigma_{bb}\\end{pmatrix}\\begin{pmatrix}-\\Sigma_{aa}^{-1}\\Sigma_{ba}^T\\\\mathbb{I}{n\\times n}\\end{pmatrix}=\\Sigma{bb\\cdot a} $$ 利用这三个量可以得到 $x_b=x_{b\\cdot a}+\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a$。因此： $$ \\mathbb{E}[x_b|x_a]=\\mu_{b\\cdot a}+\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a $$ $$ Var[x_b|x_a]=\\Sigma_{bb\\cdot a} $$ 这里同样用到了定理。 同样： $$ x_{a\\cdot b}=x_a-\\Sigma_{ab}\\Sigma_{bb}^{-1}x_b\\ \\mu_{a\\cdot b}=\\mu_a-\\Sigma_{ab}\\Sigma_{bb}^{-1}\\mu_b\\ \\Sigma_{aa\\cdot b}=\\Sigma_{aa}-\\Sigma_{ab}\\Sigma_{bb}^{-1}\\Sigma_{ba} $$ 所以： $$ \\mathbb{E}[x_a|x_b]=\\mu_{a\\cdot b}+\\Sigma_{ab}\\Sigma_{bb}^{-1}x_b $$ $$ Var[x_a|x_b]=\\Sigma_{aa\\cdot b} $$ 下面利用上边四个量，求解线性模型： 已知：$p(x)=\\mathcal{N}(\\mu,\\Lambda^{-1}),p(y|x)=\\mathcal{N}(Ax+b,L^{-1})$，求解：$p(y),p(x|y)$。 解：令 $y=Ax+b+\\epsilon,\\epsilon\\sim\\mathcal{N}(0,L^{-1})$，所以 $\\mathbb{E}[y]=\\mathbb{E}[Ax+b+\\epsilon]=A\\mu+b$，$Var[y]=A \\Lambda^{-1}A^T+L^{-1}$，因此： $$ p(y)=\\mathcal{N}(A\\mu+b,L^{-1}+A\\Lambda^{-1}A^T) $$ 引入 $z=\\begin{pmatrix}x\\y\\end{pmatrix}$，我们可以得到 $Cov[x,y]=\\mathbb{E}[(x-\\mathbb{E}[x])(y-\\mathbb{E}[y])^T]$。对于这个协方差可以直接计算： $$ \\begin{align} Cov(x,y)&amp;=\\mathbb{E}[(x-\\mu)(Ax-A\\mu+\\epsilon)^T]=\\mathbb{E}[(x-\\mu)(x-\\mu)^TA^T]=Var[x]A^T=\\Lambda^{-1}A^T \\end{align} $$ 注意到协方差矩阵的对称性，所以 $p(z)=\\mathcal{N}\\begin{pmatrix}\\mu\\A\\mu+b\\end{pmatrix},\\begin{pmatrix}\\Lambda^{-1}&amp;\\Lambda^{-1}A^T\\A\\Lambda^{-1}&amp;L^{-1}+A\\Lambda^{-1}A^T\\end{pmatrix})$。根据之前的公式，我们可以得到： $$ \\mathbb{E}[x|y]=\\mu+\\Lambda^{-1}A^T(L^{-1}+A\\Lambda^{-1}A^T)^{-1}(y-A\\mu-b) $$ $$ Var[x|y]=\\Lambda^{-1}-\\Lambda^{-1}A^T(L^{-1}+A\\Lambda^{-1}A^T)^{-1}A\\Lambda^{-1} $$","categories":[],"tags":[{"name":"统计学习","slug":"统计学习","permalink":"https://jpccc.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/"}]},{"title":"pytorch","slug":"pytorch","date":"2021-10-25T14:33:05.000Z","updated":"2021-10-25T14:33:05.364Z","comments":true,"path":"2021/10/25/pytorch/","link":"","permalink":"https://jpccc.github.io/2021/10/25/pytorch/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"DroupOut","slug":"DroupOut","date":"2021-10-25T12:45:13.000Z","updated":"2021-10-25T12:45:13.205Z","comments":true,"path":"2021/10/25/DroupOut/","link":"","permalink":"https://jpccc.github.io/2021/10/25/DroupOut/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"BN算法","slug":"BN算法","date":"2021-10-25T12:20:14.000Z","updated":"2021-10-25T14:32:34.264Z","comments":true,"path":"2021/10/25/BN算法/","link":"","permalink":"https://jpccc.github.io/2021/10/25/BN%E7%AE%97%E6%B3%95/","excerpt":"BN算法概要Batch Normalization是2015年一篇论文中提出的数据归一化方法，往往用在深度神经网络中激活层之前。其作用可以加快模型训练时的收敛速度，使得模型训练过程更加稳定，避免梯度爆炸或者梯度消失。并且起到一定的正则化作用，几乎代替了Dropout。","text":"BN算法概要Batch Normalization是2015年一篇论文中提出的数据归一化方法，往往用在深度神经网络中激活层之前。其作用可以加快模型训练时的收敛速度，使得模型训练过程更加稳定，避免梯度爆炸或者梯度消失。并且起到一定的正则化作用，几乎代替了Dropout。 BN算法产生的背景 &ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp; 首先对第一张图进行分析。 由于我们通常使用采用零均值化对网络进行参数初始化，我们初始的拟合直线也就是红色部分。另外的一条绿色直线，是我们的目标直线。从图能够直观看出，我们应该需要多次迭代才能得到我们的需要的目标直线。 我们再看第二张图 假设我们还是和第一张图有相同的分布，只是我们做了减均值，让数据均值为零。能够直观的发现可能只进行简单的微调就能够实现拟合（理想）。大大提高了我们的训练速度。因此，在训练开始前，对数据进行零均值是一个必要的操作。 但是，随着网络层次加深参数对分布的影响不定(什么意思?)，导致网络每层间以及不同迭代轮次的相同层的输入分布发生改变，导致网络需要重新适应新的分布，迫使我们降低学习率降低影响。在这个背景下BN算法开始出现。 有些人首先提出在每层增加PCA白化(先对数据进行去相关然后再进行归一化)，这样基本满足了数据的0均值、单位方差、弱相关性。但是这样是不可取的，因为在白化过程中会计算协方差矩阵、求逆等操作，计算量会很大，另外，在反向传播时，白化的操作不一定可微。因此，在此背景下BN算法开始出现。 BN算法的实现和优点上面提到了PCA白化优点，能够去相关和数据均值，标准值归一化等优点。但是当数据量比较大的情况下去相关的话需要大量的计算，因此有些人提出了只对数据进行均值和标准差归一化。叫做近似白化预处理。 $$\\hat{x}^k=\\frac{X^k-E(X^k)}{\\sqrt{Var[(x^k)}]}$$ 由于训练过程采用了batch随机梯度下降，因此$E(X^k)$指的是一批训练数据时，各神经元输入值的平均值；$\\sqrt{Var[(x^k)}]$指的是一批训练数据时各神经元输入值的标准差。 但是，这些应用到深度学习网络还远远不够，因为可能由于这种的强制转化导致数据的分布发生破坏。因此需要对公式的鲁棒性进行优化，就有人提出了变换重构的概念。就是在基础公式的基础之上加上了两个参数γ、β。这样在训练过程中就可以学习这两个参数，采用适合自己网络的BN公式。公式如下：$$y^k=\\gamma^k\\hat x^k+\\beta^k$$每一个神经元都会有一对这样的参数γ、β。这样其实当$$\\beta^k=E[x^k],\\gamma^k=\\sqrt{var[x^k]}$$时，是可以恢复出原始的某一层所学到的特征的。引入可学习重构参数γ、β，让网络可以学习恢复出原始网络所要学习的特征分布。 总结上面我们会得到BN的向前传导公式： $\\mu_\\beta\\leftarrow\\frac{1}{m}\\sum_{i=1}^nx_i$ //mnni batch mean$\\delta_\\beta^2\\leftarrow\\frac{1}{m}\\sum_{i=1}^n(x_i-\\mu_\\beta)^2$//mnni batch variance$\\hat{x}\\leftarrow\\frac{x_i-\\mu_\\beta}{\\sqrt{\\delta_\\beta^2+\\epsilon}}$//normalize$y_i\\leftarrow\\gamma\\hat{x_i}+\\beta\\equiv BN_{\\gamma,\\beta}(x_i)$ //scale and shift 2. BN算法在网络中的作用 BN算法像卷积层，池化层、激活层一样也输入一层。BN层添加在激活函数前，对输入激活函数的输入进行归一化。这样解决了输入数据发生偏移和增大的影响。 优点： 1、加快训练速度，能够增大学习率，即使小的学习率也能够有快速的学习速率;2、不用理会拟合中的droupout、L2 正则化项的参数选择，采用BN算法可以省去这两项或者只需要小的L2正则化约束。原因，BN算法后，参数进行了归一化，原本经过激活函数没有太大影响的神经元，分布变得明显，经过一个激活函数以后，神经元会自动削弱或者去除一些神经元，就不用再对其进行dropout。另外就是L2正则化，由于每次训练都进行了归一化，就很少发生由于数据分布不同导致的参数变动过大，带来的参数不断增大。3、 可以吧训练数据集打乱，防止训练发生偏移。 使用： 在卷积中，会出现每层卷积层中有（L）多个特征图。AxAxL特征矩阵。我们只需要以每个特征图为单元求取一对γ、β。 参考 [1] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift","categories":[],"tags":[{"name":"deepLearning","slug":"deepLearning","permalink":"https://jpccc.github.io/tags/deepLearning/"}]},{"title":"互信息","slug":"互信息","date":"2021-10-25T04:44:13.000Z","updated":"2021-10-26T07:24:49.438Z","comments":true,"path":"2021/10/25/互信息/","link":"","permalink":"https://jpccc.github.io/2021/10/25/%E4%BA%92%E4%BF%A1%E6%81%AF/","excerpt":"信息熵机器学习中很多地方都要根据目前的信息做出决策，信息熵主要是反应信息的不确定性；它的一个很重要的作用，就是做决策时提供一定的判断依据，比如决策树根据熵来往下设置分枝(branch)。 计算方法 $$H(X)=-\\sum_{i=1}^nP(x_i)logP(x_i)$$ 其中$P(x_i)$代表随机事件X为$x_i$的概率。","text":"信息熵机器学习中很多地方都要根据目前的信息做出决策，信息熵主要是反应信息的不确定性；它的一个很重要的作用，就是做决策时提供一定的判断依据，比如决策树根据熵来往下设置分枝(branch)。 计算方法 $$H(X)=-\\sum_{i=1}^nP(x_i)logP(x_i)$$ 其中$P(x_i)$代表随机事件X为$x_i$的概率。 信息量 信息量是对信息的度量，就跟时间的度量是秒一样；我们考虑一个离散的随机变量x，当我们观察到的这个变量的一个具体值的时候，我们接收到了多少信息呢？ 多少信息用信息量来衡量，我们接受到的信息量信息的大小跟随机事件的概率分布有关，越小概率的事情发生了产生的信息量越大，如湖南产生的地震了；越大概率的事情发生了产生的信息量越小，如太阳从东边升起来了（肯定发生嘛，没什么信息量）。这很好理解！ 所以描述信息量的函数应该是一个与随机变量的发生概率成负相关的函数，且不能为负数。 例子: 如果我们有俩个不相关的事件x和y，那么我们观察到的俩个事件同时发生时获得的信息应该等于观察到的事件各自发生时获得的信息之和，即：h(x,y) = h(x) + h(y)。 由于x，y是俩个不相关的事件，那么满足p(x,y) = p(x)*p(y)。根据上面推导，我们很容易看出h(x)一定与p(x)的对数有关（因为只有对数形式的真数相乘之后，能够对应对数的相加形式，可以试试）。因此我们有信息量公式如下：$$h(x)=-log_2p(x)$$ 下面解决两个疑问: (1). 为什么有一个负号?&nbsp;&nbsp;&nbsp;&nbsp; 信息量取概率的负对数，其实是因为信息量的定义是概率的倒数的对数。而用概率的倒数，是为了使概率越大，信息量越小，同时因为概率的倒数大于1，其对数自然大于0了。(2). 为什么底数为2?&nbsp;&nbsp;&nbsp;&nbsp;这是因为，我们只需要信息量满足低概率事件x对应于高的信息量，那么对数的选择是任意的，我们只是遵循信息论的普遍传统，使用2作为对数的底！ 信息熵 信息量度量的是一个具体事件发生了所带来的信息，而熵则是在结果出来之前对可能产生的信息量的期望——考虑该随机变量的所有可能取值，即所有可能发生事件所带来的信息量的期望。即 $$H(X)=-\\sum_{i=1}^np(x_i)logp(x_i)$$ 信息熵还可以作为一个系统复杂程度的度量，如果系统越复杂，出现不同情况的种类越多，那么他的信息熵是比较大的。如果一个系统越简单，出现情况种类很少（极端情况为1种情况，那么对应概率为1，那么对应的信息熵为0），此时的信息熵较小。 互信息 定义 互信息(Mutual Information)是衡量随机变量之间相互依赖程度的度量。 它的形象化解释是，假如明天下雨是个随机事件，假如今晚有晚霞同样是个随机事件，那么这两个随机事件互相依赖的程度是：&nbsp;&nbsp;&nbsp;&nbsp;当不知道”今晚有晚霞“情况下，”明天下雨“带来的不确定性&nbsp;&nbsp;&nbsp;&nbsp;与我们已知“今晚有晚霞“情况下，”明天下雨”带来的不确定性之差。 解释假设存在一个随机变量$X$ ，和另外一个随机变量$Y$ ，那么它们的互信息是$$I(X;Y)=H(X)-H(X|Y)$$ $H(X)$是$X$的信息熵,$H(X|Y)$是已知$Y$情况下，X带来的信息熵（条件熵）。 直观理解是，我们知道存在两个随机事件X,Y，其中一个随机事件X 给我们带来了一些不确定性H(X)，我们想衡量Y,X 之间的关系。那么，如果X,Y 存在关联，当Y已知时，X给我们的不确定性会变化，这个变化值就是X的信息熵减去当已知 Y时，X的条件熵，就是互信息。 从概率角度，互信息是由随机变量 $X,Y$ 的联合概率分布 p(x,y) 和边缘概率分布 $p(x),p(y)$ 得出。 $$I(X;Y)=\\sum_{y \\in \\cal Y}\\sum_{x \\in \\cal X}p(x,y)log(\\frac{p(x,y)}{p(x)p(y)})$$ 互信息和信息熵的关系是： 通常我们使用的最大化互信息条件，就是最大化两个随机事件的相关性。在数据集里，就是最大化两个数据集所拟合出的概率分布的相关性。当两个随机变量相同时,互信息最大，如下:$$I(X;Y)=H(X)-H(X|X)=H(X)$$ 在机器学习中，理想情况下，当互信息最大，可以认为从数据集中拟合出来的随机变量的概率分布与真实分布相同。 到这里，应该足够大家日常理解使用了，以下是性质，应用和变形，几乎都是数学。 The most common lower bound is InfoNCE [35] whose formula is given by:","categories":[],"tags":[{"name":"math","slug":"math","permalink":"https://jpccc.github.io/tags/math/"}]},{"title":"algebra","slug":"algebra","date":"2021-10-22T08:26:34.000Z","updated":"2021-11-18T05:23:04.062Z","comments":true,"path":"2021/10/22/algebra/","link":"","permalink":"https://jpccc.github.io/2021/10/22/algebra/","excerpt":"一. 向量 向量对于不同的学科有不一样的定义，我们将从三个角度对向量进行解释。 物理学 物理中的向量由长度和方向决定，长度和方向不变的情况下随意移动，表示的都是同一个向量。 计算机 计算机中的向量更多的是对数据的抽象，可以是根据面积和价格定义的一个房子特征$\\begin{bmatrix}100m^2\\\\700000￥\\end{bmatrix}$或是通过神经网络得到的图象的的一个向量。 数学 数学中的向量可以是任意东西，只要保证两个向量的相加$\\vec v + \\vec w$以及数字和向量相乘$2\\vec v$是有意义的即可。","text":"一. 向量 向量对于不同的学科有不一样的定义，我们将从三个角度对向量进行解释。 物理学 物理中的向量由长度和方向决定，长度和方向不变的情况下随意移动，表示的都是同一个向量。 计算机 计算机中的向量更多的是对数据的抽象，可以是根据面积和价格定义的一个房子特征$\\begin{bmatrix}100m^2\\\\700000￥\\end{bmatrix}$或是通过神经网络得到的图象的的一个向量。 数学 数学中的向量可以是任意东西，只要保证两个向量的相加$\\vec v + \\vec w$以及数字和向量相乘$2\\vec v$是有意义的即可。 线性代数中的向量可以理解为一个空间中的箭头，这个箭头起点落在原点。如果空间中有许多的向量，可以用点表示一个向量，即向量头的坐标。 二. 向量的基本运算 向量的加法 可以理解为在坐标系中两个向量的移动。 向量的乘法 三. 线性组合、张成空间、基 线性组合 两个数乘向量相加称为两个向量的线性组合$a\\vec v+ b\\vec w$。两个不共线的向量通过不同的线性组合可以得到二维平面中的所有向量。两个共线的向量通过线程组合只能得到一个直线的所有向量。如果两个向量都是零向量那么它只能在原点。 张成向量 所有可以表示给定向量线性组合的向量的集合称为给定向量的张成空间（span）。一般来说两个向量张成空间可以是直线、平面。三个向量张成空间可以是平面、空间。如果多个向量，并且可以移除其中一个而不减小张成空间，那么它们是线性相关的，也可以说一个向量可以表示为其他向量的线性组合$\\vec u = a \\vec v + b\\vec w$。如果所有的向量都给张成的空间增加了新的维度，它们就成为线性无关的$\\vec u \\neq a \\vec v + b\\vec w$。 基 向量空间的一组基是张成该空间的一个线性无关向量集。 四. 矩阵与线性变换(向量的基默认为(0,1)(1,0)正交基。左乘向量可以看作是向量对基向量进行操作。同一向量乘以不同的基表示对不同的基做相同的运算。) 严格意义上来说，线性变换是将向量作为输入和输出的一类函数。变化可以多种多样，线性变化将变化限制在一个特殊类型的变换上，可以简单的理解为网格线保持平行且等距分布。线性变化满足一下两个性质： 线性变化前后直线依旧是直线不能弯曲。 原点必须保持固定 可以使用基向量来描述线性变化：通过记录两个基向量$\\hat{i}$,$\\hat{j}$的变换，就可以得到其他变化后的向量。已知向量$\\vec v = \\begin{bmatrix}-1\\\\2\\end{bmatrix}$变换之前的$\\hat i$和$\\hat j$： $$\\hat{i} = \\begin{bmatrix}1 \\ 0 \\end{bmatrix} \\ \\hat{j} = \\begin{bmatrix}0 \\ 1 \\end{bmatrix} \\\\vec{v} = -1\\hat{i} + 2 \\hat{j} = \\begin{bmatrix}-1 \\ 2 \\end{bmatrix}$$ 变换之后的$\\hat i$和$\\hat j$： $$\\begin{aligned}\\hat{i} = \\begin{bmatrix}1 \\ -2 \\end{bmatrix} \\ \\hat{j} = \\begin{bmatrix}3 \\ 0 \\end{bmatrix} \\\\vec{v} = -1\\hat{i} + 2 \\hat{j} &amp;= \\begin{bmatrix}5 \\ 2 \\end{bmatrix} \\ &amp;= -1\\begin{bmatrix}1 \\ -2 \\end{bmatrix} + 2 \\begin{bmatrix}3 \\ 0 \\end{bmatrix} \\ &amp;= \\begin{bmatrix}1 &amp; 3 \\ -2 &amp; 0 \\end{bmatrix} \\begin{bmatrix}-1 \\ 2 \\end{bmatrix} \\ &amp;= \\begin{bmatrix}5 \\ 2 \\end{bmatrix} \\\\end{aligned}$$将对基向量的变换记录下来，对其作用于其它向量，就可以得到其它向量在变换后的空间中的值 我们可以将变换后的$\\hat i$和$\\hat j$写成矩阵的形式：$\\begin{bmatrix}1 &amp; 3 \\ -2 &amp; 0 \\end{bmatrix} \\begin{bmatrix}-1 \\ 2 \\end{bmatrix}$，通过矩阵的乘法得到变化后的向量。(左侧矩阵是基向量的线性变换矩阵) 如果变化后的$\\hat{i}$和$\\hat{j}$是线性相关的，变化后向量的张量就是一维空间： 五. 矩阵乘法与线性变换复合的联系 线性变化的复合 如何描述先旋转再剪切的操作呢？ 一个通俗的方法是首先左乘旋转矩阵然后左乘剪切矩阵。 两个矩阵的乘积需要从右向左读，类似函数的复合。 这样两个矩阵的乘积就对应了一个复合的线性变换，最终得到对应变换后的$\\hat{i}$和$\\hat{j}$ 这一过程具有普适性： 矩阵乘法的顺序 如何证明矩阵乘法的结合性？ $(AB)C = A(BC)$根据线性变化我们可以得出，矩阵的乘法都是以CBA的顺序变换得到，所以他们本质上相同，通过变化的形式解释比代数计算更加容易理解。 六. 三维空间的线性变化三维的空间变化和二维的类似。 同样跟踪基向量的变换，能很好的解释变换后的向量，同样两个矩阵相乘的复合变换也是。 七. 行列式 行列式的本质 行列式的本质是计算线性变化对空间的缩放比例，具体一点就是，测量一个给定区域面积增大或减小的比例。注意，面积的变化比例是对原空间中的网格来说的。也可以说成是原基向量组成的区域面积的变化。单位面积的变换代表任意区域的面积变换比例。值得注意的是： 如果一个二维线性变换的行列式为0，说明其将说明起其将整个平面压缩成一条线甚至一个点上。 所以只要检测一个矩阵的行列式是否为0，我们就可以知道矩阵所代表的变换是否将空间压缩到更小的维度上。 行列式的值表示缩放比例。 行列式为什么有负值呢？可以从两个角度考虑： 一是变换将平面进行了反转。就好像将一张纸的正面通过变换将这张纸进行了翻面。 二是考虑的$\\hat i$和$\\hat j$的相对位置。如$\\hat i$在$\\hat j$的左边，通过变换将$\\hat i$变换到了$\\hat j$的右边，那么这个变换所对应矩阵的行列式值为负。 三维空间的行列式类似，它的单位是一个单位1的立方体。 三位空间的线性变换，可以使用右手定则判断三维空间的定向。如果变换前后都可以通过右手定则得到，那么他的行列式就是正值，否则为负值. 八. 逆矩阵、列空间、秩、零空间 线性方程组 从几何的角度来思考，矩阵A表示一个线性变换，我们需要找到一个$\\vec x$使得它在变换后和$\\vec v$重合。 逆矩阵 矩阵的逆运算，记为$\\vec A = \\begin{bmatrix}3&amp;1 \\0&amp;2\\end{bmatrix}^{-1}$，对于线程方程$A \\vec x = \\vec v $来说，找到$A^{-1}$就得到解$\\vec x = A^{-1} \\vec v$。$A^{-1}A=\\begin{bmatrix}1&amp;0 \\0&amp;1\\end{bmatrix}$，什么都不做称为恒等变换。 线性方程组的解 对于方程组$A\\vec x = \\vec v$，线性变换A存在两种情况： $det(A) \\neq0$：这时空间的维数并没有改变，有且只有一个向量经过线性变换后和$\\vec v$重合。 $det(A) =0$：空间被压缩到更低的维度，这时不存在逆变换。因为不能将一个直线解压缩为一个平面(这会要求将一个单独的向量变换为一整条线的向量，函数多对一可以，一对多不行，即存在一个矩阵A将多个向量映射到一个点，但不可能存在一个矩阵A将一个点映射成一条线的向量)。但是即使不存在逆变换，解可能仍然存在，这时候目标$\\vec v$必须刚好落在压缩后的空间上。(例如一个变换将空间压缩成了一条直线，而向量$\\vec v$恰好在这条直线上。共线的情况下，则有无穷解，因为有无穷都被压缩到直线上的每一点)。 秩 秩代表变换后空间的维度。如果线性变化后将空间压缩成一条直线，那么称这个变化的秩为1；如果线性变化后向量落在二维平面，那么称这个变化的秩为2。 列空间 所有可能的输出向量$A\\vec v$构成的集合(A为基向量的集合，对其进行任意的组合(组合即乘以$\\vec{v}$)，所得到的所有向量)，称为列空间，即所有列向量张成的空间。 更精确的秩的定义就是列空间的维数。 零空间（Null space） 所有的线性变化中，零向量一定包含在列空间中，因为线性变换原点保持不动。对于非满秩的情况来说，会有一系列的向量在变换后仍为零向量（二维空间压缩为一条直线，一条线上的向量都会落到原点。） 三维空间压缩为二维平面，一条线上的向量都会落到原点。 三维空间压缩为一条直线，整个平面上的向量都会落到原点。 当$A\\vec x = \\vec v$中的$\\vec v$是一个零向量，即$A\\vec x = \\begin{bmatrix}0 \\\\0\\end{bmatrix}$时，零空间就是它所有可能的解。 非方阵、不同维度空间之间的线性变换不同维度的变换也是存在的。 一个$3\\times2$的矩阵：$\\begin{bmatrix}2&amp;0\\\\-1&amp;1\\\\-2&amp;1 \\end{bmatrix}$它的几何意义是将一个二维空间映射到三维空间上，矩阵有两列表明输入空间有两个基向量，有三行表示每个向量在变换后用三个独立的坐标描述。(A是满秩的，因为其与输入空间x的维度相等) 一个$2\\times 3$的矩阵：$\\begin{bmatrix}3&amp;1&amp;4\\\\1&amp;5&amp;9 \\end{bmatrix}$则表示将一个三维空间映射到二维空间上。 一个$1\\times 2$的矩阵：$\\begin{bmatrix}1&amp;2 \\end{bmatrix}$表示一个二维空间映射到一维空间。 (还可以从解方程的角度，通解+特解，来理解一维变二维。) # 点积与对偶性 > 点积 对于两个维度相同的向量，他们的点积计算为：$\\begin{bmatrix}1\\\\2\\end{bmatrix}\\cdot\\begin{bmatrix} 3\\\\4\\end{bmatrix}=1\\cdot3+2\\cdot4=11$。点积的几何解释是将一个向量向一个向量投影，然后两个长度相乘，如果为负数则表示反向。 为什么点积和坐标相乘联系起来了？这和对偶性有关。 对偶性 对偶性的思想是：每当看到一个多维空间到数轴上的线性变换时，他都与空间中的唯一一个向量对应，也就是说使用线性变换和与这个向量点乘等价。这个向量也叫做线性变换的对偶向量。当二维空间向一维空间映射时，如果在二维空间中等距分布的点在变换后还是等距分布的，那么这种变换就是线性的。 假设有一个线性变换A$\\begin{bmatrix}1&amp;-2\\end{bmatrix}$和一个向量$\\vec v=\\begin{bmatrix} 4\\\\3 \\end{bmatrix}$。 变换后的位置为$\\begin{bmatrix}1&amp;-2\\end{bmatrix}\\begin{bmatrix}4\\3\\end{bmatrix}=4\\cdot1+3\\cdot-2=-2$，这个变换是一个二维空间向一维空间的变化，所以变换后的结果为一个坐标值。我们可以看到线性变换的计算过程和向量的点积相同$\\begin{bmatrix}1\\-2\\end{bmatrix}\\cdot\\begin{bmatrix}4\\3\\end{bmatrix}=4\\cdot1+3\\cdot-2=-2$，所以向量和一个线性变化有着微妙的联系。假设有一个倾斜的数轴，上面有一个单位向量$\\vec v$，对于任意一个向量它在数轴上的投影都是一个数字，这表示了一个二维向量到一位空间的一种线性变换，那么如何得到这个线性变化呢？ 由之前的内容来说，我们可以观察基向量$\\vec i$和$\\vec j$的变化，从而得到对应的线性变化。 因为$\\vec i$、$\\vec j$、$\\vec u$都是单位向量，根据对称性可以得到$\\vec i$和$\\vec j$在$\\vec u$上的投影长度刚好是$\\vec u$的坐标。 这样空间中的所有向量都可以通过线性变化$\\begin{bmatrix} u_x&amp;u_y \\end{bmatrix}$得到，而这个计算过程刚好和单位向量的点积相同。 也就是为什么向量投影到直线的长度，刚好等于它与直线上单位向量的点积，对于非单位向量也是类似，只是将其扩大到对应倍数。 叉积对于两个向量所围成的面积来说，可以使用行列式计算，将两个向量看作是变换后的基向量，这样通过行列式就可以得到变换后面积缩放的比例，因为基向量的单位为1，所以就得到了对应的面积。考虑到正向，这个面积的值存在负值，这是参照基向量$\\vec i$和$\\vec j$的相对位置来说的。 真正的叉积是通过两个三维向量$\\vec v$和$\\vec w$，生成一个新的三维向量$\\vec u$，这个向量垂直于向量$\\vec v$和$\\vec w$所在的平面，长度等于它们围成的面积。叉积的反向可以通过右手定则判断： 叉积的计算方法： 线性代数看叉积参考二维向量的叉积计算： 三维的可以写成类似的形式，但是他并是真正的叉积，不过和真正的叉积已经很接近了。 我可以构造一个函数，它可以把一个三维空间映射到一维空间上。 右侧行列式是线性的，所以我们可以找到一个线性变换代替这个函数。 根据对偶性的思想，从多维空间到一维空间的线性变换，等于与对应向量的点积，这个特殊的向量$\\vec p$就是我们要找的向量。 从数值计算上: 向量$\\vec p$的计算结果刚好和叉积计算的结果相同。 从几何意义： 当向量$\\vec p$和向量$\\begin{bmatrix}x\\\\y\\\\z \\end{bmatrix}$点乘时，得到一个$\\begin{bmatrix}x\\\\y\\\\z \\end{bmatrix}$与$\\vec v$与$\\vec w$确定的平行六面体的有向体积，什么样的向量满足这个性质呢？点积的几何解释是，其他向量在$\\vec p$上的投影的长度乘以$\\vec p$的长度。对于平行六面体的体积来说，它等于$\\vec v$和$\\vec w$所确定的面积乘以$\\begin{bmatrix}x\\\\y\\\\z \\end{bmatrix}$在垂线上的投影。那么$\\vec p$要想满足这一要求，那么它就刚好符合，长度等于$\\vec v,\\vec w$所围成的面积，且刚好垂直这个平面。 基变换标准坐标系的基向量为$\\vec {i}: \\begin{bmatrix}1\\0 \\end{bmatrix}$和$\\vec {j}: \\begin{bmatrix}0\\1 \\end{bmatrix}$，假如詹妮弗有另一个坐标系：她的基向量为$\\vec i \\begin{bmatrix}2\\1 \\end{bmatrix}$和$\\vec j \\begin{bmatrix}-1\\1 \\end{bmatrix}$。对于同一个点$\\begin{bmatrix}3\\2 \\end{bmatrix}$来说他们所表示的形式不同，在詹妮弗的坐标系中表示为$\\begin{bmatrix}\\frac{5}{3}\\\\frac{1}{3} \\end{bmatrix}$。（在不同基向量下，坐标同基相乘的结果是一样的。）{詹尼弗的坐标乘以其基向量的结果(向量)是在我们坐标系中的表示。，即$\\begin{bmatrix}3\\2 \\end{bmatrix}$}从标准坐标到詹尼佛的坐标系，我能可以得到一个线性变换$A:\\begin{bmatrix}2&amp;-1\\1&amp;1 \\end{bmatrix}$。（这个变换将詹尼佛的0，1变成我们语言表示的詹尼佛的0，1） 如果想知道詹妮弗的坐标系中点$\\begin{bmatrix}3\\2 \\end{bmatrix}$在标准坐标系的位置，可以通过$\\begin{bmatrix}2&amp;-1\\1&amp;1 \\end{bmatrix}\\begin{bmatrix}3\\2 \\end{bmatrix}$得到。（基是我们的语言表示，而坐标是詹妮弗中的坐标，那么在我们的空间网格中，詹妮弗的坐标系中点$\\begin{bmatrix}3\\2 \\end{bmatrix}$所代表的向量在我们的坐标系中的坐标为$\\begin{bmatrix}4\\5 \\end{bmatrix}$） 如果想知道标准坐标系中点$\\begin{bmatrix}3\\2 \\end{bmatrix}$在詹妮弗坐标系的位置，可以通过$\\begin{bmatrix}2&amp;-1\\1&amp;1 \\end{bmatrix}^{-1}\\begin{bmatrix}3\\2 \\end{bmatrix}$得到。具体的例子，90°旋转。在标准坐标系可以跟踪基向量的变化来体现：","categories":[],"tags":[{"name":"math","slug":"math","permalink":"https://jpccc.github.io/tags/math/"}]},{"title":"Hello World","slug":"hello-world","date":"2021-10-17T07:02:13.391Z","updated":"2021-10-23T05:02:31.970Z","comments":true,"path":"2021/10/17/hello-world/","link":"","permalink":"https://jpccc.github.io/2021/10/17/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"https://jpccc.github.io/tags/pytorch/"},{"name":"math","slug":"math","permalink":"https://jpccc.github.io/tags/math/"},{"name":"统计学习","slug":"统计学习","permalink":"https://jpccc.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/"},{"name":"deepLearning","slug":"deepLearning","permalink":"https://jpccc.github.io/tags/deepLearning/"}]}