<!DOCTYPE html>
<html lang="zh-Hans">
<head>

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no" />
<meta name="author" content=" liusha" />


    
    


<meta name="description" content="人工智能、机器学习、深度学习、数据挖掘、深度聚类">
<meta property="og:type" content="website">
<meta property="og:title" content="流沙">
<meta property="og:url" content="https://jpccc.github.io/index.html">
<meta property="og:site_name" content="流沙">
<meta property="og:description" content="人工智能、机器学习、深度学习、数据挖掘、深度聚类">
<meta property="og:locale">
<meta property="article:author" content=" liusha">
<meta property="article:tag" content="人工智能、机器学习、深度学习、数据挖掘、深度聚类">
<meta name="twitter:card" content="summary">

<link rel="apple-touch-icon" href= "/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="流沙" type="application/atom+xml">



    <link rel="shortcut icon" href="/favicon.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">



<link rel="stylesheet" href="/css/style.css">



    <style> .article { opacity: 0;} </style>


<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>流沙</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: true
    }
</script>


    <script> yiliaConfig.jquery_ui = [false]; </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/avatar.png" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/"></a></h1>
        </hgroup>

        

        
            <form id="search-form">
            <input type="text" id="local-search-input" name="q" placeholder="search..." class="search form-control" autocomplete="off" autocorrect="off" searchonload="" />
            <i class="fa fa-times" onclick="resetSearch()"></i>
            </form>
            <div id="local-search-result"></div>
            <p class='no-result'>No results found <i class='fa fa-spinner fa-pulse'></i></p>
        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                        <li>关于我</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="https://jpccc.github.io/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/%E9%9A%8F%E7%AC%94">随笔</a></li>
                        
                            <li><a href="/tags/">标签云</a></li>
                        
                            <li><a href="/about/">关于我</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="mailto:jiangpc21@mails.glu.edu.cn" title="Email"></a>
                            
                                <a class="fa GitHub" href="#" title="GitHub"></a>
                            
                                <a class="fa RSS" href="/atom.xml" title="RSS"></a>
                            
                                <a class="fa QQ" href="/1778013127" title="QQ"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/deepLearning/" rel="tag">deepLearning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/math/" rel="tag">math</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pytorch/" rel="tag">pytorch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/" rel="tag">统计学习</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://hexo.io">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://pages.github.com/">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="http://moxfive.xyz/">MOxFIVE</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">机器学习</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页"></a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/avatar.png" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页"></a></h1>
            </hgroup>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="https://jpccc.github.io/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/%E9%9A%8F%E7%AC%94">随笔</a></li>
                
                    <li><a href="/tags/">标签云</a></li>
                
                    <li><a href="/about/">关于我</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="mailto:jiangpc21@mails.glu.edu.cn" title="Email"></a>
                            
                                <a class="fa GitHub" target="_blank" href="#" title="GitHub"></a>
                            
                                <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a>
                            
                                <a class="fa QQ" target="_blank" href="/1778013127" title="QQ"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="标签" friends="友情链接" about="关于我"/>
</nav>
      <div class="body-wrap">
  
    <article id="post-心得" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2030/04/07/%E5%BF%83%E5%BE%97/" class="article-date">
      <time datetime="2030-04-07T04:44:13.000Z" itemprop="datePublished">2030-04-07</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2030/04/07/%E5%BF%83%E5%BE%97/">心得</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <ol>
<li> 卷积层要求输入的图片格式为CHW的，而PIL中展示图片要求HWC的，所以需要用permute()函数调整一下。</li>
</ol>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-RCNN" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2022/04/20/RCNN/" class="article-date">
      <time datetime="2022-04-20T15:02:07.728Z" itemprop="datePublished">2022-04-20</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="R-CNN（区域卷积神经网络）"><a href="#R-CNN（区域卷积神经网络）" class="headerlink" title="R-CNN（区域卷积神经网络）"></a>R-CNN（区域卷积神经网络）</h2><p><img src="E:\笔记\markdown\reference\picture\v2-97c647f8b4c67ca873174a8df5ccd04d_720w.jpg" alt="img"></p>
<p>​                                                                                                                            模型发展图</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a><strong>1. 引言</strong></h2><p>目标检测（Object Detection) 就是一种基于<strong>目标几何</strong>和<strong>统计特征</strong>的图像分割，它将目标的分割和识别合二为一，通俗点说就是给定一张图片要精确的定位到物体所在位置，并完成对物体类别的识别。其准确性和实时性是整个系统的一项重要能力。</p>
<p>R-CNN的全称是Region-CNN （区域卷积神经网络），是第一个成功将深度学习应用到目标检测上的算法。R-CNN基于卷积神经网络(CNN)，线性回归，和支持向量机(SVM)等算法，实现目标检测技术。</p>
<p>但是这个问题并没有想象的那么简单，首先物体的尺寸变化很大，物体摆放的角度不同，形态各异，而且可以出现在图片的任何地方，有些物体还具有多个类别。</p>
<h2 id="2-任务"><a href="#2-任务" class="headerlink" title="2. 任务"></a><strong>2. 任务</strong></h2><p>R-CNN主要就是用了做目标检测任务的。先简单了解下目标检测，我的通俗理解是对于给定图片精确的找到物体所在的位置，并且标注物体的类别(一张图像中含有一个或多个物体)。</p>
<p>输入：image</p>
<p>输出：类别标签（Category label）；位置（最小外接矩形 / Bounding Box）</p>
<p><strong>模型构思</strong></p>
<p>按分类问题对待可分为两个模块：</p>
<ul>
<li>模块一：提取物体区域（Region proposal）</li>
<li>模块二：对区域进行分类识别（Classification）</li>
</ul>
<p>主要难度： 在提取区域上需要面临 不同位置，不同尺寸，提取数量很多的问题。在分类识别方面主要面对CNN分类及计算量大的问题。</p>
<h2 id="3-传统方法-gt-R-CNN"><a href="#3-传统方法-gt-R-CNN" class="headerlink" title="3. 传统方法 -&gt; R-CNN"></a><strong>3. 传统方法 -&gt; R-CNN</strong></h2><h3 id="3-1-模型概述"><a href="#3-1-模型概述" class="headerlink" title="3.1 模型概述"></a><strong>3.1 模型概述</strong></h3><p>传统的目标检测方法大多以图像识别为基础。 一般可以在图片上使用穷举法选出所所有物体可能出现的区域框，对这些区域框提取特征并使用图像识别方法分类， 得到所有分类成功的区域后,通过**非极大值抑制(Non-maximumsuppression)**输出结果。</p>
<p>R-CNN遵循传统目标检测的思路，同样采用提取框，对每个框提取特征、图像分类、 非极大值抑制四个步骤进行目标检测。只不过在提取特征这一步，将传统的特征(如 SIFT、HOG 特征等)换成了深度卷积网络提取的特征。</p>
<h3 id="3-2-R-CNN详细步骤"><a href="#3-2-R-CNN详细步骤" class="headerlink" title="3.2 R-CNN详细步骤"></a><strong>3.2 R-CNN详细步骤</strong></h3><p><img src="E:\笔记\markdown\reference\picture\v2-bb2ab5bac50ba1e6bba523082031af2e_720w.jpg" alt="img"></p>
<p><strong>R-CNN的步骤如下（对应上图）：</strong></p>
<ol>
<li><strong>图像输入</strong> 输入待检测的图像。</li>
<li><strong>区域建议</strong>（Region proposals） 对第一步输入的图像进行区域框的选取。常用的方法是Selective Search EdgeBox，主要是利用图像的边缘、纹理、色彩、颜色变化等信息在图像中选取2000个可能存在包含物体的区域（这一步骤 选择可能存在物体的区域，跟分类无关 ，包含一个物体）。</li>
<li><strong>特征提取</strong> 使用CNN网络对选取的2000存在物体的潜在区域进行特征提取。但是可能存在一些问题，由于上一步Region proposals所提取出来的图像的尺寸大小是不一样的，我们需要卷积后输出的特征尺度是一样的，所以要将Region proposals选取的区域进行一定的缩放处理（warped region）成统一的227x227的大小，再送到CNN中特征提取。R-CNN特征提取用的网络是对ImageNet上的AlexNet（AlexNet网络详解）的CNN模型进行pre-train（以下有解释，可先行了解pre-train）得到的基本的网络模型。然后需要对网络进行fine-tune，这时网络结构需要一些修改，因为AlexNet是对1000个物体分类，fc7输出为1000，因此我们需要改为（class + 1）若类别数为20则应改为20+1=21个节点，加一的原因是对图像背景类识别，判断是不是背景。其他的都用AlexNet的网络结构fine-tune（全连接），其中包括五层卷积和两层全连接层。 （在这里使用的是ImageNet竞赛上面训练好的AlexNet模型去除最后两层全连接层的模型（也可以是VGG，GoogLeNet，ResNet等）。特征提取用的是卷积神经网络代替了传统的HOG特征，Haar特征等取特征的方法。）</li>
<li><strong>SVM分类</strong> 将提取出来的特征送入SVM分类器得到分类模型，在这里每个类别对应一个SVM分类器，如果有20个类别，则会有20个SVM分类器。对于每个类别的分类器只需要判断是不是这个类别的，如果同时多个结果为Positive则选择概率之最高的。</li>
<li><strong>Bounding Box Regression</strong> 这个回归模型主要是用来修正由第二步Region proposals得到的图像区域。同第四步的分类一样，每个类别对应一个Regression模型。这个Bounding Box Regression主要是为了精准定位。它所做的就是把旧的区域（SS算法生成的区域）<img src="https://www.zhihu.com/equation?tex=P^i=(P^i_x,P^i_y,P^i_w,P^i_h)"/> 重新映射到新的区域 <img src="https://www.zhihu.com/equation?tex=G^i=(G^i_x,G^i_y,G^i_w,G^i_h)" />，其中 - 中心位置 <img src="https://www.zhihu.com/equation?tex=(x,y)"/>宽高尺寸 (<img src="https://www.zhihu.com/equation?tex=(w,h)" />) 。</li>
</ol>
<p><img src="E:\笔记\markdown\reference\picture\v2-b272a111379a78135077a7b97225c39e_720w.jpg" alt="img"></p>
<p>上图中$W_*$ 是我们要学习的参数矩阵，一共对应四个参数各有一个$w_*$矩阵。 左图下面四个式子$t_*^i$分别是在直角坐标系和极坐标系下的比例关系。左图第一个式子中$t_*^i$ 对应下面四个式子，<img src="https://www.zhihu.com/equation?tex=W^T_*\phi_5(P^i)"/> 可以但是变化delta或是一个修正，后面的范式则是正则项。在测试的时候我们首先求得delta（右图最后一个式子)，之后分别求得四个区域参数。</p>
<ol start="6">
<li><strong>使用非极大值抑制输出（针对于测试阶段）</strong> 可能几个区域选择的是同一个区域内的物体，为了获得无冗余的区域子集。通过使用非极大值抑制（loU&gt;=0.5）获取无冗余的区域子集。主要有以下几步：</li>
</ol>
<p>① 所有区域分值从大到小排列</p>
<p>② 剔除冗余，与最大分值区域loU&gt;=0.5的所有区域</p>
<p>③ 保留最大分值区域，剩余区域作为新的候选集</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-42a254ec684dcd68d8a7d1af60cb8776_720w.jpg" alt="img"></p>
<h3 id="3-3-对以上出现的一些概念的解释："><a href="#3-3-对以上出现的一些概念的解释：" class="headerlink" title="3.3 对以上出现的一些概念的解释："></a><strong>3.3 对以上出现的一些概念的解释：</strong></h3><p><strong>3.3.1 pre-train</strong></p>
<p><strong>预训练</strong>，拿一个在其他训练集上面训练好的模型作为初始模型，共用层的参数是相同的已经在其他训练集上面训练好的。e.g. ImageNet是一个很大的数据集，它的1000个分类基本涵盖主要的物体识别，是比较权威的，在前卷积层特征提取已经比较成熟，我们就可以直接把模型的结构参数拿过来直接使用。</p>
<p><strong>3.3.2 fine-tune</strong></p>
<p><strong>再训练</strong>，把模型拿过来针对现在要解决问题的数据集再次训练，以更加适应现在数据集。e.g. 我们要分类的物体不包含在ImageNet分类中，则需要针对现在的数据集再训练，重新训练得到参数。</p>
<p><strong>3.3.3 IoU（Intersection over Union）</strong></p>
<p><img src="E:\笔记\markdown\reference\picture\v2-7dbc047d8d031c935e5f27509732af95_720w.jpg" alt="img"></p>
<img src="https://www.zhihu.com/equation?tex=IoU (Intersection\  over\ Union) = （A\bigcap B）/（A\bigcup B）= SI/(SA+SB-SI)"/>

<h3 id="3-4-一些问题"><a href="#3-4-一些问题" class="headerlink" title="3.4 一些问题"></a><strong>3.4 一些问题</strong></h3><p>R-CNN虽然不再像传统方法那样穷举，但R-CNN流程的第一步中对原始图片通过Selective Search提取的候选框region proposal多达2000个左右，而这2000个候选框每个框都需要进行CNN提特征+SVM分类，计算量很大，导致R-CNN检测速度很慢，一张图都需要47s。</p>
<p>那么有没有方法提速呢？答案是有的，这2000个region proposal不都是图像的一部分吗，那么我们完全可以对图像提一次卷积层特征，然后只需要将region proposal在原图的位置映射到卷积层特征图上，这样对于一张图像我们只需要提一次卷积层特征，然后将每个region proposal的卷积层特征输入到全连接层做后续操作。</p>
<p>但现在的问题是每个region proposal的尺度不一样，而全连接层输入必须是固定的长度，所以直接这样输入全连接层肯定是不行的。SPP Net恰好可以解决这个问题。</p>
<h2 id="4-SPP-Net网络"><a href="#4-SPP-Net网络" class="headerlink" title="4. SPP-Net网络"></a><strong>4. SPP-Net网络</strong></h2><p><strong>4.1 模型概述</strong></p>
<p><strong>SPP：Spatial Pyramid Pooling（空间金字塔池化）</strong> SPP-Net是出自2015年发表在IEEE上的论文-《Spatial Pyramid Pooling in Deep ConvolutionalNetworks for Visual Recognition》。</p>
<p>空间金字塔池化， ROI Pooling详解 点击这里：<a href="https://link.zhihu.com/?target=https://blog.csdn.net/H_hei/article/details/87298459">SPP</a>，<a href="https://link.zhihu.com/?target=https://blog.csdn.net/H_hei/article/details/89791176">ROI Pooling（感兴趣区域池化）</a></p>
<p>众所周知，CNN一般都含有卷积部分和全连接部分，其中，卷积层不需要固定尺寸的图像，而全连接层是需要固定大小的输入。所以当全连接层面对各种尺寸的输入数据时，就需要对输入数据进行crop（crop就是从一个大图扣出网络输入大小的patch，比如227×227），或warp（把一个边界框bounding box的内容resize成227×227）等一系列操作以统一图片的尺寸大小，比如224<em>224（ImageNet）、32</em>32(LenNet)、96*96等。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-0b9ff5f1d4c2e85a7fd288162328ae7d_720w.jpg" alt="img"></p>
<p>正如你在上图中看到的，在R-CNN中，“因为取出的区域大小各自不同，所以需要将每个Region Proposal缩放（warp）成统一的227x227的大小并输入到CNN”。</p>
<p>但warp/crop这种预处理，导致的问题要么被拉伸变形、要么物体不全，限制了识别精确度。没太明白？说句人话就是，一张16:9比例的图片你硬是要Resize成1:1的图片，你说图片失真不？</p>
<p>SPP Net的作者Kaiming He等人逆向思考，既然由于全连接FC层的存在，普通的CNN需要通过固定输入图片的大小来使得全连接层的输入固定。那借鉴卷积层可以适应任何尺寸，为何不能在卷积层的最后加入某种结构，使得后面全连接层得到的输入变成固定的呢？</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-c165b9c20106ed1e660d50fa694f4428_720w.png" alt="img"></p>
<p><strong>4.2 两个特点</strong></p>
<p>它的特点有两个:</p>
<p>\1. 结合空间金字塔方法实现CNN的多尺度输入。SPP Net的第一个贡献就是在最后一个卷积层后，接入了金字塔池化层，保证传到下一层全连接层的输入固定。</p>
<p>换句话说，在普通的CNN机构中，输入图像的尺寸往往是固定的（比如224*224像素），输出则是一个固定维数的向量。SPP Net在普通的CNN结构中加入了ROI池化层（ROI Pooling），使得网络的输入图像可以是任意尺寸的，输出则不变，同样是一个固定维数的向量。</p>
<p>简言之，CNN原本只能固定输入、固定输出，CNN加上SSP之后，便能任意输入、固定输出。</p>
<p>ROI池化层一般跟在卷积层后面，此时网络的输入可以是任意尺度的，在SPP layer中每一个pooling的filter会根据输入调整大小，而SPP的输出则是固定维数的向量，然后给到全连接FC层。</p>
<p>\2. 只对原图提取一次卷积特征。在R-CNN中，每个候选框先resize到统一大小，然后分别作为CNN的输入，这样是很低效的。</p>
<p>而SPP Net根据这个缺点做了优化：只对原图进行一次卷积计算，便得到整张图的卷积特征feature map，然后找到每个候选框在feature map上的映射patch，将此patch作为每个候选框的卷积特征输入到SPP layer和之后的层，完成特征提取工作。</p>
<p>如此这般，R-CNN要对每个区域计算卷积，而SPPNet只需要计算一次卷积，从而节省了大量的计算时间，比R-CNN有一百倍左右的提速。</p>
<p><strong>4.3 总结： 2大改进</strong></p>
<ul>
<li>直接输入整图，所有区域共享卷积计算（一遍），在CNN输出上提取所有区域的特征</li>
<li>引入<strong>空间金字塔池化（Spatial Pyramid Pooling）</strong>，为不同的尺寸区域在CNN输出上提取特征，映射到固定尺寸的全连接层上。</li>
</ul>
<p>SPP-Net网络使用了SPP技术实现了① <strong>共享计算</strong> ② <strong>适应不同输入的尺寸</strong></p>
<p><strong>4.4 SPP-Net一些问题</strong></p>
<p>继承了R-CNN的问题</p>
<ul>
<li>① 需要存储大量特征</li>
<li>② 复杂的多阶段训练</li>
<li>③ 训练时间长</li>
</ul>
<h2 id="5-Fast-R-CNN网络"><a href="#5-Fast-R-CNN网络" class="headerlink" title="5 . Fast R-CNN网络"></a><strong>5 . Fast R-CNN网络</strong></h2><p>空间金字塔池化， ROI Pooling详解 点击这里：<a href="https://link.zhihu.com/?target=https://blog.csdn.net/H_hei/article/details/87298459">SPP</a>，<a href="https://link.zhihu.com/?target=https://blog.csdn.net/H_hei/article/details/89791176">ROI Pooling（感兴趣区域池化）</a></p>
<p><strong>5.1 改进</strong></p>
<ul>
<li>比R-CNN，SPP-Net更快的train/test，更高的准确率，召回率。</li>
<li>实现end-to-end（端对端）单阶段训练，使用多任务损失函数。</li>
<li>所有层都可以fine-tune</li>
<li>不需要离线存储特征文件</li>
</ul>
<p><strong>引入2个新技术：</strong></p>
<ul>
<li><a href="https://link.zhihu.com/?target=https://blog.csdn.net/H_hei/article/details/89791176">感兴趣区域池化层（Rol pooling layer）</a>其实就是是空间金字塔池化特殊形式，空间金字塔池化单层特例。（详细内容这里不再叙述）</li>
<li>多任务损失函数（Multi-task loss）</li>
</ul>
<p><strong>5.2 结构</strong></p>
<p><img src="E:\笔记\markdown\reference\picture\v2-cfa67a1e68faef876ad493a4d55c4f42_720w.jpg" alt="img"></p>
<p><strong>5.3 概述</strong></p>
<p>R-CNN框架图对比，可以发现主要有两处不同：一是最后一个卷积层后加了一个ROI pooling layer，二是损失函数使用了多任务损失函数(multi-task loss)，将边框回归Bounding Box Regression直接加入到CNN网络中训练。</p>
<p>(1) ROI pooling layer实际上是SPP-NET的一个精简版，SPP-NET对每个proposal使用了不同大小的金字塔映射，而ROI pooling layer只需要下采样到一个7x7的特征图。对于VGG16网络conv5_3有512个特征图，这样所有region proposal对应了一个7<em>7</em>512维度的特征向量作为全连接层的输入。</p>
<p>换言之，这个网络层可以把不同大小的输入映射到一个固定尺度的特征向量，而我们知道，conv、pooling、relu等操作都不需要固定size的输入，因此，在原始图片上执行这些操作后，虽然输入图片size不同导致得到的feature map尺寸也不同，不能直接接到一个全连接层进行分类，但是可以加入这个神奇的ROI Pooling层，对每个region都提取一个固定维度的特征表示，再通过正常的softmax进行类型识别。</p>
<p>(2) R-CNN训练过程分为了三个阶段，而Fast R-CNN直接使用softmax替代SVM分类，同时利用多任务损失函数边框回归也加入到了网络中，这样整个的训练过程是端到端的(除去Region Proposal提取阶段)。</p>
<p>也就是说，之前R-CNN的处理流程是先提proposal，然后CNN提取特征，之后用SVM分类器，最后再做bbox regression，而在Fast R-CNN中，作者巧妙的把bbox regression放进了神经网络内部，与region分类和并成为了一个multi-task模型，实际实验也证明，这两个任务能够共享卷积特征，并相互促进。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-203fc9626f0a0325957276d2efa45ca8_720w.jpg" alt="img"></p>
<p>所以，Fast-RCNN很重要的一个贡献是成功的让人们看到了Region Proposal + CNN这一框架实时检测的希望，原来多类检测真的可以在保证准确率的同时提升处理速度，也为后来的Faster R-CNN做下了铺垫。</p>
<p><em>画一画重点：</em></p>
<p>R-CNN有一些相当大的缺点（把这些缺点都改掉了，就成了Fast R-CNN）。</p>
<p>大缺点：由于每一个候选框都要独自经过CNN，这使得花费的时间非常多。</p>
<p>解决：共享卷积层，现在不是每一个候选框都当做输入进入CNN了，而是输入一张完整的图片，在第五个卷积层再得到每个候选框的特征</p>
<p>原来的方法：许多候选框（比如两千个）–&gt;CNN–&gt;得到每个候选框的特征–&gt;分类+回归</p>
<p>现在的方法：一张完整图片–&gt;CNN–&gt;得到每张候选框的特征–&gt;分类+回归</p>
<p>所以容易看见，Fast R-CNN相对于R-CNN的提速原因就在于：不过不像R-CNN把每个候选区域给深度网络提特征，而是整张图提一次特征，再把候选框映射到conv5上，而SPP只需要计算一次特征，剩下的只需要在conv5层上操作就可以了。</p>
<p>在性能上提升也是相当明显的：</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-52673d1fc49a78c9b8c2e2acee26cca7_720w.jpg" alt="img"></p>
<p><strong>5.4 多任务损失函数</strong></p>
<p><strong>分类器loss：</strong></p>
<p><strong>bounding box回归L1 loss：</strong></p>
<p>多任务损失函数：</p>
<h2 id="6-Faster-R-CNN网络"><a href="#6-Faster-R-CNN网络" class="headerlink" title="6 . Faster R-CNN网络"></a><strong>6 . Faster R-CNN网络</strong></h2><p><img src="E:\笔记\markdown\reference\picture\v2-611e9da13b129bedc0034220947a0a0d_720w.jpg" alt="img"></p>
<p>同 Fast R-CNN 相比 Faster R-CNN引入了RPN（Region Proposal Network）即 <strong>Faster R-CNN = Fast R-CNN + RPN</strong></p>
<p><strong>6.1 RPN网络介绍参考：</strong></p>
<p><strong>重点：</strong></p>
<p><a href="https://link.zhihu.com/?target=https://blog.csdn.net/qq_36269513/article/details/80421990">RegionProposal Network)RPN网络结构及详解</a></p>
<p><strong>6.2 集成RPN网络（Region Proposal NetWork）的优点</strong></p>
<ul>
<li>取代离线Selective Search模块，解决性能瓶颈（之前提取的区域建议都是离线存储的）</li>
<li>进一步共享卷积层计算</li>
<li>基于Attention注意机制，引导Fast R-CNN关注区域</li>
<li>Region proposals量少质优</li>
<li>高准确率，召回率</li>
</ul>
<p><strong>6.3 RPN网络loss</strong></p>
<p><strong>6.4 训练过程（分步训练）</strong></p>
<p><strong>Step1 - 训练RPN网络</strong></p>
<ul>
<li>- 卷集层初始化 使用ImageNet上pre-train模型参数</li>
</ul>
<p><strong>Step2 - 训练Fast R-CNN网络</strong></p>
<ul>
<li>- 卷集层初始化 使用ImageNet上pre-train模型参数</li>
<li>- Region proposals由Step1的RPN生成</li>
</ul>
<p><strong>Step3 - 调优RPN网络</strong></p>
<ul>
<li>- 卷集层初始化 Fast R-CNN的卷积层参数</li>
<li>- 固定卷积层，finetune剩余层</li>
</ul>
<p><strong>Step4 - 调优Fast R-CNN网络</strong></p>
<ul>
<li>- 固定卷积层，finetune剩余层</li>
<li>- Region proposals由Step3的RPN生成</li>
</ul>
<h2 id="7-小结："><a href="#7-小结：" class="headerlink" title="7. 小结："></a><strong>7. 小结：</strong></h2><p>最后总结一下各大算法的步骤：</p>
<p><strong>RCNN</strong></p>
<ol>
<li>在图像中确定约1000-2000个候选框 (使用选择性搜索Selective Search)</li>
<li>每个候选框内图像块缩放至相同大小，并输入到CNN内进行特征提取</li>
<li>对候选框中提取出的特征，使用分类器判别是否属于一个特定类</li>
<li>对于属于某一类别的候选框，用回归器进一步调整其位置</li>
</ol>
<p><strong>Fast R-CNN</strong></p>
<ol>
<li>在图像中确定约1000-2000个候选框 (使用选择性搜索Selective Search)</li>
<li>对整张图片输进CNN，得到feature map</li>
<li>找到每个候选框在feature map上的映射patch，将此patch作为每个候选框的卷积特征输入到SPP layer和之后的层</li>
<li>对候选框中提取出的特征，使用分类器判别是否属于一个特定类</li>
<li>对于属于某一类别的候选框，用回归器进一步调整其位置</li>
</ol>
<p><strong>Faster R-CNN</strong></p>
<ol>
<li>对整张图片输进CNN，得到feature map</li>
<li>卷积特征输入到RPN，得到候选框的特征信息</li>
<li>对候选框中提取出的特征，使用分类器判别是否属于一个特定类</li>
<li>对于属于某一类别的候选框，用回归器进一步调整其位置</li>
</ol>
<blockquote>
<p><strong>简言之，即如本文开头所列</strong><br><strong>R-CNN（Selective Search + CNN + SVM）</strong><br><strong>SPP-net（ROI Pooling）</strong><br><strong>Fast R-CNN（Selective Search + CNN + ROI）</strong><br><strong>Faster R-CNN（RPN + CNN + ROI）</strong></p>
</blockquote>
<p>R-CNN, SPP-NET, Fast R-CNN, Faster R-CNN一路走来，基于深度学习目标检测的流程变得越来越精简，精度越来越高，速度也越来越快。</p>
<h2 id="8-参考："><a href="#8-参考：" class="headerlink" title="8. 参考："></a><strong>8. 参考：</strong></h2><p>算法之道 结构之法 ： <a href="https://link.zhihu.com/?target=https://blog.csdn.net/v_JULY_v/article/details/80170182">https://blog.csdn.net/v_JULY_v/article/details/80170182</a></p>
<p>RPN网络结构及详解： <a href="https://link.zhihu.com/?target=https://blog.csdn.net/qq_36269513/article/details/80421990">https://blog.csdn.net/qq_36269513/article/details/80421990</a></p>
<p>ROI Pooling：<a href="https://link.zhihu.com/?target=https://blog.csdn.net/H_hei/article/details/89791176">ROI Pooling（感兴趣区域池化）</a></p>
<p>SPP-Net： <a href="https://link.zhihu.com/?target=https://blog.csdn.net/H_hei/article/details/87298459">https://blog.csdn.net/H_hei/art</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-faster rcnn" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2022/04/20/faster%20rcnn/" class="article-date">
      <time datetime="2022-04-20T14:54:10.912Z" itemprop="datePublished">2022-04-20</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h1 id="RCNN-将CNN引入目标检测的开山之作"><a href="#RCNN-将CNN引入目标检测的开山之作" class="headerlink" title="RCNN- 将CNN引入目标检测的开山之作"></a>RCNN- 将CNN引入目标检测的开山之作</h1><p><a href="https://link.zhihu.com/?target=https://github.com/rbgirshick/rcnn">RCNN</a> (论文：Rich feature hierarchies for accurate object detection and semantic segmentation) 是将CNN方法引入目标检测领域， 大大提高了目标检测效果，可以说改变了目标检测领域的主要研究思路， 紧随其后的系列文章：<a href="https://link.zhihu.com/?target=https://github.com/rbgirshick/fast-rcnn">Fast RCNN</a>, <a href="https://link.zhihu.com/?target=https://github.com/ShaoqingRen/faster_rcnn">Faster RCNN</a> 。</p>
<p>【论文主要特点】（相对传统方法的改进）</p>
<ul>
<li>速度： 经典的目标检测算法使用滑动窗法依次判断所有可能的区域。本文则(采用Selective Search方法)预先提取一系列较可能是物体的候选区域，之后仅在这些候选区域上(采用CNN)提取特征，进行判断。</li>
<li>训练集： 经典的目标检测算法在区域中提取人工设定的特征。本文则采用深度网络进行特征提取。使用两个数据库： 一个较大的识别库（ImageNet ILSVC 2012）：标定每张图片中物体的类别。一千万图像，1000类。 一个较小的检测库（PASCAL VOC 2007）：标定每张图片中，物体的类别和位置，一万图像，20类。 本文使用识别库进行预训练得到CNN（有监督预训练），而后用检测库调优参数，最后在检测库上评测。</li>
</ul>
<p>看到这里也许你已经对很多名词很困惑，下面会解释。先来看看它的基本流程：</p>
<h2 id="【基本流程-】"><a href="#【基本流程-】" class="headerlink" title="【基本流程 ===================================】"></a>【基本流程 ===================================】</h2><p>RCNN算法分为4个步骤</p>
<ol>
<li>候选区域生成： 一张图像生成1K~2K个候选区域 （采用Selective Search 方法）</li>
<li>特征提取： 对每个候选区域，使用深度卷积网络提取特征 （CNN）</li>
<li>类别判断： 特征送入每一类的SVM 分类器，判别是否属于该类</li>
<li>位置精修： 使用回归器精细修正候选框位置</li>
</ol>
<p><img src="E:\笔记\markdown\reference\picture\v2-32e78b7f2e29c3e4e159a52ed38a6f73_720w.png" alt="img"></p>
<h2 id="【基础知识-】"><a href="#【基础知识-】" class="headerlink" title="【基础知识 ===================================】"></a>【基础知识 ===================================】</h2><h3 id="Selective-Search-主要思想"><a href="#Selective-Search-主要思想" class="headerlink" title="Selective Search 主要思想:"></a><strong><a href="https://link.zhihu.com/?target=http://koen.me/research/pub/uijlings-ijcv2013-draft.pdf">Selective Search</a> 主要思想:</strong></h3><ol>
<li>使用一种过分割手段，将图像分割成小区域 (1k~2k 个)</li>
<li>查看现有小区域，按照合并规则合并可能性最高的相邻两个区域。重复直到整张图像合并成一个区域位置</li>
<li>输出所有曾经存在过的区域，所谓候选区域</li>
</ol>
<p>其中合并规则如下： 优先合并以下四种区域：</p>
<ul>
<li>颜色（颜色直方图）相近的</li>
<li>纹理（梯度直方图）相近的</li>
<li>合并后总面积小的： 保证合并操作的尺度较为均匀，避免一个大区域陆续“吃掉”其他小区域 （例：设有区域a-b-c-d-e-f-g-h。较好的合并方式是：ab-cd-ef-gh -&gt; abcd-efgh -&gt; abcdefgh。 不好的合并方法是：ab-c-d-e-f-g-h -&gt;abcd-e-f-g-h -&gt;abcdef-gh -&gt; abcdefgh）</li>
<li>合并后，总面积在其BBOX中所占比例大的： 保证合并后形状规则。<br><img src="E:\笔记\markdown\reference\picture\v2-616af8e96637b9b3280e71e05877db59_720w.png" alt="img"></li>
</ul>
<p>上述四条规则只涉及区域的颜色直方图、梯度直方图、面积和位置。合并后的区域特征可以直接由子区域特征计算而来，速度较快。</p>
<h3 id="有监督预训练与无监督预训练"><a href="#有监督预训练与无监督预训练" class="headerlink" title="有监督预训练与无监督预训练:"></a><strong>有监督预训练与无监督预训练:</strong></h3><p>(1)无监督预训练(Unsupervised pre-training)</p>
<p>预训练阶段的样本不需要人工标注数据，所以就叫做无监督预训练。</p>
<p>(2)有监督预训练(Supervised pre-training)</p>
<p>所谓的有监督预训练也可以把它称之为迁移学习。比如你已经有一大堆标注好的人脸年龄分类的图片数据，训练了一个CNN，用于人脸的年龄识别。然后当你遇到新的项目任务时：人脸性别识别，那么这个时候你可以利用已经训练好的年龄识别CNN模型，去掉最后一层，然后其它的网络层参数就直接复制过来，继续进行训练，让它输出性别。这就是所谓的迁移学习，说的简单一点就是把一个任务训练好的参数，拿到另外一个任务，作为神经网络的初始参数值,这样相比于你直接采用随机初始化的方法，精度可以有很大的提高。</p>
<p>对于目标检测问题： 图片分类标注好的训练数据非常多，但是物体检测的标注数据却很少，如何用少量的标注数据，训练高质量的模型，这就是文献最大的特点，这篇论文采用了迁移学习的思想： 先用了ILSVRC2012这个训练数据库（这是一个图片分类训练数据库），先进行网络图片<strong>分类</strong>训练。这个数据库有大量的标注数据，共包含了1000种类别物体，因此预训练阶段CNN模型的输出是1000个神经元（当然也直接可以采用Alexnet训练好的模型参数）。</p>
<h3 id="重叠度（IOU）"><a href="#重叠度（IOU）" class="headerlink" title="重叠度（IOU）:"></a><strong>重叠度（IOU）:</strong></h3><p>物体检测需要定位出物体的bounding box，就像下面的图片一样，我们不仅要定位出车辆的bounding box 我们还要识别出bounding box 里面的物体就是车辆。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-0659a27df35fd2f62cd00127ca8d1a21_720w.png" alt="img"></p>
<p>对于bounding box的定位精度，有一个很重要的概念： 因为我们算法不可能百分百跟人工标注的数据完全匹配，因此就存在一个定位精度评价公式：IOU。 它定义了两个bounding box的重叠度，如下图所示</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-6fe13f10a9cb286f06aa1e3e2a2b29bc_720w.png" alt="img"></p>
<p><img src="E:\笔记\markdown\reference\picture\v2-e26ffc0835bc30dede8d82989ef9e178_720w.png" alt="img"></p>
<p>就是矩形框A、B的重叠面积占A、B并集的面积比例。</p>
<h3 id="非极大值抑制（NMS）："><a href="#非极大值抑制（NMS）：" class="headerlink" title="非极大值抑制（NMS）："></a><strong>非极大值抑制（</strong>NMS<strong>）：</strong></h3><p>RCNN会从一张图片中找出n个可能是物体的矩形框，然后为每个矩形框做类别分类概率：</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-19c03377416e437a288e29bd27e97c14_720w.png" alt="img"></p>
<p>就像上面的图片一样，定位一个车辆，最后算法就找出了一堆的方框，我们需要判别哪些矩形框是没用的。非极大值抑制的方法是：先假设有6个矩形框，根据分类器的类别分类概率做排序，假设从小到大属于车辆的概率 分别为A、B、C、D、E、F。</p>
<p>(1)从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;</p>
<p>(2)假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。</p>
<p>(3)从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。</p>
<p>就这样一直重复，找到所有被保留下来的矩形框。</p>
<p>非极大值抑制（NMS）顾名思义就是抑制不是极大值的元素，搜索局部的极大值。这个局部代表的是一个邻域，邻域有两个参数可变，一是邻域的维数，二是邻域的大小。这里不讨论通用的NMS算法，而于在目标检测中用于提取分数最高的窗口的。例如在行人检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到NMS来选取那些邻域里分数最高（是行人的概率最大），并且抑制那些分数低的窗口。</p>
<h3 id="VOC物体检测任务"><a href="#VOC物体检测任务" class="headerlink" title="VOC物体检测任务:"></a><strong>VOC物体检测任务:</strong></h3><p>相当于一个竞赛，里面包含了20个物体类别：<a href="https://link.zhihu.com/?target=http://host.robots.ox.ac.uk/pascal/VOC/voc2012/examples/index.html">PASCAL VOC2011 Example Images</a> 还有一个背景，总共就相当于21个类别，因此一会设计fine-tuning CNN的时候，我们softmax分类输出层为21个神经元。</p>
<h2 id="【各个阶段详解-】"><a href="#【各个阶段详解-】" class="headerlink" title="【各个阶段详解 ===================================】"></a>【各个阶段详解 ===================================】</h2><p>总体思路再回顾：</p>
<p>首先对每一个输入的图片产生近2000个不分种类的候选区域（region proposals），然后使用CNNs从每个候选框中提取一个固定长度的特征向量（4096维度），接着对每个取出的特征向量使用特定种类的线性SVM进行分类。也就是总个过程分为三个程序：<strong>a、找出候选框；b、利用CNN提取特征向量；c、利用SVM进行特征向量分类。</strong></p>
<p><img src="E:\笔记\markdown\reference\picture\v2-1738e9bdb129fea5d46d73218606aebd_720w.png" alt="img"></p>
<h3 id="候选框搜索阶段："><a href="#候选框搜索阶段：" class="headerlink" title="候选框搜索阶段："></a><strong>候选框搜索阶段：</strong></h3><p>当我们输入一张图片时，我们要搜索出所有可能是物体的区域，这里采用的就是前面提到的Selective Search方法，通过这个算法我们搜索出2000个候选框。然后从上面的总流程图中可以看到，搜出的候选框是矩形的，而且是大小各不相同。然而CNN对输入图片的大小是有固定的，如果把搜索到的矩形选框不做处理，就扔进CNN中，肯定不行。因此对于每个输入的候选框都需要缩放到固定的大小。下面我们讲解要怎么进行缩放处理，为了简单起见我们假设下一阶段CNN所需要的输入图片大小是个正方形图片227*227。因为我们经过selective search 得到的是矩形框，paper试验了两种不同的处理方法：</p>
<p>(1)各向异性缩放</p>
<p>这种方法很简单，就是不管图片的长宽比例，管它是否扭曲，进行缩放就是了，全部缩放到CNN输入的大小227*227，如下图(D)所示；</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-59449e8409b943f384c4cc3bf789d8b9_720w.png" alt="img"></p>
<p>(2)各向同性缩放</p>
<p>因为图片扭曲后，估计会对后续CNN的训练精度有影响，于是作者也测试了“各向同性缩放”方案。有两种办法</p>
<p>A、先扩充后裁剪： 直接在原始图片中，把bounding box的边界进行扩展延伸成正方形，然后再进行裁剪；如果已经延伸到了原始图片的外边界，那么就用bounding box中的颜色均值填充；如上图(B)所示;</p>
<p>B、先裁剪后扩充：先把bounding box图片裁剪出来，然后用固定的背景颜色填充成正方形图片(背景颜色也是采用bounding box的像素颜色均值),如上图(C)所示;</p>
<p>对于上面的异性、同性缩放，文献还有个padding处理，上面的示意图中第1、3行就是结合了padding=0,第2、4行结果图采用padding=16的结果。经过最后的试验，作者发现采用各向异性缩放、padding=16的精度最高。</p>
<p>（备注：候选框的搜索策略作者也考虑过使用一个滑动窗口的方法，然而由于更深的网络，更大的输入图片和滑动步长，使得使用滑动窗口来定位的方法充满了挑战。）</p>
<h3 id="CNN特征提取阶段："><a href="#CNN特征提取阶段：" class="headerlink" title="CNN特征提取阶段："></a><strong>CNN特征提取阶段：</strong></h3><p><strong>1、算法实现</strong></p>
<p>a、网络结构设计阶段</p>
<p>网络架构两个可选方案：第一选择经典的Alexnet；第二选择VGG16。经过测试Alexnet精度为58.5%，VGG16精度为66%。VGG这个模型的特点是选择比较小的卷积核、选择较小的跨步，这个网络的精度高，不过计算量是Alexnet的7倍。后面为了简单起见，我们就直接选用Alexnet，并进行讲解；Alexnet特征提取部分包含了5个卷积层、2个全连接层，在Alexnet中p5层神经元个数为9216、 f6、f7的神经元个数都是4096，通过这个网络训练完毕后，最后提取特征每个输入候选框图片都能得到一个4096维的特征向量。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-03e65630d303565dba3a997911e72881_720w.png" alt="img"></p>
<p><img src="E:\笔记\markdown\reference\picture\v2-002f73d5bb38dfe66e39ff472aca6c31_720w.png" alt="img"></p>
<p>b、网络有监督预训练阶段 （图片数据库：ImageNet ILSVC ）</p>
<p>参数初始化部分：物体检测的一个难点在于，物体标签训练数据少，如果要直接采用随机初始化CNN参数的方法，那么目前的训练数据量是远远不够的。这种情况下，最好的是采用某些方法，把参数初始化了，然后在进行有监督的参数微调，这里文献采用的是有监督的预训练。所以paper在设计网络结构的时候，是直接用Alexnet的网络，然后连参数也是直接采用它的参数，作为初始的参数值，然后再fine-tuning训练。网络优化求解时采用随机梯度下降法，学习率大小为0.001；</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-4a8097e292784ffaff747417b71c863d_720w.png" alt="img"></p>
<p>C、fine-tuning阶段 （图片数据库： PASCAL VOC）</p>
<p>我们接着采用 selective search 搜索出来的候选框 （PASCAL VOC 数据库中的图片） 继续对上面预训练的CNN模型进行fine-tuning训练。假设要检测的物体类别有N类，那么我们就需要把上面预训练阶段的CNN模型的最后一层给替换掉，替换成N+1个输出的神经元(加1，表示还有一个背景) (20 + 1bg = 21)，然后这一层直接采用参数随机初始化的方法，其它网络层的参数不变；接着就可以开始继续SGD训练了。开始的时候，SGD学习率选择0.001，在每次训练的时候，我们batch size大小选择128，其中32个事正样本、96个事负样本。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-728cc0822b07a6db24468698463efb89_720w.png" alt="img"></p>
<p>关于正负样本问题：</p>
<p>一张照片我们得到了2000个候选框。然而人工标注的数据一张图片中就只标注了正确的bounding box，我们搜索出来的2000个矩形框也不可能会出现一个与人工标注完全匹配的候选框。因此在CNN阶段我们需要用IOU为2000个bounding box打标签。如果用selective search挑选出来的候选框与物体的人工标注矩形框（PASCAL VOC的图片都有人工标注）的重叠区域IoU大于0.5，那么我们就把这个候选框标注成物体类别（正样本），否则我们就把它当做背景类别（负样本）。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-f67cd928e318ec00bc6047075c88e0b8_720w.png" alt="img"></p>
<p>（备注： 如果不针对特定任务进行fine-tuning，而是把CNN当做特征提取器，卷积层所学到的特征其实就是基础的共享特征提取层，就类似于SIFT算法一样，可以用于提取各种图片的特征，而f6、f7所学习到的特征是用于针对特定任务的特征。打个比方：对于人脸性别识别来说，一个CNN模型前面的卷积层所学习到的特征就类似于学习人脸共性特征，然后全连接层所学习的特征就是针对性别分类的特征了）</p>
<p><strong>2. 疑惑点</strong>： CNN训练的时候，本来就是对bounding box的物体进行识别分类训练，在训练的时候最后一层softmax就是分类层。那么为什么作者闲着没事干要先用CNN做特征提取（提取fc7层数据），然后再把提取的特征用于训练svm分类器？</p>
<p>这个是因为svm训练和cnn训练过程的正负样本定义方式各有不同，导致最后采用CNN softmax输出比采用svm精度还低。事情是这样的，cnn在训练的时候，对训练数据做了比较宽松的标注，比如一个bounding box可能只包含物体的一部分，那么我也把它标注为正样本，用于训练cnn；采用这个方法的主要原因在于因为CNN容易过拟合，所以需要大量的训练数据，所以在CNN训练阶段我们是对Bounding box的位置限制条件限制的比较松(IOU只要大于0.5都被标注为正样本了)；然而svm训练的时候，因为svm适用于少样本训练，所以对于训练样本数据的IOU要求比较严格，我们只有当bounding box把整个物体都包含进去了，我们才把它标注为物体类别，然后训练svm，具体请看下文。</p>
<h3 id="SVM训练、测试阶段"><a href="#SVM训练、测试阶段" class="headerlink" title="SVM训练、测试阶段"></a><strong>SVM训练、测试阶段</strong></h3><p>训练阶段：</p>
<p>这是一个二分类问题，我么假设我们要检测车辆。我们知道只有当bounding box把整量车都包含在内，那才叫正样本；如果bounding box 没有包含到车辆，那么我们就可以把它当做负样本。但问题是当我们的检测窗口只有部分包含物体，那该怎么定义正负样本呢？作者测试了IOU阈值各种方案数值0,0.1,0.2,0.3,0.4,0.5。最后通过训练发现，如果选择IOU阈值为0.3效果最好（选择为0精度下降了4个百分点，选择0.5精度下降了5个百分点）,即当重叠度小于0.3的时候，我们就把它标注为负样本。一旦CNN f7层特征被提取出来，那么我们将为每个物体类训练一个svm分类器。当我们用CNN提取2000个候选框，可以得到2000**4096这样的特征向量矩阵，然后我们只需要把这样的一个矩阵与svm权值矩阵4096*N点乘(N为分类类别数目，因为我们训练的N个svm，每个svm包含了4096个权值w)，就可以得到结果了。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-3ef21dd028fd210f92107c1ded528045_720w.png" alt="img"></p>
<p>得到的特征输入到SVM进行分类看看这个feature vector所对应的region proposal是需要的物体还是无关的实物(background) 。</p>
<p> 排序，canny边界检测之后就得到了我们需要的bounding-box。</p>
<blockquote>
<p>再回顾总结一下：整个系统分为三个部分：1.产生不依赖与特定类别的region proposals，这些region proposals定义了一个整个检测器可以获得的候选目标2.一个大的卷积神经网络，对每个region产生一个固定长度的特征向量3.一系列特定类别的线性SVM分类器。</p>
</blockquote>
<p>位置精修： 目标检测问题的衡量标准是重叠面积：许多看似准确的检测结果，往往因为候选框不够准确，重叠面积很小。故需要一个位置精修步骤。 回归器：对每一类目标，使用一个线性脊回归器进行精修。正则项λ=10000。 输入为深度网络pool5层的4096维特征，输出为xy方向的缩放和平移。 训练样本：判定为本类的候选框中和真值重叠面积大于0.6的候选框。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-7e2c472157f6a4028db9f8ba3c0eb744_720w.png" alt="img"></p>
<p>测试阶段：</p>
<p>使用selective search的方法在测试图片上提取2000个region propasals ，将每个region proposals归一化到227x227，然后在CNN中正向传播，将最后一层得到的特征提取出来。然后对于每一个类别，使用为这一类训练的SVM分类器对提取的特征向量进行打分，得到测试图片中对于所有region proposals的对于这一类的分数，再使用贪心的非极大值抑制（NMS）去除相交的多余的框。再对这些框进行canny边缘检测，就可以得到bounding-box(then B-BoxRegression)。</p>
<p>（非极大值抑制（NMS）先计算出每一个bounding box的面积，然后根据score进行排序，把score最大的bounding box作为选定的框，计算其余bounding box与当前最大score与box的IoU，去除IoU大于设定的阈值的bounding box。然后重复上面的过程，直至候选bounding box为空，然后再将score小于一定阈值的选定框删除得到这一类的结果（然后继续进行下一个分类）。作者提到花费在region propasals和提取特征的时间是13s/张-GPU和53s/张-CPU，可以看出时间还是很长的，不能够达到及时性。</p>
<p>本文主要整理自以下文章：</p>
<ul>
<li><a href="https://link.zhihu.com/?target=http://blog.csdn.net/u011534057/article/details/51240387">RCNN学习笔记(0):rcnn简介</a></li>
<li><a href="https://link.zhihu.com/?target=http://blog.csdn.net/u011534057/article/details/51218218">RCNN学习笔记(1):Rich feature hierarchies for accurate object detection and semantic segmentation</a></li>
<li>《Rich feature hierarchies for Accurate Object Detection and Segmentation》</li>
<li>《Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition》</li>
</ul>
<h1 id="SPPnet"><a href="#SPPnet" class="headerlink" title="SPPnet"></a>SPPnet</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>沿着上一篇RCNN的思路，我们继续探索目标检测的痛点，其中RCNN使用CNN作为特征提取器，首次使得目标检测跨入深度学习的阶段。但是RCNN对于每一个区域候选都需要首先将图片放缩到固定的尺寸（224*224），然后为每个区域候选提取CNN特征。容易看出这里面存在的一些性能瓶颈：</p>
<ul>
<li>速度瓶颈：重复为每个region proposal提取特征是极其费时的，Selective Search对于每幅图片产生2K左右个region proposal，也就是意味着一幅图片需要经过2K次的完整的CNN计算得到最终的结果。</li>
<li>性能瓶颈：对于所有的region proposal防缩到固定的尺寸会导致我们不期望看到的几何形变，而且由于速度瓶颈的存在，不可能采用多尺度或者是大量的数据增强去训练模型。</li>
</ul>
<p>但是为什么CNN需要固定的输入呢？CNN网络可以分解为卷积网络部分以及全连接网络部分。我们知道卷积网络的参数主要是卷积核，完全能够适用任意大小的输入，并且能够产生任意大小的输出。但是全连接层部分不同，全连接层部分的参数是神经元对于所有输入的连接权重，也就是说输入尺寸不固定的话，全连接层参数的个数都不能固定。</p>
<p>何凯明团队的SPPNet给出的解决方案是，既然只有全连接层需要固定的输入，那么我们在全连接层前加入一个网络层，让他对任意的输入产生固定的输出不就好了吗？一种常见的想法是对于最后一层卷积层的输出pooling一下，但是这个pooling窗口的尺寸及步伐设置为相对值，也就是输出尺寸的一个比例值，这样对于任意输入经过这层后都能得到一个固定的输出。SPPnet在这个想法上继续加入SPM的思路，SPM其实在传统的机器学习特征提取中很常用，主要思路就是对于一副图像分成若干尺度的一些块，比如一幅图像分成1份，4份，8份等。然后对于每一块提取特征然后融合在一起，这样就可以兼容多个尺度的特征啦。SPPNet首次将这种思想应用在CNN中，对于卷积层特征我们也先给他分成不同的尺寸，然后每个尺寸提取一个固定维度的特征，最后拼接这些特征不就是一个固定维度的输入了吗？</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-da59abcbe56803aeeef24ffb5131ce83_720w.png" alt="img"></p>
<p>上面这个图可以看出SPPnet和RCNN的区别，首先是输入不需要放缩到指定大小。其次是增加了一个空间金字塔池化层，还有最重要的一点是每幅图片只需要提取一次特征。</p>
<p>通过上述方法虽然解决了CNN输入任意大小图片的问题，但是还是需要重复为每个region proposal提取特征啊，能不能我们直接根据region proposal定位到他在卷积层特征的位置，然后直接对于这部分特征处理呢？答案是肯定的，我们将在下一章节介绍。</p>
<h2 id="网络细节"><a href="#网络细节" class="headerlink" title="网络细节"></a>网络细节</h2><ul>
<li><strong>卷积层特征图</strong></li>
</ul>
<p><img src="E:\笔记\markdown\reference\picture\v2-523707e94ccb850ca4c23cc94054a144_720w.png" alt="img"></p>
<p>SPPNet通过可视化Conv5层特征，发现卷积特征其实保存了空间位置信息（数学推理中更容易发现这点），并且每一个卷积核负责提取不同的特征，比如C图175、55卷积核的特征，其中175负责提取窗口特征，55负责提取圆形的类似于车轮的特征。我们可以通过传统的方法聚集这些特征，例如词袋模型或是空间金字塔的方法。</p>
<ul>
<li><strong>空间金字塔池化层</strong></li>
</ul>
<p><img src="E:\笔记\markdown\reference\picture\v2-62c008799df798656236258c64082340_720w.png" alt="img"></p>
<p>上图的空间金字塔池化层是SPPNet的核心，其主要目的是对于任意尺寸的输入产生固定大小的输出。思路是对于任意大小的feature map首先分成16、4、1个块，然后在每个块上最大池化，池化后的特征拼接得到一个固定维度的输出。以满足全连接层的需要。不过因为不是针对于目标检测的，所以输入的图像为一整副图像。</p>
<ul>
<li><strong>SPPNet应用于图像分类</strong></li>
</ul>
<p>SPPNet的能够接受任意尺寸图片的输入，但是训练难点在于所有的深度学习框架都需要固定大小的输入，因此SPPNet做出了多阶段多尺寸训练方法。在每一个epoch的时候，我们先将图像放缩到一个size，然后训练网络。训练完整后保存网络的参数，然后resize 到另外一个尺寸，并在之前权值的基础上再次训练模型。相比于其他的CNN网络，SPPNet的优点是可以方便地进行多尺寸训练，而且对于同一个尺度，其特征也是个空间金字塔的特征，综合了多个特征的空间多尺度信息。</p>
<ul>
<li><strong>SPPNet应用于目标检测</strong></li>
</ul>
<p><img src="E:\笔记\markdown\reference\picture\v2-d68eaa673b48c3176eb48b3cb16a761f_720w.png" alt="img"></p>
<p>SPPNet理论上可以改进任何CNN网络，通过空间金字塔池化，使得CNN的特征不再是单一尺度的。但是SPPNet更适用于处理目标检测问题，首先是网络可以介绍任意大小的输入，也就是说能够很方便地多尺寸训练。其次是空间金字塔池化能够对于任意大小的输入产生固定的输出，这样使得一幅图片的多个region proposal提取一次特征成为可能。SPPNet的做法是：</p>
<ol>
<li>首先通过selective search产生一系列的region proposal，参见：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27467369">目标检测（1）-Selective Search - 知乎专栏</a></li>
<li>然后训练多尺寸识别网络用以提取区域特征，其中处理方法是每个尺寸的最短边大小在尺寸集合中：</li>
</ol>
<p><img src="E:\笔记\markdown\reference\picture\v2-237603b04a4f5f801924219f4fdfad99_720w.png" alt="img"></p>
<p>训练的时候通过上面提到的多尺寸训练方法，也就是在每个epoch中首先训练一个尺寸产生一个model，然后加载这个model并训练第二个尺寸，直到训练完所有的尺寸。空间金字塔池化使用的尺度为：1<em>1，2</em>2，3<em>3，6</em>6，一共是50个bins。</p>
<p>3.在测试时，每个region proposal选择能使其包含的像素个数最接近224*224的尺寸，提取相 应特征。</p>
<p>由于我们的空间金字塔池化可以接受任意大小的输入，因此对于每个region proposal将其映射到feature map上，然后仅对这一块feature map进行空间金字塔池化就可以得到固定维度的特征用以训练CNN了。关于从region proposal映射到feature map的细节我们待会儿去说。</p>
<p>4.训练SVM，BoundingBox回归</p>
<p>这部分和RCNN完全一致，参见：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27473413">目标检测（2）-RCNN - 知乎专栏</a></p>
<ul>
<li><strong>实验结果</strong></li>
</ul>
<p>其中单一尺寸训练结果低于RCNN1.2%，但是速度是其102倍，5个尺寸的训练结果与RCNN相当，其速度为RCNN的38倍。</p>
<ul>
<li><strong>如何从一个region proposal 映射到feature map的位置？</strong></li>
</ul>
<p>SPPNet通过角点尽量将图像像素映射到feature map感受野的中央，假设每一层的padding都是p/2，p为卷积核大小。对于feature map的一个像素（x’,y’），其实际感受野为：（Sx‘，Sy’），其中S为之前所有层步伐的乘积。然后对于region proposal的位置，我们获取左上右下两个点对应的feature map的位置，然后取特征就好了。左上角映射为：</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-8c5eddc9f856822aad5ae8d030ce1779_720w.png" alt="img"></p>
<p>右下角映射为：</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-7a4ce0c60b8fcac5eb7ffe365f99572e_720w.png" alt="img"></p>
<p>当然，如果padding大小不一致，那么就需要计算相应的偏移值啦。</p>
<h2 id="存在的不足"><a href="#存在的不足" class="headerlink" title="存在的不足"></a>存在的不足</h2><p>和RCNN一样，SPP也需要训练CNN提取特征，然后训练SVM分类这些特征。需要巨大的存储空间，并且分开训练也很复杂。而且selective search的方法提取特征是在CPU上进行的，相对于GPU来说还是比较慢的。针对这些问题的改进，我们将在Fast RCNN以及Faster RCNN中介绍，敬请期待。</p>
<h1 id="Fast-RCNN"><a href="#Fast-RCNN" class="headerlink" title="Fast RCNN"></a>Fast RCNN</h1><p>Fast-RCNN是基于RCNN和SPPnet的进一步改进。Fast-RCNN针对RCNN的以下三点缺点做了改进：</p>
<ol>
<li>RCNN是多阶段的模型，使用selective search算法筛选候选区域-&gt;<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:%22338211515%22%7D">卷积神经网络</a>提取特征值-&gt;SVM判断类型，非常繁琐。</li>
<li>RCNN训练非常消耗时间和内存。</li>
<li>RCNN的识别速度很慢。</li>
</ol>
<h2 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h2><p><img src="E:\笔记\markdown\reference\picture\v2-ced13fd5cc13bcccfcaf3afa20dce95e_b.jpg" alt="img">Fast-RCNN整体结构图</p>
<p><strong>从上图可以看到，相比起RCNN，Fast-RCNN使用全连接层替代了SVM来识别物体，并且Fast-RCNN摒弃了以前每一个候选区域分别放入卷积神经网络进行特征提取的方法，将整个图片直接放入卷积神经网络提取特征，避免了重复计算，提高了检测的速度。</strong></p>
<p>上面这个图片可能有点抽象，下面这个图片摘自<a href="https://link.zhihu.com/?target=https://blog.csdn.net/shenxiaolu1984/article/details/51036677">博客</a>，更加清晰的展示了网络的细节：</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-d6b522d5eb9ff43c3bcd66c8448844d1_b.jpg" alt="img"></p>
<p>​                                                                                                                Fast-RCNN网络结构图</p>
<ul>
<li>一张（224,224,3）的图片进入网络（VGG16）进行<strong>提取特征。</strong></li>
<li>进入ROI Pooling层</li>
<li>再经过两个output都为4096维的全连接层</li>
<li>分别经过output各为21和84维的全连接层（并列的，前者是分类输出，有21个种类；后者是回归输出，输出边框的位置[x,y,w,h], 21*4=84）</li>
</ul>
<p>下面这张来自<a href="https://link.zhihu.com/?target=https://www.robots.ox.ac.uk/~tvg/publications/talks/fast-rcnn-slides.pdf">Fast-RCNN的PPT</a>图可能更加清晰一些：</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-be21dff7c814eb9d470d35805f965ebb_b.jpg" alt="img">Fast-RCNN结构图</p>
<p>从上图可以看到，卷积不再是对每个region proposal（候选区域）进行，而是直接对整张图像，这样减少了很多重复计算。原来RCNN是对每个候选区域分别做卷积，因为一张图像中有2000左右的候选区域，肯定相互之间的重叠率很高，因此产生重复计算。</p>
<p><strong>每一个候选区域都是输入图片的一张子图，那么每一个候选区域都对应<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=%E7%89%B9%E5%BE%81%E5%9B%BE&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:%22338211515%22%7D">特征图</a>中的一个部分。</strong></p>
<h2 id="ROI"><a href="#ROI" class="headerlink" title="ROI"></a>ROI</h2><p>使用Selective Search算法对输入图片进行提取候选区域，候选区域的大小不一，所以对应特征图的区域也不一样，而全连接层的输入是固定的，这样就无法直接输入到全连接层中。如何将其变化成统一的格式呢？</p>
<p>ROI Pooling的作用是对不同大小的候选区域，从最后卷积层输出的特征图提取大小固定的特征图。</p>
<p>使用ROI Pooling层将每个候选区域均匀分成M×N块，对每块进行Max Pooling。将所有输出值组合起来便形成固定大小为H×W的特征图。这样就可以将特征图上大小不一的候选区域转变为大小统一的数据，送入下一层。图片来自<a href="https://link.zhihu.com/?target=https://blog.csdn.net/shenxiaolu1984/article/details/51036677">博客</a>。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-95e27f746e16ee25da88871c3114ff76_b.jpg" alt="img">ROI Pooling示意图</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><strong>总损失</strong>如下：</p>
<img src='https://www.zhihu.com/equation?tex=L(p, u, t^u , v) = Lcls(p, u) + λ[u ≥ 1]Lloc(t^u, v)'>

<p>损失函数分为两个部分：<img src='https://www.zhihu.com/equation?tex=Lcls'> 表示分类损失，<img src='https://www.zhihu.com/equation?tex=Lloc'> 表示定位损失(回归损失).</p>
<p>λ用于判断背景，如果分类不是背景，λ=1</p>
<p>分类如果是背景λ=0，则不考虑定位损失,即：</p>
<p>$\begin{equation} L=\left{ \begin{aligned} Lcls+λLloc&amp; &amp; {u不为背景}\ Lcls &amp; &amp; {u为背景}\ \end{aligned} \right. \end{equation}$</p>
<p><strong>分类损失：</strong></p>
<p>对于分类的全连接神经网络，它将产生由Softmax产生的概率p=(p0,p1…,pk)。这里K=20,所以共有21个概率值。</p>
<p><img src='https://www.zhihu.com/equation?tex= Lcls(p, u)=−log(p_u)'>，其中<img src='https://www.zhihu.com/equation?tex=p_u'> 是真实类型预测的那个概率。也就是说这里只计算一个 ，其他概率值不算。</p>
<p><strong>定位损失：</strong></p>
<p>对于bounding-box回归的全连接网络，它产生的是位置信息（x,y,w,h），分类神经网络每一个概率值，它都对应有一个位置信息。所以网络最后输出维度为：21*4=84。 </p>
<p><img src="E:\笔记\markdown\reference\picture\equation-165157845950121.svg+xml" alt="[公式]"> 表示第k个种类所对应的位置信息。</p>
<p><img src="E:\笔记\markdown\reference\picture\equation-165157845950122.svg+xml" alt="[公式]"> 表示真实的位置信息</p>
<p><img src="E:\笔记\markdown\reference\picture\in{x%2Cy%2Cw%2Ch}}{smooth_{L_i}(t_i^u-v_i)}.svg+xml" alt="[公式]"> </p>
<p>其中smooth函数:</p>
<p><img src="E:\笔记\markdown\reference\picture\end{equation}-165157845950123.svg+xml" alt="[公式]"> </p>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>看一下训练时候的整体结构图：</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-5fbec4583ca50103e33d421463d3a3a0_b.jpg" alt="img"></p>
<p>在调优训练时，每一个mini-batch中首先加入N张完整图片，而后加入从N张图片中选取的R个候选框。这R个候选框可以复用N张图片前5个阶段的网络特征。<strong>实际选择N=2， R=128</strong></p>
<p><strong>训练数据构成</strong></p>
<p>N张完整图片以50%概率水平翻转。</p>
<p>R个候选框的构成方式如下：</p>
<table>
<thead>
<tr>
<th>类别</th>
<th>比例</th>
<th>方式</th>
</tr>
</thead>
<tbody><tr>
<td>前景</td>
<td>25%</td>
<td>与某个真值重叠在[0.5,1]的候选框</td>
</tr>
<tr>
<td>背景</td>
<td>75%</td>
<td>与真值重叠的最大值在[0.1,0.5)的候选框</td>
</tr>
</tbody></table>
<h2 id="SVD加速"><a href="#SVD加速" class="headerlink" title="SVD加速"></a>SVD加速</h2><p>分类和位置调整都是通过全连接层实现的，假设全连接层参数为W，尺寸u × v 一次前向传播即为：Y=Wx,计算复次数为u × v</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-7b188e68f559382e0e1ddce4d4dfb837_b.jpg" alt="img">网络中各个部分计算所占用的时间</p>
<p>从上图可以看出，两个全连接层（fc6、fc7）占用了44.9%的时间，非常消耗时间。</p>
<p>为此，论文中对W进行SVD分解，并用前t个特征值近似：</p>
<p><img src="E:\笔记\markdown\reference\picture\equation-165157845950124.svg+xml" alt="[公式]"> </p>
<p>原来的前向传播分解成两步： <img src="E:\笔记\markdown\reference\picture\equation-165157845950125.svg+xml" alt="[公式]"> </p>
<p>计算复杂度变为u × t + v × t=(u + v) × t</p>
<p>在实现时，相当于把一个全连接层拆分成两个，中间以一个<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=%E4%BD%8E%E7%BB%B4%E6%95%B0%E6%8D%AE&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:%22338211515%22%7D">低维数据</a>相连。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-3d11b57e6904c18bc8bd89efc4a06bda_b.jpg" alt="img"></p>
<p>使用SVD加速后，各部分使用时间如下所示：</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-8781d554a612c8a2c6686f4ac0df7a22_b.jpg" alt="img"></p>
<p>​                                                                使用SVD加速后，网络中各个部分计算所占用的时间</p>
<p>可以看到加速确实有效果了。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="E:\笔记\markdown\reference\picture\v2-ff6a2918f64d1977fad99bb840facd7b_b.jpg" alt="img"></p>
<p><img src="E:\笔记\markdown\reference\picture\v2-f0af16bde4eb2271920696e00ed5d465_b.jpg" alt="img"></p>
<h2 id="参考连接："><a href="#参考连接：" class="headerlink" title="参考连接："></a>参考连接：</h2><p><a href="https://link.zhihu.com/?target=https://www.robots.ox.ac.uk/~tvg/publications/talks/fast-rcnn-slides.pdf">https://www.robots.ox.ac.uk/~tvg/publications/talks/fast-rcnn-slides.pdf</a></p>
<p><a href="https://link.zhihu.com/?target=https://blog.csdn.net/shenxiaolu1984/article/details/51036677">https://blog.csdn.net/s</a>henxiaolu1984/article/details/51036677</p>
<p><a href="https://link.zhihu.com/?target=https://blog.csdn.net/u014380165/article/details/72851319">Fast RCNN算法详解_AI之路-CSDN博客_fast rcnn</a></p>
<p><a href="https://link.zhihu.com/?target=https://www.cnblogs.com/CZiFan/p/9903518.html">Fast R-CNN（理解） - CZiFan - 博客园</a></p>
<p><a href="https://link.zhihu.com/?target=https://blog.csdn.net/u014380165/article/details/72851319">Fast RCNN算法详解_AI之路-CSDN博客_fast rcnn</a></p>
<h1 id="Faster-RCNN"><a href="#Faster-RCNN" class="headerlink" title="Faster RCNN"></a>Faster RCNN</h1><p>经过R-CNN和Fast RCNN的积淀，Ross B. Girshick在2016年提出了新的Faster RCNN，在结构上，Faster RCNN已经将特征抽取(feature extraction)，proposal提取，bounding box regression(rect refine)，classification都整合在了一个网络中，使得综合性能有较大提高，在检测速度方面尤为明显。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-c0172be282021a1029f7b72b51079ffe_720w.jpg" alt="img">图1 Faster RCNN基本结构（来自原论文）</p>
<p>依作者看来，如图1，Faster RCNN其实可以分为4个主要内容：</p>
<ol>
<li>Conv layers。作为一种CNN网络目标检测方法，Faster RCNN首先使用一组基础的conv+relu+pooling层提取image的feature maps。该feature maps被共享用于后续RPN层和全连接层。</li>
<li>Region Proposal Networks。RPN网络用于生成region proposals。该层通过softmax判断anchors属于positive或者negative，再利用bounding box regression修正anchors获得精确的proposals。</li>
<li>Roi Pooling。该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。</li>
<li>Classification。利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。</li>
</ol>
<p>所以本文以上述4个内容作为切入点介绍Faster R-CNN网络。</p>
<p>图2展示了python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构，可以清晰的看到该网络对于一副任意大小PxQ的图像：</p>
<ul>
<li>首先缩放至固定大小MxN，然后将MxN图像送入网络；</li>
<li>而Conv layers中包含了13个conv层+13个relu层+4个pooling层；</li>
<li>RPN网络首先经过3x3卷积，再分别生成positive anchors和对应bounding box regression偏移量，然后计算出proposals；</li>
<li>而Roi Pooling层则利用proposals从feature maps中提取proposal feature送入后续全连接和softmax网络作classification（即分类proposal到底是什么object）。</li>
</ul>
<p><img src="E:\笔记\markdown\reference\picture\v2-e64a99b38f411c337f538eb5f093bdf3_720w.jpg" alt="img">图2 faster_rcnn_test.pt网络结构 （pascal_voc/VGG16/faster_rcnn_alt_opt/faster_rcnn_test.pt）</p>
<p><em>本文不会讨论任何关于R-CNN家族的历史，分析清楚最新的Faster R-CNN就够了，并不需要追溯到那么久。实话说我也不了解R-CNN，更不关心。有空不如看看新算法。</em></p>
<p>新出炉的pytorch官方Faster RCNN代码导读：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/145842317">捋一捋pytorch官方FasterRCNN代码1308 赞同 · 21 评论文章<img src="E:\笔记\markdown\reference\picture\v2-dc78d0ad1012ad65040bdb7eb657f381_180x120.jpg" alt="img"></a></p>
<h2 id="1-Conv-layers"><a href="#1-Conv-layers" class="headerlink" title="1 Conv layers"></a>1 Conv layers</h2><p>Conv layers包含了conv，pooling，relu三种层。以python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构为例，如图2，Conv layers部分共有13个conv层，13个relu层，4个pooling层。这里有一个非常容易被忽略但是又无比重要的信息，在Conv layers中：</p>
<ol>
<li>所有的conv层都是：kernel_size=3，pad=1，stride=1</li>
<li>所有的pooling层都是：kernel_size=2，pad=0，stride=2</li>
</ol>
<p>为何重要？在Faster RCNN Conv layers中对所有的卷积都做了扩边处理（ pad=1，即填充一圈0），导致原图变为 (M+2)x(N+2)大小，再做3x3卷积后输出MxN 。正是这种设置，导致Conv layers中的conv层不改变输入和输出矩阵大小。如图3：</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-3c772e9ed555eb86a97ef9c08bf563c9_720w.jpg" alt="img">图3 卷积示意图</p>
<p>类似的是，Conv layers中的pooling层kernel_size=2，stride=2。这样每个经过pooling层的MxN矩阵，都会变为(M/2)x(N/2)大小。综上所述，在整个Conv layers中，conv和relu层不改变输入输出大小，只有pooling层使输出长宽都变为输入的1/2。</p>
<p>那么，一个MxN大小的矩阵经过Conv layers固定变为(M/16)x(N/16)！这样Conv layers生成的feature map中都可以和原图对应起来。</p>
<h2 id="2-Region-Proposal-Networks-RPN"><a href="#2-Region-Proposal-Networks-RPN" class="headerlink" title="2 Region Proposal Networks(RPN)"></a>2 Region Proposal Networks(RPN)</h2><p>经典的检测方法生成检测框都非常耗时，如OpenCV adaboost使用滑动窗口+图像金字塔生成检测框；或如R-CNN使用SS(Selective Search)方法生成检测框。而Faster RCNN则抛弃了传统的滑动窗口和SS方法，直接使用RPN生成检测框，这也是Faster R-CNN的巨大优势，能极大提升检测框的生成速度。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-1908feeaba591d28bee3c4a754cca282_720w.jpg" alt="img">图4 RPN网络结构</p>
<p>上图4展示了RPN网络的具体结构。可以看到RPN网络实际分为2条线，上面一条通过softmax分类anchors获得positive和negative分类，下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。而最后的Proposal层则负责综合positive anchors和对应bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能。</p>
<h3 id="2-1-多通道图像卷积基础知识介绍"><a href="#2-1-多通道图像卷积基础知识介绍" class="headerlink" title="2.1 多通道图像卷积基础知识介绍"></a>2.1 多通道图像卷积基础知识介绍</h3><p>在介绍RPN前，还要多解释几句基础知识，已经懂的看官老爷跳过就好。</p>
<ol>
<li>对于单通道图像+单卷积核做卷积，第一章中的图3已经展示了；</li>
<li>对于多通道图像+多卷积核做卷积，计算方式如下：</li>
</ol>
<p><img src="E:\笔记\markdown\reference\picture\v2-8d72777321cbf1336b79d839b6c7f9fc_720w.jpg" alt="img">图5 多通道卷积计算方式</p>
<p>如图5，输入有3个通道，同时有2个卷积核。对于每个卷积核，先在输入3个通道分别作卷积，再将3个通道结果加起来得到卷积输出。所以对于某个卷积层，无论输入图像有多少个通道，输出图像通道数总是等于卷积核数量！</p>
<p>对多通道图像做1x1卷积，其实就是将输入图像于每个通道乘以卷积系数后加在一起，即相当于把原图像中本来各个独立的通道“联通”在了一起。</p>
<h3 id="2-2-anchors"><a href="#2-2-anchors" class="headerlink" title="2.2 anchors"></a>2.2 anchors</h3><p>提到RPN网络，就不能不说anchors。所谓anchors，实际上就是一组由rpn/generate_anchors.py生成的矩形。直接运行作者demo中的generate_anchors.py可以得到以下输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[[ -84.  -40.   99.   55.]</span><br><span class="line"> [-176.  -88.  191.  103.]</span><br><span class="line"> [-360. -184.  375.  199.]</span><br><span class="line"> [ -56.  -56.   71.   71.]</span><br><span class="line"> [-120. -120.  135.  135.]</span><br><span class="line"> [-248. -248.  263.  263.]</span><br><span class="line"> [ -36.  -80.   51.   95.]</span><br><span class="line"> [ -80. -168.   95.  183.]</span><br><span class="line"> [-168. -344.  183.  359.]]</span><br></pre></td></tr></table></figure>

<p>其中每行的4个值 <img src="E:\笔记\markdown\reference\picture\equation.svg+xml" alt="[公式]"> 表矩形左上和右下角点坐标。9个矩形共有3种形状，长宽比为大约为 <img src="E:\笔记\markdown\reference\picture}.svg+xml" alt="[公式]"> 三种，如图6。实际上通过anchors就引入了检测中常用到的多尺度方法。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-7abead97efcc46a3ee5b030a2151643f_720w.jpg" alt="img">图6 anchors示意图</p>
<p>注：关于上面的anchors size，其实是根据检测图像设置的。在python demo中，会把任意大小的输入图像reshape成800x600（即图2中的M=800，N=600）。再回头来看anchors的大小，anchors中长宽1:2中最大为352x704，长宽2:1中最大736x384，基本是cover了800x600的各个尺度和形状。</p>
<p>那么这9个anchors是做什么的呢？借用Faster RCNN论文中的原图，如图7，遍历Conv layers计算获得的feature maps，为每一个点都配备这9种anchors作为初始的检测框。这样做获得检测框很不准确，不用担心，后面还有2次bounding box regression可以修正检测框位置。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-c93db71cc8f4f4fd8cfb4ef2e2cef4f4_720w.jpg" alt="img">图7</p>
<p>解释一下上面这张图的数字。</p>
<ol>
<li>在原文中使用的是ZF model中，其Conv Layers中最后的conv5层num_output=256，对应生成256张特征图，所以相当于feature map每个点都是256-dimensions</li>
<li>在conv5之后，做了rpn_conv/3x3卷积且num_output=256，相当于每个点又融合了周围3x3的空间信息（猜测这样做也许更鲁棒？反正我没测试），同时256-d不变（如图4和图7中的红框）</li>
<li>假设在conv5 feature map中每个点上有k个anchor（默认k=9），而每个anhcor要分positive和negative，所以每个点由256d feature转化为cls=2•k scores；而每个anchor都有(x, y, w, h)对应4个偏移量，所以reg=4•k coordinates</li>
<li>补充一点，全部anchors拿去训练太多了，训练程序会在合适的anchors中<strong>随机</strong>选取128个postive anchors+128个negative anchors进行训练（什么是合适的anchors下文5.1有解释）</li>
</ol>
<p>注意，在本文讲解中使用的VGG conv5 num_output=512，所以是512d，其他类似。</p>
<p><strong>其实RPN最终就是在原图尺度上，设置了密密麻麻的候选Anchor。然后用cnn去判断哪些Anchor是里面有目标的positive anchor，哪些是没目标的negative anchor。所以，仅仅是个二分类而已！</strong></p>
<p>那么Anchor一共有多少个？原图800x600，VGG下采样16倍，feature map每个点设置9个Anchor，所以：</p>
<p><img src="E:\笔记\markdown\reference\picture\tag{1}.svg+xml" alt="[公式]"></p>
<p>其中ceil()表示向上取整，是因为VGG输出的feature map size= 50*38。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-4b15828dfee19be726835b671748cc4d_720w.jpg" alt="img">图8 Gernerate Anchors</p>
<h3 id="2-3-softmax判定positive与negative"><a href="#2-3-softmax判定positive与negative" class="headerlink" title="2.3 softmax判定positive与negative"></a>2.3 softmax判定positive与negative</h3><p>一副MxN大小的矩阵送入Faster RCNN网络后，到RPN网络变为(M/16)x(N/16)，不妨设 W=M/16，H=N/16。在进入reshape与softmax之前，先做了1x1卷积，如图9：</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-1ab4b6c3dd607a5035b5203c76b078f3_720w.jpg" alt="img">图9 RPN中判定positive/negative网络结构</p>
<p>该1x1卷积的caffe prototxt定义如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;rpn_cls_score&quot;</span><br><span class="line">  type: &quot;Convolution&quot;</span><br><span class="line">  bottom: &quot;rpn/output&quot;</span><br><span class="line">  top: &quot;rpn_cls_score&quot;</span><br><span class="line">  convolution_param &#123;</span><br><span class="line">    num_output: 18   # 2(positive/negative) * 9(anchors)</span><br><span class="line">    kernel_size: 1 pad: 0 stride: 1</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到其num_output=18，也就是经过该卷积的输出图像为WxHx18大小（注意第二章开头提到的卷积计算方式）。这也就刚好对应了feature maps每一个点都有9个anchors，同时每个anchors又有可能是positive和negative，所有这些信息都保存WxHx(9*2)大小的矩阵。为何这样做？后面接softmax分类获得positive anchors，也就相当于初步提取了检测目标候选区域box（一般认为目标在positive anchors中）。</p>
<p>那么为何要在softmax前后都接一个reshape layer？其实只是为了便于softmax分类，至于具体原因这就要从caffe的实现形式说起了。在caffe基本数据结构blob中以如下形式保存数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">blob=[batch_size, channel，height，width]</span><br></pre></td></tr></table></figure>

<p>对应至上面的保存positive/negative anchors的矩阵，其在caffe blob中的存储形式为[1, 2x9, H, W]。而在softmax分类时需要进行positive/negative二分类，所以reshape layer会将其变为[1, 2, 9xH, W]大小，即单独“腾空”出来一个维度以便softmax分类，之后再reshape回复原状。贴一段caffe softmax_loss_layer.cpp的reshape函数的解释，非常精辟：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;Number of labels must match number of predictions; &quot;</span></span><br><span class="line"><span class="string">&quot;e.g., if softmax axis == 1 and prediction shape is (N, C, H, W), &quot;</span></span><br><span class="line"><span class="string">&quot;label count (number of labels) must be N*H*W, &quot;</span></span><br><span class="line"><span class="string">&quot;with integer values in &#123;0, 1, ..., C-1&#125;.&quot;</span>;</span><br></pre></td></tr></table></figure>

<p>综上所述，RPN网络中利用anchors和softmax初步提取出positive anchors作为候选区域（另外也有实现用sigmoid代替softmax，输出[1, 1, 9xH, W]后接sigmoid进行positive/negative二分类，原理一样）。</p>
<h3 id="2-4-bounding-box-regression原理"><a href="#2-4-bounding-box-regression原理" class="headerlink" title="2.4 bounding box regression原理"></a>2.4 bounding box regression原理</h3><p>如图9所示绿色框为飞机的Ground Truth(GT)，红色为提取的positive anchors，即便红色的框被分类器识别为飞机，但是由于红色的框定位不准，这张图相当于没有正确的检测出飞机。所以我们希望采用一种方法对红色的框进行微调，使得positive anchors和GT更加接近。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-93021a3c03d66456150efa1da95416d3_720w.jpg" alt="img">图10</p>
<p>对于窗口一般使用四维向量 <img src="E:\笔记\markdown\reference\picture\equation-16504664534921.svg+xml" alt="[公式]"> 表示，分别表示窗口的中心点坐标和宽高。对于图 11，红色的框A代表原始的positive Anchors，绿色的框G代表目标的GT，我们的目标是寻找一种关系，使得输入原始的anchor A经过映射得到一个跟真实窗口G更接近的回归窗口G’，即：</p>
<ul>
<li>给定anchor <img src="E:\笔记\markdown\reference\picture\equation-16504664534922.svg+xml" alt="[公式]"> 和 <img src="E:\笔记\markdown\reference\picture\equation-16504664534933.svg+xml" alt="[公式]"></li>
<li>寻找一种变换<strong>F，</strong>使得：<img src="E:\笔记\markdown\reference\picture\equation-16504664534934.svg+xml" alt="[公式]">，其中<img src="E:\笔记\markdown\reference\picture\equation-16504664534935.svg+xml" alt="[公式]"></li>
</ul>
<p><img src="E:\笔记\markdown\reference\picture\v2-ea7e6e48662bfa68ec73bdf32f36bb85_720w.jpg" alt="img">图11</p>
<p>那么经过何种变换<strong>F</strong>才能从图10中的anchor A变为G’呢？ 比较简单的思路就是:</p>
<ul>
<li>先做平移</li>
</ul>
<p><img src="E:\笔记\markdown\reference\picture\tag{2}.svg+xml" alt="[公式]"></p>
<p><img src="E:\笔记\markdown\reference\picture\tag{3}.svg+xml" alt="[公式]"></p>
<ul>
<li>再做缩放</li>
</ul>
<p><img src="E:\笔记\markdown\reference\picture\tag{4}.svg+xml" alt="[公式]"></p>
<p><img src="E:\笔记\markdown\reference\picture\tag{5}.svg+xml" alt="[公式]"></p>
<p>观察上面4个公式发现，需要学习的是 <img src="E:\笔记\markdown\reference\picture\equation-16504664534936.svg+xml" alt="[公式]"> 这四个变换。当输入的anchor A与GT相差较小时，可以认为这种变换是一种线性变换， 那么就可以用线性回归来建模对窗口进行微调（注意，只有当anchors A和GT比较接近时，才能使用线性回归模型，否则就是复杂的非线性问题了）。</p>
<p>接下来的问题就是如何通过线性回归获得 <img src="E:\笔记\markdown\reference\picture\equation-16504664534936.svg+xml" alt="[公式]"> 了。线性回归就是给定输入的特征向量X, 学习一组参数W, 使得经过线性回归后的值跟真实值Y非常接近，即<img src="E:\笔记\markdown\reference\picture\equation-16504664534937.svg+xml" alt="[公式]">。对于该问题，输入X是cnn feature map，定义为Φ；同时还有训练传入A与GT之间的变换量，即<img src="E:\笔记\markdown\reference\picture\equation-16504664534938.svg+xml" alt="[公式]">。输出是<img src="E:\笔记\markdown\reference\picture\equation.svg+xml" alt="[公式]">四个变换。那么目标函数可以表示为：</p>
<p><img src="E:\笔记\markdown\reference\picture\tag{6}.svg+xml" alt="[公式]"></p>
<p>其中 <img src="E:\笔记\markdown\reference\picture\phi(A).svg+xml" alt="[公式]"> 是对应anchor的feature map组成的特征向量， <img src="E:\笔记\markdown\reference\picture\equation-16504664534949.svg+xml" alt="[公式]"> 是需要学习的参数， <img src="E:\笔记\markdown\reference\picture\equation-165046645349410.svg+xml" alt="[公式]"> 是得到的预测值（*表示 x，y，w，h，也就是每一个变换对应一个上述目标函数）。为了让预测值 <img src="E:\笔记\markdown\reference\picture\equation-16504669589211.svg+xml" alt="[公式]"> 与真实值 <img src="E:\笔记\markdown\reference\picture\equation-165046645349411.svg+xml" alt="[公式]"> 差距最小，设计L1损失函数：</p>
<p><img src="E:\笔记\markdown\reference\picture\tag{7}.svg+xml" alt="[公式]"></p>
<p>函数优化目标为：</p>
<p><img src="E:\笔记\markdown\reference\picture\tag{8}.svg+xml" alt="[公式]"></p>
<p>为了方便描述，这里以L1损失为例介绍，而真实情况中一般使用soomth-L1损失。</p>
<p>需要说明，只有在GT与需要回归框位置比较接近时，才可近似认为上述线性变换成立。<br>说完原理，对应于Faster RCNN原文，positive anchor与ground truth之间的平移量 <img src="E:\笔记\markdown\reference\picture\equation-165046645349412.svg+xml" alt="[公式]"> 与尺度因子 <img src="E:\笔记\markdown\reference\picture\equation-165046645349413.svg+xml" alt="[公式]"> 如下：</p>
<p><img src="E:\笔记\markdown\reference\picture\tag{9}.svg+xml" alt="[公式]"></p>
<p><img src="E:\笔记\markdown\reference\picture\tag{10}.svg+xml" alt="[公式]"></p>
<p>对于训练bouding box regression网络回归分支，输入是cnn feature Φ，监督信号是Anchor与GT的差距 <img src="E:\笔记\markdown\reference\picture\equation-165046645349414.svg+xml" alt="[公式]">，即训练目标是：输入 Φ的情况下使网络输出与监督信号尽可能接近。那么当bouding box regression工作时，再输入Φ时，回归网络分支的输出就是每个Anchor的平移量和变换尺度 <img src="E:\笔记\markdown\reference\picture\equation-16504669589212.svg+xml" alt="[公式]">，显然即可用来修正Anchor位置了。</p>
<h3 id="2-5-对proposals进行bounding-box-regression"><a href="#2-5-对proposals进行bounding-box-regression" class="headerlink" title="2.5 对proposals进行bounding box regression"></a>2.5 对proposals进行bounding box regression</h3><p>在了解bounding box regression后，再回头来看RPN网络第二条线路，如图12。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-8241c8076d60156248916fe2f1a5674a_720w.jpg" alt="img">图12 RPN中的bbox reg</p>
<p>先来看一看上图11中1x1卷积的caffe prototxt定义：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;rpn_bbox_pred&quot;</span><br><span class="line">  type: &quot;Convolution&quot;</span><br><span class="line">  bottom: &quot;rpn/output&quot;</span><br><span class="line">  top: &quot;rpn_bbox_pred&quot;</span><br><span class="line">  convolution_param &#123;</span><br><span class="line">    num_output: 36   # 4 * 9(anchors)</span><br><span class="line">    kernel_size: 1 pad: 0 stride: 1</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到其 num_output=36，即经过该卷积输出图像为WxHx36，在caffe blob存储为[1, 4x9, H, W]，这里相当于feature maps每个点都有9个anchors，每个anchors又都有4个用于回归的</p>
<p><img src="E:\笔记\markdown\reference\picture\tag{11}.svg+xml" alt="[公式]"></p>
<p>变换量。</p>
<p>回到图8，VGG输出 <img src="E:\笔记\markdown\reference\picture\equation-165046645349415.svg+xml" alt="[公式]"> 的特征，对应设置 <img src="E:\笔记\markdown\reference\picture\equation-165046645349416.svg+xml" alt="[公式]"> 个anchors，而RPN输出：</p>
<ol>
<li>大小为<img src="E:\笔记\markdown\reference\picture\equation-165046645349417.svg+xml" alt="[公式]"> 的positive/negative softmax分类特征矩阵</li>
<li>大小为 <img src="E:\笔记\markdown\reference\picture\equation-165046645349418.svg+xml" alt="[公式]"> 的regression坐标回归特征矩阵</li>
</ol>
<p>恰好满足RPN完成positive/negative分类+bounding box regression坐标回归.</p>
<h3 id="2-6-Proposal-Layer"><a href="#2-6-Proposal-Layer" class="headerlink" title="2.6 Proposal Layer"></a>2.6 Proposal Layer</h3><p>Proposal Layer负责综合所有 <img src="E:\笔记\markdown\reference\picture\equation-165046645349519.svg+xml" alt="[公式]"> 变换量和positive anchors，计算出精准的proposal，送入后续RoI Pooling Layer。还是先来看看Proposal Layer的caffe prototxt定义：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &#x27;proposal&#x27;</span><br><span class="line">  type: &#x27;Python&#x27;</span><br><span class="line">  bottom: &#x27;rpn_cls_prob_reshape&#x27;</span><br><span class="line">  bottom: &#x27;rpn_bbox_pred&#x27;</span><br><span class="line">  bottom: &#x27;im_info&#x27;</span><br><span class="line">  top: &#x27;rois&#x27;</span><br><span class="line">  python_param &#123;</span><br><span class="line">    module: &#x27;rpn.proposal_layer&#x27;</span><br><span class="line">    layer: &#x27;ProposalLayer&#x27;</span><br><span class="line">    param_str: &quot;&#x27;feat_stride&#x27;: 16&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Proposal Layer有3个输入：positive vs negative anchors分类器结果rpn_cls_prob_reshape，对应的bbox reg的 <img src="E:\笔记\markdown\reference\picture\equation-165046645349519.svg+xml" alt="[公式]"> 变换量rpn_bbox_pred，以及im_info；另外还有参数feat_stride=16，这和图4是对应的。</p>
<p>首先解释im_info。对于一副任意大小PxQ图像，传入Faster RCNN前首先reshape到固定MxN，im_info=[M, N, scale_factor]则保存了此次缩放的所有信息。然后经过Conv Layers，经过4次pooling变为WxH=(M/16)x(N/16)大小，其中feature_stride=16则保存了该信息，用于计算anchor偏移量。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-1e43500c7cc9a9de211d737bc347ced9_720w.jpg" alt="img">图13</p>
<p>Proposal Layer forward（caffe layer的前传函数）按照以下顺序依次处理：</p>
<ol>
<li>生成anchors，利用<img src="E:\笔记\markdown\reference\picture\equation-165046645349519.svg+xml" alt="[公式]">对所有的anchors做bbox regression回归（这里的anchors生成和训练时完全一致）</li>
<li>按照输入的positive softmax scores由大到小排序anchors，提取前pre_nms_topN(e.g. 6000)个anchors，即提取修正位置后的positive anchors</li>
<li>限定超出图像边界的positive anchors为图像边界，防止后续roi pooling时proposal超出图像边界（见文章底部QA部分图21）</li>
<li>剔除尺寸非常小的positive anchors</li>
<li>对剩余的positive anchors进行NMS（nonmaximum suppression）</li>
<li>Proposal Layer有3个输入：positive和negative anchors分类器结果rpn_cls_prob_reshape，对应的bbox reg的(e.g. 300)结果作为proposal输出</li>
</ol>
<p>之后输出proposal=[x1, y1, x2, y2]，注意，由于在第三步中将anchors映射回原图判断是否超出边界，所以这里输出的proposal是对应MxN输入图像尺度的，这点在后续网络中有用。另外我认为，严格意义上的检测应该到此就结束了，后续部分应该属于识别了。</p>
<p>RPN网络结构就介绍到这里，总结起来就是：<br><strong>生成anchors -&gt; softmax分类器提取positvie anchors -&gt; bbox reg回归positive anchors -&gt; Proposal Layer生成proposals</strong></p>
<h2 id="3-RoI-pooling"><a href="#3-RoI-pooling" class="headerlink" title="3 RoI pooling"></a>3 RoI pooling</h2><p>而RoI Pooling层则负责收集proposal，并计算出proposal feature maps，送入后续网络。从图2中可以看到Rol pooling层有2个输入：</p>
<ol>
<li>原始的feature maps</li>
<li>RPN输出的proposal boxes（大小各不相同）</li>
</ol>
<h3 id="3-1-为何需要RoI-Pooling"><a href="#3-1-为何需要RoI-Pooling" class="headerlink" title="3.1 为何需要RoI Pooling"></a>3.1 为何需要RoI Pooling</h3><p>先来看一个问题：对于传统的CNN（如AlexNet和VGG），当网络训练好后输入的图像尺寸必须是固定值，同时网络输出也是固定大小的vector or matrix。如果输入图像大小不定，这个问题就变得比较麻烦。有2种解决办法：</p>
<ol>
<li>从图像中crop一部分传入网络</li>
<li>将图像warp成需要的大小后传入网络</li>
</ol>
<p><img src="E:\笔记\markdown\reference\picture\v2-e525342cbde476a11c48a6be393f226c_720w.jpg" alt="img">图14 crop与warp破坏图像原有结构信息</p>
<p>两种办法的示意图如图14，可以看到无论采取那种办法都不好，要么crop后破坏了图像的完整结构，要么warp破坏了图像原始形状信息。</p>
<p>回忆RPN网络生成的proposals的方法：对positive anchors进行bounding box regression，那么这样获得的proposals也是大小形状各不相同，即也存在上述问题。所以Faster R-CNN中提出了RoI Pooling解决这个问题。不过RoI Pooling确实是从<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/1406.4729">Spatial Pyramid Pooling</a>发展而来，但是限于篇幅这里略去不讲，有兴趣的读者可以自行查阅相关论文。</p>
<h3 id="3-2-RoI-Pooling原理"><a href="#3-2-RoI-Pooling原理" class="headerlink" title="3.2 RoI Pooling原理"></a>3.2 RoI Pooling原理</h3><p>分析之前先来看看RoI Pooling Layer的caffe prototxt的定义：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;roi_pool5&quot;</span><br><span class="line">  type: &quot;ROIPooling&quot;</span><br><span class="line">  bottom: &quot;conv5_3&quot;</span><br><span class="line">  bottom: &quot;rois&quot;</span><br><span class="line">  top: &quot;pool5&quot;</span><br><span class="line">  roi_pooling_param &#123;</span><br><span class="line">    pooled_w: 7</span><br><span class="line">    pooled_h: 7</span><br><span class="line">    spatial_scale: 0.0625 # 1/16</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其中有新参数pooled_w和pooled_h，另外一个参数spatial_scale认真阅读的读者肯定已经知道知道用途。RoI Pooling layer forward过程：</p>
<ul>
<li>由于proposal是对应MxN尺度的，所以首先使用spatial_scale参数将其映射回(M/16)x(N/16)大小的feature map尺度；</li>
<li>再将每个proposal对应的feature map区域水平分为 <img src="E:\笔记\markdown\reference\picture\text{pooled_h}.svg+xml" alt="[公式]"> 的网格；</li>
<li>对网格的每一份都进行max pooling处理。</li>
</ul>
<p>这样处理后，即使大小不同的proposal输出结果都是 <img src="E:\笔记\markdown\reference\picture\text{pooled_h}.svg+xml" alt="[公式]"> 固定大小，实现了固定长度输出。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-e3108dc5cdd76b871e21a4cb64001b5c_720w.jpg" alt="img">图15 proposal示意图</p>
<h2 id="4-Classification"><a href="#4-Classification" class="headerlink" title="4 Classification"></a>4 Classification</h2><p>Classification部分利用已经获得的proposal feature maps，通过full connect层与softmax计算每个proposal具体属于那个类别（如人，车，电视等），输出cls_prob概率向量；同时再次利用bounding box regression获得每个proposal的位置偏移量bbox_pred，用于回归更加精确的目标检测框。Classification部分网络结构如图16。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-9377a45dc8393d546b7b52a491414ded_720w.jpg" alt="img">图16 Classification部分网络结构图</p>
<p>从RoI Pooling获取到7x7=49大小的proposal feature maps后，送入后续网络，可以看到做了如下2件事：</p>
<ol>
<li>通过全连接和softmax对proposals进行分类，这实际上已经是识别的范畴了</li>
<li>再次对proposals进行bounding box regression，获取更高精度的rect box</li>
</ol>
<p>这里来看看全连接层InnerProduct layers，简单的示意图如图17，</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-38594a97f33ff56fc72542a20a78116d_720w.jpg" alt="img">图17 全连接层示意图</p>
<p>其计算公式如下：</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-f56d3209f9a7d5f27d77ead7489ab70f_720w.jpg" alt="img"></p>
<p>其中W和bias B都是预先训练好的，即大小是固定的，当然输入X和输出Y也就是固定大小。所以，这也就印证了之前Roi Pooling的必要性。到这里，我想其他内容已经很容易理解，不在赘述了。</p>
<h2 id="5-Faster-RCNN训练"><a href="#5-Faster-RCNN训练" class="headerlink" title="5 Faster RCNN训练"></a>5 Faster RCNN训练</h2><p>Faster R-CNN的训练，是在已经训练好的model（如VGG_CNN_M_1024，VGG，ZF）的基础上继续进行训练。实际中训练过程分为6个步骤：</p>
<ol>
<li>在已经训练好的model上，训练RPN网络，对应stage1_rpn_train.pt</li>
<li>利用步骤1中训练好的RPN网络，收集proposals，对应rpn_test.pt</li>
<li>第一次训练Fast RCNN网络，对应stage1_fast_rcnn_train.pt</li>
<li>第二训练RPN网络，对应stage2_rpn_train.pt</li>
<li>再次利用步骤4中训练好的RPN网络，收集proposals，对应rpn_test.pt</li>
<li>第二次训练Fast RCNN网络，对应stage2_fast_rcnn_train.pt</li>
</ol>
<p>可以看到训练过程类似于一种“迭代”的过程，不过只循环了2次。至于只循环了2次的原因是应为作者提到：”A similar alternating training can be run for more iterations, but we have observed negligible improvements”，即循环更多次没有提升了。接下来本章以上述6个步骤讲解训练过程。</p>
<p>下面是一张训练过程流程图，应该更加清晰：</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-ddfcf3dc29976e384b047418aec9002d_720w.jpg" alt="img"></p>
<h3 id="5-1-训练RPN网络"><a href="#5-1-训练RPN网络" class="headerlink" title="5.1 训练RPN网络"></a>5.1 训练RPN网络</h3><p>在该步骤中，首先读取RBG提供的预训练好的model（本文使用VGG），开始迭代训练。来看看stage1_rpn_train.pt网络结构，如图19。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-c39aef1d06e08e4e0cec96b10f50a779_720w.jpg" alt="img">图19 stage1_rpn_train.pt（考虑图片大小，Conv Layers中所有的层都画在一起了，如红圈所示，后续图都如此处理）</p>
<p>与检测网络类似的是，依然使用Conv Layers提取feature maps。整个网络使用的Loss如下：</p>
<p><img src="E:\笔记\markdown\reference\picture\tag{12}.svg+xml" alt="[公式]"></p>
<p>上述公式中 <img src="E:\笔记\markdown\reference\picture\equation-165046645349520.svg+xml" alt="[公式]"> 表示anchors index， <img src="E:\笔记\markdown\reference\picture\equation-165046645349521.svg+xml" alt="[公式]"> 表示positive softmax probability，<img src="E:\笔记\markdown\reference\picture\equation-165046645349522.svg+xml" alt="[公式]">代表对应的GT predict概率（即当第i个anchor与GT间IoU&gt;0.7，认为是该anchor是positive，<img src="E:\笔记\markdown\reference\picture\equation-165046645349523.svg+xml" alt="[公式]">；反之IoU&lt;0.3时，认为是该anchor是negative，<img src="E:\笔记\markdown\reference\picture\equation-165046645349524.svg+xml" alt="[公式]">；至于那些0.3&lt;IoU&lt;0.7的anchor则不参与训练）；<img src="E:\笔记\markdown\reference\picture\equation-165046645349525.svg+xml" alt="[公式]">代表predict bounding box，<img src="E:\笔记\markdown\reference\picture\equation-165046645349526.svg+xml" alt="[公式]">代表对应positive anchor对应的GT box。可以看到，整个Loss分为2部分：</p>
<ol>
<li>cls loss，即rpn_cls_loss层计算的softmax loss，用于分类anchors为positive与negative的网络训练</li>
<li>reg loss，即rpn_loss_bbox层计算的soomth L1 loss，用于bounding box regression网络训练。注意在该loss中乘了 <img src="E:\笔记\markdown\reference\picture\equation-165046645349522.svg+xml" alt="[公式]"> ，相当于只关心positive anchors的回归（其实在回归中也完全没必要去关心negative）。</li>
</ol>
<p>由于在实际过程中，<img src="E:\笔记\markdown\reference\picture\text{cls}.svg+xml" alt="[公式]">和<img src="E:\笔记\markdown\reference\picture\text{reg}.svg+xml" alt="[公式]">差距过大，用参数λ平衡二者（如<img src="E:\笔记\markdown\reference\picture\text{cls}%3D256.svg+xml" alt="[公式]">，<img src="E:\笔记\markdown\reference\picture\text{reg}%3D2400.svg+xml" alt="[公式]">时设置 <img src="E:\笔记\markdown\reference\picture\approx10.svg+xml" alt="[公式]"> ），使总的网络Loss计算过程中能够均匀考虑2种Loss。这里比较重要是 <img src="E:\笔记\markdown\reference\picture\text{reg}-165046645349627.svg+xml" alt="[公式]"> 使用的soomth L1 loss，计算公式如下：</p>
<p><img src="E:\笔记\markdown\reference\picture\tag{13}.svg+xml" alt="[公式]"></p>
<p><img src="E:\笔记\markdown\reference\picture\tag{14}.svg+xml" alt="[公式]"></p>
<p>了解数学原理后，反过来看图18：</p>
<ol>
<li>在RPN训练阶段，rpn-data（python AnchorTargetLayer）层会按照和test阶段Proposal层完全一样的方式生成Anchors用于训练</li>
<li>对于rpn_loss_cls，输入的rpn_cls_scors_reshape和rpn_labels分别对应 <img src="E:\笔记\markdown\reference\picture\equation-165046645349628.svg+xml" alt="[公式]"> 与 <img src="E:\笔记\markdown\reference\picture\equation-165046645349629.svg+xml" alt="[公式]"> ， <img src="E:\笔记\markdown\reference\picture\text{cls}.svg+xml" alt="[公式]"> 参数隐含在<img src="E:\笔记\markdown\reference\picture\equation-16504669589223.svg+xml" alt="[公式]">与<img src="E:\笔记\markdown\reference\picture\equation-16504669589224.svg+xml" alt="[公式]">的caffe blob的大小中</li>
<li>对于rpn_loss_bbox，输入的rpn_bbox_pred和rpn_bbox_targets分别对应 <img src="E:\笔记\markdown\reference\picture\equation-165046645349525.svg+xml" alt="[公式]"> 与 <img src="E:\笔记\markdown\reference\picture\equation-165046645349526.svg+xml" alt="[公式]"> ，rpn_bbox_inside_weigths对应 <img src="E:\笔记\markdown\reference\picture\equation-16504669589224.svg+xml" alt="[公式]">，rpn_bbox_outside_weigths未用到（从smooth_L1_Loss layer代码中可以看到），而 <img src="E:\笔记\markdown\reference\picture\text{reg}.svg+xml" alt="[公式]"> 同样隐含在caffe blob大小中</li>
</ol>
<p>这样，公式与代码就完全对应了。特别需要注意的是，在训练和检测阶段生成和存储anchors的顺序完全一样，这样训练结果才能被用于检测！</p>
<h3 id="5-2-通过训练好的RPN网络收集proposals"><a href="#5-2-通过训练好的RPN网络收集proposals" class="headerlink" title="5.2 通过训练好的RPN网络收集proposals"></a>5.2 通过训练好的RPN网络收集proposals</h3><p>在该步骤中，利用之前的RPN网络，获取proposal rois，同时获取positive softmax probability，如图20，然后将获取的信息保存在python pickle文件中。该网络本质上和检测中的RPN网络一样，没有什么区别。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-1ac5f8a2899ee413464ecf7866f8f840_720w.jpg" alt="img">图20 rpn_test.pt</p>
<h3 id="5-3-训练Faster-RCNN网络"><a href="#5-3-训练Faster-RCNN网络" class="headerlink" title="5.3 训练Faster RCNN网络"></a>5.3 训练Faster RCNN网络</h3><p>读取之前保存的pickle文件，获取proposals与positive probability。从data层输入网络。然后：</p>
<ol>
<li>将提取的proposals作为rois传入网络，如图21蓝框</li>
<li>计算bbox_inside_weights+bbox_outside_weights，作用与RPN一样，传入soomth_L1_loss layer，如图21绿框</li>
</ol>
<p>这样就可以训练最后的识别softmax与最终的bounding box regression了。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-fbece817952865689187e68f0af86792_720w.jpg" alt="img">图21 stage1_fast_rcnn_train.pt</p>
<p>之后的stage2训练都是大同小异，不再赘述了。Faster R-CNN还有一种end-to-end的训练方式，可以一次完成train，有兴趣请自己看作者GitHub吧。</p>
<p><a href="https://link.zhihu.com/?target=https://github.com/rbgirshick/py-faster-rcnn">rbgirshick py-faster-rcnngithub.com/rbgirshick/py-faster-rcnn<img src="E:\笔记\markdown\reference\picture\v2-9f4e9c49a8e59e08abe70f8ba9b14fef_ipico.jpg" alt="img"></a></p>
<h2 id="QA"><a href="#QA" class="headerlink" title="QA"></a>QA</h2><p>此篇文章初次成文于2016年内部学习分享，再后来经多次修正和完善成为现在的样子。感谢大家一直以来的支持，现在总结常见疑问回答如下：</p>
<ul>
<li>为什么Anchor坐标中有负数</li>
</ul>
<p>回顾anchor生成步骤：首先生成9个base anchor，然后通过坐标偏移在 <img src="E:\笔记\markdown\reference\picture\equation-165046645349630.svg+xml" alt="[公式]"> 大小的 <img src="E:\笔记\markdown\reference\picture\frac{1}{16}.svg+xml" alt="[公式]"> 下采样FeatureMap每个点都放上这9个base anchor，就形成了 <img src="E:\笔记\markdown\reference\picture\equation-165046645349416.svg+xml" alt="[公式]"> 个anhcors。至于这9个base anchor坐标是什么其实并不重要，不同代码实现也许不同。</p>
<p>显然这里面有一部分边缘anchors会超出图像边界，而真实中不会有超出图像的目标，所以会有clip anchor步骤。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-9d67146e0cb10397d8c2170794412608_720w.jpg" alt="img">图21 clip anchor</p>
<ul>
<li>Anchor到底与网络输出如何对应</li>
</ul>
<p>VGG输出 <img src="E:\笔记\markdown\reference\picture\equation-165046645349415.svg+xml" alt="[公式]"> 的特征，对应设置 <img src="E:\笔记\markdown\reference\picture\equation-165046645349416.svg+xml" alt="[公式]"> 个anchors，而RPN输出 <img src="E:\笔记\markdown\reference\picture\equation-165046645349417.svg+xml" alt="[公式]"> 的分类特征矩阵和 <img src="E:\笔记\markdown\reference\picture\equation-165046645349418.svg+xml" alt="[公式]"> 的坐标回归特征矩阵。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-82196feb7b528d76411feb90bfec2af4_720w.jpg" alt="img">图22 anchor与网络输出如何对应方式</p>
<p>其实在实现过程中，每个点的 <img src="E:\笔记\markdown\reference\picture\equation-165046645349731.svg+xml" alt="[公式]"> 个分类特征与 <img src="E:\笔记\markdown\reference\picture\equation-165046645349732.svg+xml" alt="[公式]"> 回归特征，与 <img src="E:\笔记\markdown\reference\picture\equation-165046645349733.svg+xml" alt="[公式]"> 个anchor逐个对应即可，这实际是一种“人为设置的逻辑映射”。当然，也可以不这样设置，但是无论如何都需要保证<strong>在训练和测试过程中映射方式必须一致</strong>。</p>
<ul>
<li>为何有ROI Pooling还要把输入图片resize到固定大小的MxN</li>
</ul>
<p>由于引入ROI Pooling，从原理上说Faster R-CNN确实能够检测任意大小的图片。但是由于在训练的时候需要使用大batch训练网络，而不同大小输入拼batch在实现的时候代码较为复杂，而且当时以Caffe为代表的第一代深度学习框架也不如Tensorflow和PyTorch灵活，所以作者选择了把输入图片resize到固定大小的800x600。这应该算是历史遗留问题。</p>
<p>另外很多问题，都是属于具体实现问题，真诚的建议读者阅读代码自行理解。</p>
<h2 id="拓展"><a href="#拓展" class="headerlink" title="拓展"></a>拓展</h2><p>关于torchvision中的FasterRCNN代码：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/145842317">捋一捋pytorch官方FasterRCNN代码1308 赞同 · 21 评论文章<img src="E:\笔记\markdown\reference\picture\v2-dc78d0ad1012ad65040bdb7eb657f381_180x120.jpg" alt="img"></a></p>
<p>Faster RCNN在文字检测中的应用：CTPN</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/34757009">场景文字检测—CTPN原理与实现678 赞同 · 54 评论文章<img src="E:\笔记\markdown\reference\picture\v2-2ea98126ebc05e35be28efe598a021ed_180x120.jpg" alt="img"></a></p>
<p>Faster RCNN在高德导航中的应用：</p>
<p><a href="https://link.zhihu.com/?target=https://mp.weixin.qq.com/s/IJUMCOBhgXHv7VC1YT4q_g">机器学习在交通标志检测与精细分类中的应用mp.weixin.qq.com/s/IJUMCOBhgXHv7VC1YT4q_g<img src="E:\笔记\markdown\reference\picture\v2-b7f1350a3179674d93d60d8ad9b52c0f_180x120.jpg" alt="img"></a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-无监督学习" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2022/04/20/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" class="article-date">
      <time datetime="2022-04-20T14:38:48.702Z" itemprop="datePublished">2022-04-20</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h1 id="The-Illustrated-Self-Supervised-Learning"><a href="#The-Illustrated-Self-Supervised-Learning" class="headerlink" title="The Illustrated Self-Supervised Learning"></a><a target="_blank" rel="noopener" href="https://amitness.com/2020/02/illustrated-self-supervised-learning/">The Illustrated Self-Supervised Learning</a></h1><p>I first got introduced to self-supervised learning in a <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=7I0Qt7GALVk&t=2639s">talk</a> by Yann Lecun, where he introduced the “cake analogy” to illustrate the importance of self-supervised learning. In the talk, he said:</p>
<blockquote>
<p>“If intelligence is a cake, the bulk of the cake is self-supervised learning, the icing on the cake is supervised learning, and the cherry on the cake is reinforcement learning (RL).”</p>
</blockquote>
<p>Though the analogy is <a target="_blank" rel="noopener" href="https://www.dropbox.com/s/fdw7q8mx3x4wr0c/2017_12_xx_NIPS-keynote-final.pdf?dl=0">debated</a>, we have seen the impact of self-supervised learning in the Natural Language Processing field where recent developments (Word2Vec, Glove, ELMO, BERT) have embraced self-supervision and achieved state of the art results.</p>
<p><img src="E:\笔记\markdown\reference\picture\self-supervised-nlp-to-vision-16504267629161.png" alt="img"></p>
<p>Curious to know the current state of self-supervised learning in the Computer Vision field, I read up on existing literature on self-supervised learning applied to computer vision through a <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.06162">recent survey paper</a> by Jing et. al.</p>
<p>In this post, I will explain what is self-supervised learning and summarize the patterns of problem formulation being used in self-supervised learning with visualizations.</p>
<h2 id="Why-Self-Supervised-Learning"><a href="#Why-Self-Supervised-Learning" class="headerlink" title="Why Self-Supervised Learning?"></a>Why Self-Supervised Learning?</h2><p>To apply supervised learning with deep neural networks, we need enough labeled data. To acquire that, human annotators manually label data which is both a time consuming and expensive process. There are also fields such as the medical field where getting enough data is a challenge itself. Thus, a major bottleneck in current supervised learning paradigm is the label generation part.</p>
<p><img src="E:\笔记\markdown\reference\picture\supervised-manual-annotation-16504267629163.png" alt="Manual Annotation in Supervised Learning"></p>
<h2 id="What-is-Self-Supervised-Learning"><a href="#What-is-Self-Supervised-Learning" class="headerlink" title="What is Self-Supervised Learning?"></a>What is Self-Supervised Learning?</h2><p>Self supervised learning is a method that poses the following question to formulate an unsupervised learning problem as a supervised one:</p>
<blockquote>
<p>Can we design the task in such a way that we can generate virtually unlimited labels from our existing images and use that to learn the representations?</p>
</blockquote>
<p><img src="E:\笔记\markdown\reference\picture\supervised-automated-16504267629165.png" alt="Automating manual labeling with Self Supervised Learning"></p>
<p>In self-supervised learning, we replace the human annotation block by creatively exploiting some property of data to set up a pseudo-supervised task. For example, here instead of labeling images as cat/dog, we could instead rotate them by 0/90/180/270 degrees and train a model to predict rotation. We can generate virtually unlimited training data from millions of images we have freely available on the internet.</p>
<p><img src="E:\笔记\markdown\reference\picture\self-supervised-workflow-16504267629167.png" alt="Self-supervised Learning Workflow Diagram"></p>
<p>Figure: End to End Workflow of Self-Supervised Learning</p>
<p>Once we learn representations from these millions of images, we can use transfer learning to fine-tune it on some supervised task like image classification of cats vs dogs with very few examples.</p>
<p><img src="E:\笔记\markdown\reference\picture\self-supervised-finetuning-16504267629169.png" alt="img"></p>
<h2 id="Survey-of-Self-Supervised-Learning-Methods"><a href="#Survey-of-Self-Supervised-Learning-Methods" class="headerlink" title="Survey of Self-Supervised Learning Methods"></a>Survey of Self-Supervised Learning Methods</h2><p>Let’s now understand the various approaches researchers have proposed to exploit image and video properties and apply self-supervised learning for representation learning.</p>
<h3 id="A-Self-Supervised-Learning-from-Image"><a href="#A-Self-Supervised-Learning-from-Image" class="headerlink" title="A. Self-Supervised Learning from Image"></a>A. Self-Supervised Learning from Image</h3><h4 id="Pattern-1-Reconstruction"><a href="#Pattern-1-Reconstruction" class="headerlink" title="Pattern 1: Reconstruction"></a>Pattern 1: Reconstruction</h4><h5 id="1-Image-Colorization"><a href="#1-Image-Colorization" class="headerlink" title="1. Image Colorization"></a>1. <a target="_blank" rel="noopener" href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#1-image-colorization"><strong>Image Colorization</strong></a></h5><p>Formulation:</p>
<blockquote>
<p>What if we prepared pairs of (grayscale, colorized) images by applying grayscale to millions of images we have freely available?</p>
</blockquote>
<p><img src="E:\笔记\markdown\reference\picture\ss-colorization-data-gen-165042676291611.png" alt="Data Generation for Image Colorization"></p>
<p>We could use an encoder-decoder architecture based on a fully convolutional neural network and compute the L2 loss between the predicted and actual color images.</p>
<p><img src="E:\笔记\markdown\reference\picture\ss-image-colorization-165042676291613.png" alt="Architecture for Image Colorization"></p>
<p>To solve this task, the model has to learn about different objects present in the image and related parts so that it can paint those parts in the same color. Thus, representations learned are useful for downstream tasks.<img src="E:\笔记\markdown\reference\picture\ss-colorization-learning-165042676291615.png" alt="Learning to colorize images"></p>
<p><strong>Papers:</strong><br>        <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1603.08511">Colorful Image Colorization</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1705.02999">Real-Time User-Guided Image Colorization with Learned Deep Priors</a> | <a target="_blank" rel="noopener" href="http://iizuka.cs.tsukuba.ac.jp/projects/colorization/en/">Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification</a></p>
<h5 id="2-Image-Superresolution"><a href="#2-Image-Superresolution" class="headerlink" title="2. Image Superresolution"></a>2. <a target="_blank" rel="noopener" href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#2-image-superresolution"><strong>Image Superresolution</strong></a></h5><p>Formulation:</p>
<blockquote>
<p>What if we prepared training pairs of (small, upscaled) images by downsampling millions of images we have freely available?</p>
</blockquote>
<p><img src="E:\笔记\markdown\reference\picture\ss-superresolution-training-gen-165042676291617.png" alt="Training Data for Superresolution"></p>
<p>GAN based models such as <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1609.04802">SRGAN</a> are popular for this task. A generator takes a low-resolution image and outputs a high-resolution image using a fully convolutional network. The actual and generated images are compared using both mean-squared-error and content loss to imitate human-like quality comparison. A binary-classification discriminator takes an image and classifies whether it’s an actual high-resolution image(1) or a fake generated superresolution image(0). This interplay between the two models leads to generator learning to produce images with fine details.</p>
<p><img src="E:\笔记\markdown\reference\picture\ss-srgan-architecture-165042676291619.png" alt="Architecture for SRGAN"></p>
<p>Both generator and discriminator learn semantic features that can be used for downstream tasks.</p>
<p><strong>Papers</strong>:<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1609.04802">Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</a></p>
<h5 id="3-Image-Inpainting"><a href="#3-Image-Inpainting" class="headerlink" title="3. Image Inpainting"></a>3. <a target="_blank" rel="noopener" href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#3-image-inpainting"><strong>Image Inpainting</strong></a></h5><p>Formulation:</p>
<blockquote>
<p>What if we prepared training pairs of (corrupted, fixed) images by randomly removing part of images?</p>
</blockquote>
<p><img src="E:\笔记\markdown\reference\picture\ss-image-inpainting-data-gen-165042676291621.png" alt="Training Data for Image Inpainting"></p>
<p>Similar to superresolution, we can leverage a GAN-based architecture where the Generator can learn to reconstruct the image while discriminator separates real and generated images.</p>
<p><img src="E:\笔记\markdown\reference\picture\ss-inpainting-architecture-165042676291723.png" alt="Architecture for Image Inpainting"></p>
<p>For downstream tasks, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1604.07379">Pathak et al.</a> have shown that semantic features learned by such a generator give 10.2% improvement over random initialization on the <a target="_blank" rel="noopener" href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html">PASCAL VOC 2012</a> semantic segmentation challenge while giving &lt;4% improvements over classification and object detection.</p>
<p><strong>Papers</strong>:<br>        <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1604.07379">Context encoders: Feature learning by inpainting</a></p>
<h5 id="4-Cross-Channel-Prediction"><a href="#4-Cross-Channel-Prediction" class="headerlink" title="4. Cross-Channel Prediction"></a>4. <a target="_blank" rel="noopener" href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#4-cross-channel-prediction"><strong>Cross-Channel Prediction</strong></a></h5><p>Formulation:</p>
<blockquote>
<p>What if we predict one channel of the image from the other channel and combine them to reconstruct the original image?</p>
</blockquote>
<p>Zhang et al. used this idea in their paper called “Split-Brain Autoencoder”. To understand the idea of the paper, let’s take an example of a color image of tomato.</p>
<p><img src="E:\笔记\markdown\reference\picture\split-brain-autoencoder-165042676291725.png" alt="img"></p>
<p>Example adapted from “Split-Brain Autoencoder” paper</p>
<p>For this color image, we can split it into grayscale and color channels. Then, for the grayscale channel, we predict the color channel and for the color channel part, we predict the grayscale channel. The two predicted channels $X_1$ and $X_2$ are combined to get back a reconstruction of the original image. We can compare this reconstruction to the original color image to get a loss and improve the model.</p>
<p>This same setup can be applied for images with depth as well where we use the color channels and the depth channels from a RGB-HHA image to predict each other and compare output image and original image.</p>
<p><img src="E:\笔记\markdown\reference\picture\split-brain-autoencoder-rgbhha-165042676291727.png" alt="img"></p>
<p>Example adapted from “Split-Brain Autoencoder” paper</p>
<p><strong>Papers</strong>:<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.09842">Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction</a></p>
<h4 id="Pattern-2-Common-Sense-Tasks"><a href="#Pattern-2-Common-Sense-Tasks" class="headerlink" title="Pattern 2: Common Sense Tasks"></a>Pattern 2: Common Sense Tasks</h4><h5 id="1-Image-Jigsaw-Puzzle"><a href="#1-Image-Jigsaw-Puzzle" class="headerlink" title="1. Image Jigsaw Puzzle"></a>1. <a target="_blank" rel="noopener" href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#1-image-jigsaw-puzzle"><strong>Image Jigsaw Puzzle</strong></a></h5><p>Formulation:</p>
<blockquote>
<p>What if we prepared training pairs of (shuffled, ordered) puzzles by randomly shuffling patches of images?</p>
</blockquote>
<p><img src="E:\笔记\markdown\reference\picture\ss-image-jigsaw-data-165042676291729.png" alt="Training Data For Image Jigsaw Puzzle"></p>
<p>Even with only 9 patches, there can be 362880 possible puzzles. To overcome this, only a subset of possible permutations is used such as 64 permutations with the highest hamming distance.</p>
<p><img src="E:\笔记\markdown\reference\picture\ss-jigsaw-permutations-165042676291731.png" alt="Number of Permutations in Image Jigsaw"></p>
<p>Suppose we use a permutation that changes the image as shown below. Let’s use the permutation number 64 from our total available 64 permutations.</p>
<p><img src="E:\笔记\markdown\reference\picture\ss-jigsaw-permutation-64-165042676291733.png" alt="Example of single permutation in jigsaw"></p>
<p>Now, to recover back the original patches, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1603.09246">Noroozi et al.</a> proposed a neural network called context-free network (CFN) as shown below. Here, the individual patches are passed through the same siamese convolutional layers that have shared weights. Then, the features are combined in a fully-connected layer. In the output, the model has to predict which permutation was used from the 64 possible classes. If we know the permutation, we can solve the puzzle.</p>
<p><img src="E:\笔记\markdown\reference\picture\ss-jigsaw-architecture-165042676291735.png" alt="Architecture for Image Jigsaw Task"></p>
<p>To solve the Jigsaw puzzle, the model needs to learn to identify how parts are assembled in an object, relative positions of different parts of objects and shape of objects. Thus, the representations are useful for downstream tasks in classification and detection.</p>
<p><strong>Papers</strong>:<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1603.09246">Unsupervised learning of visual representations by solving jigsaw puzzles</a></p>
<h5 id="2-Context-Prediction"><a href="#2-Context-Prediction" class="headerlink" title="2. Context Prediction"></a>2. <a target="_blank" rel="noopener" href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#2-context-prediction"><strong>Context Prediction</strong></a></h5><p>Formulation:</p>
<blockquote>
<p>What if we prepared training pairs of (image-patch, neighbor) by randomly taking an image patch and one of its neighbors around it from large, unlabeled image collection?</p>
</blockquote>
<p><img src="E:\笔记\markdown\reference\picture\ss-context-prediction-gen-165042676291737.png" alt="Training Data for Context Prediction"></p>
<p>To solve this pre-text task, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1505.05192">Doersch et al.</a> used an architecture similar to that of a jigsaw puzzle. We pass the patches through two siamese ConvNets to extract features, concatenate the features and do a classification over 8 classes denoting the 8 possible neighbor positions.</p>
<p><img src="E:\笔记\markdown\reference\picture\ss-context-prediction-architecture-165042676291739.png" alt="Architecture for Context Prediction"></p>
<p><strong>Papers</strong>:<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1505.05192">Unsupervised Visual Representation Learning by Context Prediction</a></p>
<h5 id="3-Geometric-Transformation-Recognition"><a href="#3-Geometric-Transformation-Recognition" class="headerlink" title="3. Geometric Transformation Recognition"></a>3. <a target="_blank" rel="noopener" href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#3-geometric-transformation-recognition"><strong>Geometric Transformation Recognition</strong></a></h5><p>Formulation:</p>
<blockquote>
<p>What if we prepared training pairs of (rotated-image, rotation-angle) by randomly rotating images by (0, 90, 180, 270) from large, unlabeled image collection?</p>
</blockquote>
<p><img src="E:\笔记\markdown\reference\picture\ss-geometric-transformation-gen-165042676291741.png" alt="Training Data for Geometric Transformation"></p>
<p>To solve this pre-text task, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.07728">Gidaris et al.</a> propose an architecture where a rotated image is passed through a ConvNet and the network has to classify it into 4 classes(0/90/270/360 degrees).<img src="E:\笔记\markdown\reference\picture\ss-geometric-transformation-architecture-165042676291743.png" alt="Architecture for Geometric Transformation Predction"></p>
<p>Though a very simple idea, the model has to understand location, types and pose of objects in an image to solve this task and as such, the representations learned are useful for downstream tasks.</p>
<p><strong>Papers</strong>:<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.07728">Unsupervised Representation Learning by Predicting Image Rotations</a></p>
<h4 id="Pattern-3-Automatic-Label-Generation"><a href="#Pattern-3-Automatic-Label-Generation" class="headerlink" title="Pattern 3: Automatic Label Generation"></a>Pattern 3: Automatic Label Generation</h4><h5 id="1-Image-Clustering"><a href="#1-Image-Clustering" class="headerlink" title="1. Image Clustering"></a>1. <a target="_blank" rel="noopener" href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#1-image-clustering"><strong>Image Clustering</strong></a></h5><p>Formulation:</p>
<blockquote>
<p>What if we prepared training pairs of (image, cluster-number) by performing clustering on large, unlabeled image collection?</p>
</blockquote>
<p><img src="E:\笔记\markdown\reference\picture\ss-image-clustering-gen-165042676291845.png" alt="Training Data for Image Clustering"></p>
<p>To solve this pre-text task, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.05520">Caron et al.</a> propose an architecture called deep clustering. Here, the images are first clustered and the clusters are used as classes. The task of the ConvNet is to predict the cluster label for an input image.<img src="E:\笔记\markdown\reference\picture\ss-deep-clustering-architecture-165042676291847.png" alt="Architecture for Deep Clustering"></p>
<p><strong>Papers</strong>:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://amitness.com/2020/04/deepcluster/">Deep clustering for unsupervised learning of visual features</a></li>
<li><a target="_blank" rel="noopener" href="https://amitness.com/2020/04/illustrated-self-labelling/">Self-labelling via simultaneous clustering and representation learning</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1608.08792">CliqueCNN: Deep Unsupervised Exemplar Learning</a></li>
</ul>
<h5 id="2-Synthetic-Imagery"><a href="#2-Synthetic-Imagery" class="headerlink" title="2. Synthetic Imagery"></a>2. <a target="_blank" rel="noopener" href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#2-synthetic-imagery"><strong>Synthetic Imagery</strong></a></h5><p>Formulation:</p>
<blockquote>
<p>What if we prepared training pairs of (image, properties) by generating synthetic images using game engines and adapting it to real images?</p>
</blockquote>
<p><img src="E:\笔记\markdown\reference\picture\synthetic-imagery-data-165042676291849.png" alt="Training Data for Sythetic Imagery"></p>
<p>To solve this pre-text task, <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1711.09082.pdf">Ren et al.</a> propose an architecture where weight-shared ConvNets are trained on both synthetic and real images and then a discriminator learns to classify whether ConvNet features fed to it is of a synthetic image or a real image. Due to adversarial nature, the shared representations between real and synthetic images get better.<img src="E:\笔记\markdown\reference\picture\ss-synthetic-image-architecture-165042676291851.png" alt="Architecture for Synthetic Image Training"></p>
<h3 id="B-Self-Supervised-Learning-From-Video"><a href="#B-Self-Supervised-Learning-From-Video" class="headerlink" title="B. Self-Supervised Learning From Video"></a>B. Self-Supervised Learning From Video</h3><h4 id="1-Frame-Order-Verification"><a href="#1-Frame-Order-Verification" class="headerlink" title="1. Frame Order Verification"></a>1. <a target="_blank" rel="noopener" href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#1-frame-order-verification"><strong>Frame Order Verification</strong></a></h4><p>Formulation:</p>
<blockquote>
<p>What if we prepared training pairs of (video frames, correct/incorrect order) by shuffling frames from videos of objects in motion?</p>
</blockquote>
<p><img src="E:\笔记\markdown\reference\picture\ss-frame-order-data-gen-165042676291853.png" alt="Training Data for Video Order"></p>
<p>To solve this pre-text task, <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1711.09082.pdf">Misra et al.</a> propose an architecture where video frames are passed through weight-shared ConvNets and the model has to figure out whether the frames are in the correct order or not. In doing so, the model learns not just spatial features but also takes into account temporal features.</p>
<p><img src="E:\笔记\markdown\reference\picture\ss-temporal-order-architecture-165042676291855.png" alt="Architecture for Frame Order Verification"></p>
<p><strong>Papers</strong>:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1603.08561">Shuffle and Learn: Unsupervised Learning using Temporal Order Verification</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.06646">Self-Supervised Video Representation Learning With Odd-One-Out Networks</a></li>
</ul>
<h2 id="Citation-Info-BibTex"><a href="#Citation-Info-BibTex" class="headerlink" title="Citation Info (BibTex)"></a><a target="_blank" rel="noopener" href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#citation-info-bibtex">Citation Info (BibTex)</a></h2><p>If you found this blog post useful, please consider citing it as:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@misc&#123;chaudhary2020selfsupervised,</span><br><span class="line">  title   = &#123;The Illustrated Self-Supervised Learning&#125;,</span><br><span class="line">  author  = &#123;Amit Chaudhary&#125;,</span><br><span class="line">  year    = 2020,</span><br><span class="line">  note    = &#123;\url&#123;https://amitness.com/2020/02/illustrated-self-supervised-learning&#125;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="References"><a href="#References" class="headerlink" title="References"></a><a target="_blank" rel="noopener" href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#references">References</a></h2><ul>
<li>Jing, et al. “<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.06162">Self-Supervised Visual Feature Learning with Deep Neural Networks: A Survey.</a>”</li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-自监督" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2022/04/20/%E8%87%AA%E7%9B%91%E7%9D%A3/" class="article-date">
      <time datetime="2022-04-20T03:52:22.314Z" itemprop="datePublished">2022-04-20</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-图像的操作" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2022/04/11/%E5%9B%BE%E5%83%8F%E7%9A%84%E6%93%8D%E4%BD%9C/" class="article-date">
      <time datetime="2022-04-11T02:32:32.733Z" itemprop="datePublished">2022-04-11</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h2 id="PIL"><a href="#PIL" class="headerlink" title="PIL"></a>PIL</h2><blockquote>
<p>  PIL，全称 Python Imaging Library，是一个功能非常强大而且简单易用的图像处理库,已经是Python平台事实上的图像处理标准库了。PIL功能非常强大，但API却非常简单易用。但是，由于 PIL 仅支持到Python 2.7，加上年久失修，于是一群志愿者在 PIL 的基础上创建了兼容 Python 3 的版本，名字叫 Pillow ，我们可以通过安装 Pillow 来使用 PIL。</p>
</blockquote>
<h3 id="1-pip-安装-pillow"><a href="#1-pip-安装-pillow" class="headerlink" title="1. pip 安装 pillow"></a>1. pip 安装 pillow</h3><p>在 Ubuntu 下通过一个简单的命令<code>pip3 install pillow</code>即可成功安装库。</p>
<p>如果遇到<code>Permission denied</code>安装失败，请加上<code>sudo</code>重试。</p>
<h3 id="2-打开、保存、显示图片"><a href="#2-打开、保存、显示图片" class="headerlink" title="2. 打开、保存、显示图片"></a>2. 打开、保存、显示图片</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">image = Image.<span class="built_in">open</span>(<span class="string">&#x27;cat.jpg&#x27;</span>)</span><br><span class="line">image.show()</span><br><span class="line">image.save(<span class="string">&#x27;1.jpg&#x27;</span>,<span class="string">&#x27;jpeg&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(image.mode, image.size, image.<span class="built_in">format</span>)</span><br><span class="line"><span class="comment"># RGB (481, 321) JPEG</span></span><br></pre></td></tr></table></figure>

<ul>
<li>  mode 属性为图片的模式，RGB 代表彩色图像，L 代表光照图像也即灰度图像等</li>
<li>  size 属性为图片的大小(宽度，长度)</li>
<li>  format 属性为图片的格式，如常见的 PNG、JPEG 等</li>
</ul>
<h3 id="3-转换图片模式"><a href="#3-转换图片模式" class="headerlink" title="3. 转换图片模式"></a>3. 转换图片模式</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">image.show()</span><br><span class="line">grey_image = image.convert(<span class="string">&#x27;L&#x27;</span>)</span><br><span class="line">grey_image.show()</span><br></pre></td></tr></table></figure>



<p><img src="E:\笔记\markdown\reference\picture\v2-fa0ae3e48ef7a9645c252b49c16fce06_720w.jpg" alt="img"></p>
<p><img src="E:\笔记\markdown\reference\picture\v2-0e23c82abff1c3673d55358c7e6f1155_720w.jpg" alt="img"></p>
<ul>
<li>  任何支持的图片模式都可以直接转为彩色模式或者灰度模式，但是，若是想转化为其他模式，则需要借助一个中间模式（通常是彩色）来进行过转</li>
</ul>
<h3 id="4-通道分离合并"><a href="#4-通道分离合并" class="headerlink" title="4. 通道分离合并"></a>4. 通道分离合并</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">r, g, b = image.split()</span><br><span class="line">im = Image.merge(<span class="string">&#x27;RGB&#x27;</span>, (b, g, r))</span><br></pre></td></tr></table></figure>

<ul>
<li>  彩色图像可以分离出 R、G、B 通道，但若是灰度图像，则返回灰度图像本身。然后，可以将 R、G、B 通道按照一定的顺序再合并成彩色图像。</li>
</ul>
<h3 id="5-图片裁剪、旋转和改变大小"><a href="#5-图片裁剪、旋转和改变大小" class="headerlink" title="5. 图片裁剪、旋转和改变大小"></a>5. 图片裁剪、旋转和改变大小</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">box = (<span class="number">100</span>, <span class="number">100</span>, <span class="number">300</span>, <span class="number">300</span>)</span><br><span class="line">region = image.crop(box)</span><br><span class="line">region = region.transpose(Image.ROTATE_180)</span><br><span class="line">image.paste(region, box)</span><br><span class="line">image.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image, ImageFilter</span><br><span class="line"><span class="comment"># 打开一个jpg图像文件，注意是当前路径:</span></span><br><span class="line">im = Image.<span class="built_in">open</span>(<span class="string">&#x27;test.jpg&#x27;</span>)</span><br><span class="line"><span class="comment"># 应用模糊滤镜:</span></span><br><span class="line">im2 = im.<span class="built_in">filter</span>(ImageFilter.BLUR)</span><br></pre></td></tr></table></figure>

<p><img src="E:\笔记\markdown\reference\picture\v2-6eb679436bfcefcf51d57c95d5e1ef10_720w.jpg" alt="img"></p>
<ul>
<li>  通过定义一个 4 元组，依次为左上角 X 坐标、Y 坐标，右下角 X 坐标、Y 坐标，可以対原图片的某一区域进行裁剪，然后进行一定处理后可以在原位置粘贴回去。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">im = image.resize((300, 300))</span><br><span class="line">im = image.rotate(45)  # 逆时针旋转 45 度</span><br><span class="line">im = image.transpose(Image.FLIP_LEFT_RIGHT) # 左右翻转</span><br><span class="line">im = im.transpose(Image.FLIP_TOP_BOTTOM)# 上下翻转</span><br><span class="line"># 缩放到50%:</span><br><span class="line">im.thumbnail((w//2, h//2))</span><br></pre></td></tr></table></figure>

<h3 id="6-像素值操作"><a href="#6-像素值操作" class="headerlink" title="6. 像素值操作"></a>6. 像素值操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">out = image.point(<span class="keyword">lambda</span> i: i * <span class="number">1.2</span>) <span class="comment"># 对每个像素值乘以 1.2</span></span><br><span class="line">source = image.split()</span><br><span class="line">out = source[<span class="number">0</span>].point(<span class="keyword">lambda</span> i: i &gt; <span class="number">128</span> <span class="keyword">and</span> <span class="number">255</span>) <span class="comment"># 对 R 通道进行二值化</span></span><br></pre></td></tr></table></figure>



<p><img src="E:\笔记\markdown\reference\picture\v2-8317adc4f04515dec04330c85bd332c7_720w.jpg" alt="img"></p>
<ul>
<li>  i &gt; 128 and 255，当 i &lt;= 128 时，返回 False 也即 0,；反之返回 255 。</li>
</ul>
<h3 id="7-和-Numpy-数组之间的转化"><a href="#7-和-Numpy-数组之间的转化" class="headerlink" title="7. 和 Numpy 数组之间的转化"></a>7. 和 Numpy 数组之间的转化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">array = np.array(image)</span><br><span class="line"><span class="built_in">print</span>(array.shape) <span class="comment">#(321, 481, 3)</span></span><br><span class="line">image = Image.fromarray(array)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#tesnor 转pil</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">image = x.cpu().clone()</span><br><span class="line">image = image.squeeze(<span class="number">0</span>)  <span class="comment"># 压缩一维</span></span><br><span class="line">image = transforms.ToPILImage()(image)  <span class="comment"># 自动转换为0-255</span></span><br><span class="line"><span class="comment"># image.show()</span></span><br><span class="line">file = <span class="string">&quot;./pic/&quot;</span> + <span class="built_in">str</span>(step) + <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">Image.Image.save(image, file)</span><br></pre></td></tr></table></figure>

<h3 id="8-绘图"><a href="#8-绘图" class="headerlink" title="8.绘图"></a>8.绘图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image, ImageDraw, ImageFont, ImageFilter</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机字母:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rndChar</span>():</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">chr</span>(random.randint(<span class="number">65</span>, <span class="number">90</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机颜色1:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rndColor</span>():</span></span><br><span class="line">    <span class="keyword">return</span> (random.randint(<span class="number">64</span>, <span class="number">255</span>), random.randint(<span class="number">64</span>, <span class="number">255</span>), random.randint(<span class="number">64</span>, <span class="number">255</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机颜色2:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rndColor2</span>():</span></span><br><span class="line">    <span class="keyword">return</span> (random.randint(<span class="number">32</span>, <span class="number">127</span>), random.randint(<span class="number">32</span>, <span class="number">127</span>), random.randint(<span class="number">32</span>, <span class="number">127</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 240 x 60:</span></span><br><span class="line">width = <span class="number">60</span> * <span class="number">4</span></span><br><span class="line">height = <span class="number">60</span></span><br><span class="line">image = Image.new(<span class="string">&#x27;RGB&#x27;</span>, (width, height), (<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>))</span><br><span class="line"><span class="comment"># 创建Font对象:</span></span><br><span class="line">font = ImageFont.truetype(<span class="string">&#x27;Arial.ttf&#x27;</span>, <span class="number">36</span>)</span><br><span class="line"><span class="comment"># 创建Draw对象:</span></span><br><span class="line">draw = ImageDraw.Draw(image)</span><br><span class="line"><span class="comment"># 填充每个像素:</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(width):</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(height):</span><br><span class="line">        draw.point((x, y), fill=rndColor())</span><br><span class="line"><span class="comment"># 输出文字:</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">    draw.text((<span class="number">60</span> * t + <span class="number">10</span>, <span class="number">10</span>), rndChar(), font=font, fill=rndColor2())</span><br><span class="line"><span class="comment"># 模糊:</span></span><br><span class="line">image = image.<span class="built_in">filter</span>(ImageFilter.BLUR)</span><br><span class="line">image.save(<span class="string">&#x27;code.jpg&#x27;</span>, <span class="string">&#x27;jpeg&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://pillow.readthedocs.org/">官方文档</a></p>
<h2 id="plt"><a href="#plt" class="headerlink" title="plt"></a>plt</h2><h2 id="opencv"><a href="#opencv" class="headerlink" title="opencv"></a>opencv</h2><p>openCV具体笔记见对应的ipynb文件</p>
<p>注意：</p>
<ol>
<li> ​    opencv的读取格式为：W,H,C，其中C是BGR的，plt等是RGB的。</li>
</ol>
<h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><h3 id="一、-概念"><a href="#一、-概念" class="headerlink" title="一、 概念"></a>一、 概念</h3><p>在图像分类任务中，图像数据的增广是一种常用的<strong>正则化方法</strong>，主要用于增加训练数据集，让数据集尽可能的多样化，使得训练的模型具有更强的<strong>泛化能力</strong>，常用于数据量不足或者模型参数较多的场景。除了 ImageNet 分类任务标准数据增广方法外，还有8种数据增广方式非常常用，这里对其进行简单的介绍和对比，大家也可以将这些增广方法应用到自己的任务中，以获得模型精度的提升。这8种数据增广方式在ImageNet上的精度指标如 <strong>图1</strong> 所示。</p>
<p><img src="https://paddlepedia.readthedocs.io/en/latest/_images/main_image_aug.png" alt="img"></p>
<center>图一 8种数据增广方法</center>

<h3 id="二、常用数据增广方法"><a href="#二、常用数据增广方法" class="headerlink" title="二、常用数据增广方法"></a>二、<a target="_blank" rel="noopener" href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#id2">常用数据增广方法</a></h3><blockquote>
<p>注：如果没有特殊说明，本章节中所有示例为 ImageNet 分类，并且假设最终输入网络的数据维为：[batch-size, 3, 224, 224]</p>
</blockquote>
<p>在ImageNet 分类任务中，训练阶段的标准数据增广方法为以下几步：</p>
<ol>
<li><p>图像解码：简写为 <code>ImageDecode</code></p>
</li>
<li><p>随机裁剪到长宽均为 224 的图像：简写为 <code>RandCrop</code></p>
</li>
<li><p>水平方向随机翻转：简写为 <code>RandFlip</code></p>
</li>
<li><p>图像数据的归一化：简写为 <code>Normalize</code></p>
</li>
<li><p>图像数据的重排，<code>[224, 224, 3]</code> 变为 <code>[3, 224, 224]</code>：简写为 <code>Transpose</code></p>
</li>
<li><p>多幅图像数据组成 batch 数据，如 <code>batch-size</code> 个 <code>[3, 224, 224]</code> 的图像数据拼组成 <code>[batch-size, 3, 224, 224]</code>：简写为 <code>Batch</code></p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基于pytorch框架实现以上基本的图像增广操作</span></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>相比于上述标准的图像增广方法，研究者也提出了很多改进的图像增广策略，这些策略均是在标准增广方法的不同阶段插入一定的操作，基于这些策略操作所处的不同阶段，我们将其分为了三类：</p>
<ol>
<li><p>对 <code>RandCrop</code> (上述的阶段2)后的 224 的图像进行一些变换: AutoAugment，RandAugment</p>
</li>
<li><p>对<code>Transpose</code> (上述的阶段5)后的 224 的图像进行一些裁剪: CutOut，RandErasing，HideAndSeek，GridMask</p>
</li>
<li><p>对 <code>Batch</code>(上述的阶段6) 后的数据进行混合: Mixup，Cutmix</p>
</li>
</ol>
<p>增广后的可视化效果如 <strong>图2</strong> 所示。</p>
<p><img src="E:\笔记\markdown\reference\picture\image_aug_samples_s.jpg" alt="图2 数据增广后可视化"></p>
<center> 图2 数据增广后可视化</center>

<p>下文将介绍这些策略的原理与使用方法，其中，每种数据增广策略的参考论文与参考开源代码均在下面的介绍中列出。</p>
<p>以 <strong>图3</strong> 为测试图像，第三节将基于测试图像进行变换，并将变换后的效果进行可视化。</p>
<blockquote>
<p>由于<code>RandCrop</code>是随机裁剪，变换前后的图像内容可能会有一定的差别，无法直观地对比变换前后的图像。因此，本节将 <code>RandCrop</code> 替换为 <code>Resize</code>。</p>
</blockquote>
<p><img src="E:\笔记\markdown\reference\picture\test_baseline.jpeg" alt="图3 测试图像"></p>
<center>图3 测试图像</center>

<h3 id="三、图像变换类"><a href="#三、图像变换类" class="headerlink" title="三、图像变换类"></a>三、<a target="_blank" rel="noopener" href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#id3">图像变换类</a></h3><p>图像变换类指的是对 <code>RandCrop</code> 后的224 的图像进行一些变换，主要包括：</p>
<ul>
<li>AutoAugment[1]</li>
<li>RandAugment[2]</li>
</ul>
<h4 id="3-1-AutoAugment"><a href="#3-1-AutoAugment" class="headerlink" title="3.1 AutoAugment"></a>3.1 <a target="_blank" rel="noopener" href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#autoaugment">AutoAugment</a></h4><p>论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1805.09501v1">https://arxiv.org/abs/1805.09501v1</a></p>
<p>开源代码github地址：<a target="_blank" rel="noopener" href="https://github.com/DeepVoltaire/AutoAugment">https://github.com/DeepVoltaire/AutoAugment</a></p>
<p>不同于常规的人工设计图像增广方式，AutoAugment 是在一系列图像增广子策略的搜索空间中通过搜索算法找到的适合特定数据集的图像增广方案。针对 ImageNet 数据集，最终搜索出来的数据增广方案包含 25 个子策略组合，每个子策略中都包含两种变换，针对每幅图像都随机的挑选一个子策略组合，然后以一定的概率来决定是否执行子策略中的每种变换。</p>
<p>结果如 <strong>图4</strong> 所示。</p>
<p><img src="E:\笔记\markdown\reference\picture\test_autoaugment.jpeg" alt="图4 AutoAugment后图像可视化"></p>
<center>图4 AutoAugment后图像可视化</center>

<h4 id="3-2-RandAugment"><a href="#3-2-RandAugment" class="headerlink" title="3.2 RandAugment"></a>3.2 <a target="_blank" rel="noopener" href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#randaugment">RandAugment</a></h4><p>论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.13719.pdf">https://arxiv.org/pdf/1909.13719.pdf</a></p>
<p>开源代码github地址：<a target="_blank" rel="noopener" href="https://github.com/heartInsert/randaugment">https://github.com/heartInsert/randaugment</a></p>
<p><code>AutoAugment</code> 的搜索方法比较暴力，直接在数据集上搜索针对该数据集的最优策略，其计算量很大。在 <code>RandAugment</code> 文章中作者发现，一方面，针对越大的模型，越大的数据集，使用 <code>AutoAugment</code> 方式搜索到的增广方式产生的收益也就越小；另一方面，这种搜索出的最优策略是针对该数据集的，其迁移能力较差，并不太适合迁移到其他数据集上。</p>
<p>在 <code>RandAugment</code> 中，作者提出了一种随机增广的方式，不再像 <code>AutoAugment</code> 中那样使用特定的概率确定是否使用某种子策略，而是所有的子策略都会以同样的概率被选择到，论文中的实验也表明这种数据增广方式即使在大模型的训练中也具有很好的效果。</p>
<p>结果如 <strong>图5</strong> 所示。</p>
<p><img src="E:\笔记\markdown\reference\picture\test_randaugment.jpeg" alt="图5 RandAugment后图像可视化"></p>
<center>图5 RandAugment后图像可视化</center>

<h3 id="四、图像裁剪类"><a href="#四、图像裁剪类" class="headerlink" title="四、图像裁剪类"></a>四、<a target="_blank" rel="noopener" href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#id4">图像裁剪类</a></h3><p>图像裁剪类主要是对<code>Transpose</code> 后的 224 的图像进行一些裁剪，并将裁剪区域的像素值置为特定的常数（默认为0），主要包括：</p>
<ul>
<li>CutOut[3]</li>
<li>RandErasing[4]</li>
<li>HideAndSeek[5]</li>
<li>GridMask[6]</li>
</ul>
<p>图像裁剪的这些增广并非一定要放在归一化之后，也有不少实现是放在归一化之前的，也就是直接对 uint8 的图像进行操作，两种方式的差别是：如果直接对 uint8 的图像进行操作，那么再经过归一化之后被裁剪的区域将不再是纯黑或纯白（减均值除方差之后像素值不为0）。而对归一后之后的数据进行操作，裁剪的区域会是纯黑或纯白。</p>
<p>上述的裁剪变换思路是相同的，都是为了解决训练出的模型在有遮挡数据上泛化能力较差的问题，不同的是他们的裁剪方式、区域不太一样。</p>
<h4 id="4-1-Cutout"><a href="#4-1-Cutout" class="headerlink" title="4.1 Cutout"></a>4.1 <a target="_blank" rel="noopener" href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#cutout">Cutout</a></h4><p>论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1708.04552">https://arxiv.org/abs/1708.04552</a></p>
<p>开源代码github地址：<a target="_blank" rel="noopener" href="https://github.com/uoguelph-mlrg/Cutout">https://github.com/uoguelph-mlrg/Cutout</a></p>
<p>Cutout 可以理解为 Dropout 的一种扩展操作，不同的是 Dropout 是对图像经过网络后生成的特征进行遮挡，而 Cutout 是直接对输入的图像进行遮挡，相对于Dropout，Cutout 对噪声的鲁棒性更好。作者在论文中也进行了说明，这样做法有以下两点优势：(1) 通过 Cutout 可以模拟真实场景中主体被部分遮挡时的分类场景；(2) 可以促进模型充分利用图像中更多的内容来进行分类，防止网络只关注显著性的图像区域，从而发生过拟合。</p>
<p>结果如 <strong>图6</strong> 所示。</p>
<p><img src="E:\笔记\markdown\reference\picture\test_cutout.jpeg" alt="图6 Cutout后图像可视化"></p>
<center>图6 Cutout后图像可视化</center>

<h4 id="4-2-RandomErasing"><a href="#4-2-RandomErasing" class="headerlink" title="4.2 RandomErasing"></a>4.2 <a target="_blank" rel="noopener" href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#randomerasing">RandomErasing</a></h4><p>论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1708.04896.pdf">https://arxiv.org/pdf/1708.04896.pdf</a></p>
<p>开源代码github地址：<a target="_blank" rel="noopener" href="https://github.com/zhunzhong07/Random-Erasing">https://github.com/zhunzhong07/Random-Erasing</a></p>
<p><code>RandomErasing</code> 与 <code>Cutout</code> 方法类似，同样是为了解决训练出的模型在有遮挡数据上泛化能力较差的问题，作者在论文中也指出，随机裁剪的方式与随机水平翻转具有一定的互补性。作者也在行人再识别（REID）上验证了该方法的有效性。与<code>Cutout</code>不同的是，在<code>RandomErasing</code>中，图片以一定的概率接受该种预处理方法，生成掩码的尺寸大小与长宽比也是根据预设的超参数随机生成。</p>
<p>结果如 <strong>图7</strong> 所示。</p>
<p><img src="E:\笔记\markdown\reference\picture\test_randomerassing.jpeg" alt="图7 RandomErasing后图像可视化"></p>
<center>图7 RandomErasing后图像可视化</center>

<h4 id="4-3-HideAndSeek"><a href="#4-3-HideAndSeek" class="headerlink" title="4.3 HideAndSeek"></a>4.3 <a target="_blank" rel="noopener" href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#hideandseek">HideAndSeek</a></h4><p>论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1811.02545.pdf">https://arxiv.org/pdf/1811.02545.pdf</a></p>
<p>开源代码github地址：<a target="_blank" rel="noopener" href="https://github.com/kkanshul/Hide-and-Seek">https://github.com/kkanshul/Hide-and-Seek</a></p>
<p><code>HideAndSeek</code>论文将图像分为若干块区域(patch)，对于每块区域，都以一定的概率生成掩码，不同区域的掩码含义如 <strong>图8</strong> 所示。</p>
<p><img src="E:\笔记\markdown\reference\picture\hide-and-seek-visual.png" alt="图8 HideAndSeek分块掩码图"></p>
<center>图8 HideAndSeek分块掩码图</center>

<p>结果如 <strong>图9</strong> 所示。</p>
<p><img src="E:\笔记\markdown\reference\picture\test_hideandseek.jpeg" alt="图9 HideAndSeek后图像可视化"></p>
<center>图9 HideAndSeek后图像可视化</center>

<h4 id="4-4-GridMask"><a href="#4-4-GridMask" class="headerlink" title="4.4 GridMask"></a>4.4 <a target="_blank" rel="noopener" href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#gridmask">GridMask</a></h4><p>论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2001.04086">https://arxiv.org/abs/2001.04086</a></p>
<p>开源代码github地址：<a target="_blank" rel="noopener" href="https://github.com/akuxcw/GridMask">https://github.com/akuxcw/GridMask</a></p>
<p>作者在论文中指出，此前存在的基于对图像 crop 的方法存在两个问题，如 <strong>图10</strong> 所示：</p>
<ol>
<li><p>过度删除区域可能造成目标主体大部分甚至全部被删除，或者导致上下文信息的丢失，导致增广后的数据成为噪声数据；</p>
</li>
<li><p>保留过多的区域，对目标主体及上下文基本产生不了什么影响，失去增广的意义。</p>
</li>
</ol>
<p><img src="E:\笔记\markdown\reference\picture\gridmask-0.png" alt="图10 增广后的噪声数据"></p>
<center>图10 增广后的噪声数据</center>

<p>因此如果避免过度删除或过度保留成为需要解决的核心问题。</p>
<p><code>GridMask</code>是通过生成一个与原图分辨率相同的掩码，并将掩码进行随机翻转，与原图相乘，从而得到增广后的图像，通过超参数控制生成的掩码网格的大小。</p>
<p>在训练过程中，有两种以下使用方法：</p>
<ol>
<li>设置一个概率p，从训练开始就对图片以概率p使用<code>GridMask</code>进行增广。</li>
<li>一开始设置增广概率为0，随着迭代轮数增加，对训练图片进行<code>GridMask</code>增广的概率逐渐增大，最后变为p。</li>
</ol>
<p>论文中验证上述第二种方法的训练效果更好一些。</p>
<p>结果如 <strong>图11</strong> 所示。</p>
<p><img src="E:\笔记\markdown\reference\picture\test_gridmask.jpeg" alt="图11 GridMask后图像可视化"></p>
<center>图11 GridMask后图像可视化</center>

<h3 id="五、图像混叠"><a href="#五、图像混叠" class="headerlink" title="五、图像混叠"></a>五、<a target="_blank" rel="noopener" href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#id5">图像混叠</a></h3><p>图像混叠主要对 <code>Batch</code> 后的数据进行混合，包括：</p>
<ul>
<li>Mixup[7]</li>
<li>Cutmix[8]</li>
</ul>
<p>前文所述的图像变换与图像裁剪都是针对单幅图像进行的操作，而图像混叠是对两幅图像进行融合，生成一幅图像，两种方法的主要区别为混叠的方式不太一样。</p>
<h4 id="5-1-Mixup"><a href="#5-1-Mixup" class="headerlink" title="5.1 Mixup"></a>5.1 <a target="_blank" rel="noopener" href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#mixup">Mixup</a></h4><p>论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1710.09412.pdf">https://arxiv.org/pdf/1710.09412.pdf</a></p>
<p>开源代码github地址：<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/mixup-cifar10">https://github.com/facebookresearch/mixup-cifar10</a></p>
<p>Mixup 是最先提出的图像混叠增广方案，其原理简单、方便实现，不仅在图像分类上，在目标检测上也取得了不错的效果。为了便于实现，通常只对一个 batch 内的数据进行混叠，在 <code>Cutmix</code> 中也是如此。</p>
<p>如下是 <code>imaug</code> 中的实现，需要指出的是，下述实现会出现对同一幅进行相加的情况，也就是最终得到的图和原图一样，随着 <code>batch-size</code> 的增加这种情况出现的概率也会逐渐减小。</p>
<p>结果如 <strong>图12</strong> 所示。</p>
<p><img src="E:\笔记\markdown\reference\picture\test_mixup.png" alt="图12 Mixup后图像可视化"></p>
<center>图12 Mixup后图像可视化</center>

<h4 id="5-2-Cutmix"><a href="#5-2-Cutmix" class="headerlink" title="5.2 Cutmix"></a>5.2 <a target="_blank" rel="noopener" href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#cutmix">Cutmix</a></h4><p>论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1905.04899v2.pdf">https://arxiv.org/pdf/1905.04899v2.pdf</a></p>
<p>开源代码github地址：<a target="_blank" rel="noopener" href="https://github.com/clovaai/CutMix-PyTorch">https://github.com/clovaai/CutMix-PyTorch</a></p>
<p>与 <code>Mixup</code> 直接对两幅图进行相加不一样，<code>Cutmix</code> 是从一幅图中随机裁剪出一个 <code>ROI</code>，然后覆盖当前图像中对应的区域。</p>
<p>结果如 <strong>图13</strong> 所示。</p>
<p><img src="E:\笔记\markdown\reference\picture\test_cutmix.png" alt="图13 Cutmix后图像可视化"></p>
<center>图13 Cutmix后图像可视化</center>

<h3 id="六、实验"><a href="#六、实验" class="headerlink" title="六、实验"></a>六、<a target="_blank" rel="noopener" href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#id6">实验</a></h3><p>基于PaddleClas套件，使用上述几种数据增广方法在ImageNet1k数据集上进行了实验测试，每个方法的分类精度如下。</p>
<table>
<thead>
<tr>
<th>模型</th>
<th>初始学习率策略</th>
<th>l2 decay</th>
<th>batch size</th>
<th>epoch</th>
<th>数据变化策略</th>
<th>Top1 Acc</th>
<th>论文中结论</th>
</tr>
</thead>
<tbody><tr>
<td>ResNet50</td>
<td>0.1/cosine_decay</td>
<td>0.0001</td>
<td>256</td>
<td>300</td>
<td>标准变换</td>
<td>0.7731</td>
<td>-</td>
</tr>
<tr>
<td>ResNet50</td>
<td>0.1/cosine_decay</td>
<td>0.0001</td>
<td>256</td>
<td>300</td>
<td>AutoAugment</td>
<td>0.7795</td>
<td>0.7763</td>
</tr>
<tr>
<td>ResNet50</td>
<td>0.1/cosine_decay</td>
<td>0.0001</td>
<td>256</td>
<td>300</td>
<td>mixup</td>
<td>0.7828</td>
<td>0.7790</td>
</tr>
<tr>
<td>ResNet50</td>
<td>0.1/cosine_decay</td>
<td>0.0001</td>
<td>256</td>
<td>300</td>
<td>cutmix</td>
<td>0.7839</td>
<td>0.7860</td>
</tr>
<tr>
<td>ResNet50</td>
<td>0.1/cosine_decay</td>
<td>0.0001</td>
<td>256</td>
<td>300</td>
<td>cutout</td>
<td>0.7801</td>
<td>-</td>
</tr>
<tr>
<td>ResNet50</td>
<td>0.1/cosine_decay</td>
<td>0.0001</td>
<td>256</td>
<td>300</td>
<td>gridmask</td>
<td>0.7785</td>
<td>0.7790</td>
</tr>
<tr>
<td>ResNet50</td>
<td>0.1/cosine_decay</td>
<td>0.0001</td>
<td>256</td>
<td>300</td>
<td>random-augment</td>
<td>0.7770</td>
<td>0.7760</td>
</tr>
<tr>
<td>ResNet50</td>
<td>0.1/cosine_decay</td>
<td>0.0001</td>
<td>256</td>
<td>300</td>
<td>random erasing</td>
<td>0.7791</td>
<td>-</td>
</tr>
<tr>
<td>ResNet50</td>
<td>0.1/cosine_decay</td>
<td>0.0001</td>
<td>256</td>
<td>300</td>
<td>hide and seek</td>
<td>0.7743</td>
<td>0.7720</td>
</tr>
</tbody></table>
<p><strong>注意</strong>：</p>
<ul>
<li>在实验中，为了便于对比，将l2 decay固定设置为1e-4，在实际使用中，推荐尝试使用更小的l2 decay。结合数据增广，发现将l2 decay由1e-4减小为7e-5均能带来至少0.3~0.5%的精度提升。</li>
<li>在使用数据增广后，由于训练数据更难，所以训练损失函数可能较大，训练集的准确率相对较低，但其拥有更好的泛化能力，所以验证集的准确率相对较高。</li>
<li>在使用数据增广后，模型可能会趋于欠拟合状态，建议可以适当的调小<code>l2_decay</code>的值来获得更高的验证集准确率。</li>
</ul>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a><a target="_blank" rel="noopener" href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#id7">参考文献</a></h3><p>[1] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1805.09501v1">Autoaugment: Learning augmentation strategies from data</a></p>
<p>[2] <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1909.13719.pdf">Randaugment: Practical automated data augmentation with a reduced search space</a></p>
<p>[3] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1708.04552">Improved regularization of convolutional neural networks with cutout</a></p>
<p>[4] <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1708.04896.pdf">Random erasing data augmentation</a></p>
<p>[5] <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1811.02545.pdf">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</a></p>
<p>[6] <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2001.04086">GridMask Data Augmentation</a></p>
<p>[7] <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1710.09412.pdf">mixup: Beyond empirical risk minimization</a></p>
<p>[8] <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1905.04899v2.pdf">Cutmix: Regularization strategy to train strong classifiers with localizable features</a>)</p>
<h3 id="实验中用到的增强方法总结"><a href="#实验中用到的增强方法总结" class="headerlink" title="实验中用到的增强方法总结"></a>实验中用到的增强方法总结</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># transforms 类</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transforms</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        transform= [</span><br><span class="line">                transforms.RandomResizedCrop(size=args.image_size,scale=(<span class="number">0.2</span>, <span class="number">1.0</span>),ratio=(<span class="number">3</span> / <span class="number">4</span>, <span class="number">4</span> / <span class="number">3</span>)),</span><br><span class="line">                transforms.ColorJitter(brightness=<span class="number">0.4</span>, contrast=<span class="number">0.4</span>, saturation=<span class="number">0.4</span>),</span><br><span class="line">                transforms.RandomGrayscale(p=<span class="number">0.2</span>),</span><br><span class="line">                transforms.ToTensor(),</span><br><span class="line">                transforms.Normalize(mean=[<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>],std=[<span class="number">0.2470</span>, <span class="number">0.2435</span>, <span class="number">0.2616</span>])</span><br><span class="line">        ]</span><br><span class="line">        self.train_transform = transforms.Compose(transform)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.train_transform(x),self.train_transform(x)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 调用方法一</span></span><br><span class="line">dataset = CIFAR100(root=args.dataset_root,train=<span class="literal">False</span>,download=<span class="literal">True</span>,transform=Transforms())</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>


      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-特色包" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2022/04/07/%E7%89%B9%E8%89%B2%E5%8C%85/" class="article-date">
      <time datetime="2022-04-07T10:10:44.000Z" itemprop="datePublished">2022-04-07</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/04/07/%E7%89%B9%E8%89%B2%E5%8C%85/">特色包</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h1 id="控制台颜色"><a href="#控制台颜色" class="headerlink" title="控制台颜色"></a>控制台颜色</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> termcolor <span class="keyword">import</span> colored</span><br><span class="line"><span class="built_in">print</span>(colored(<span class="string">&#x27;Fill memory bank for mining the nearest neighbors (train) ...&#x27;</span>, <span class="string">&#x27;blue&#x27;</span>))</span><br></pre></td></tr></table></figure>

<h1 id="Logging"><a href="#Logging" class="headerlink" title="Logging"></a>Logging</h1><blockquote>
<p>  This module defines functions and classes which implement a flexible event logging system for applications and libraries.</p>
<p>  Python logging 模块定义了为应用程序和库实现灵活的事件日志记录的函数和类。</p>
</blockquote>
<p>程序开发过程中，很多程序都有记录日志的需求，并且日志包含的信息有正常的程序访问日志还可能有错误、警告等信息输出，Python 的 logging 模块提供了标准的日志接口，可以通过它存储各种格式的日志,日志记录提供了一组便利功能，用于简单的日志记录用法。</p>
<ul>
<li>  使用 Python Logging 模块的主要好处是所有 Python 模块都可以参与日志记录</li>
<li>  Logging 模块提供了大量具有灵活性的功能</li>
</ul>
<p><strong>日志记录函数以它们用来跟踪的事件的级别或严重性命名。下面描述了标准级别及其适用性（从高到低的顺序）：</strong></p>
<table>
<thead>
<tr>
<th>日志等级(level)</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<p><strong>日志级别等级排序</strong>：critical &gt; error &gt; warning &gt; info &gt; debug</p>
<p><strong>级别越高打印的日志越少，反之亦然，即</strong></p>
<ul>
<li>  debug : 打印全部的日志( notset 等同于 debug )</li>
<li>  info : 打印 info, warning, error, critical 级别的日志</li>
<li>  warning : 打印 warning, error, critical 级别的日志</li>
<li>  error : 打印 error, critical 级别的日志</li>
<li>  critical : 打印 critical 级别</li>
</ul>
<h2 id="一、-Logging-模块日志记录方式"><a href="#一、-Logging-模块日志记录方式" class="headerlink" title="一、 Logging 模块日志记录方式"></a><strong>一、 Logging 模块日志记录方式</strong></h2><p>Logging 模块提供了两种日志记录方式：</p>
<ul>
<li>  一种方式是使用 Logging 提供的模块级别的函数</li>
<li>  另一种方式是使用 Logging 日志系统的四大组件记录</li>
</ul>
<h2 id="1、Logging-定义的模块级别函数"><a href="#1、Logging-定义的模块级别函数" class="headerlink" title="1、Logging 定义的模块级别函数"></a><strong>1、Logging 定义的模块级别函数</strong></h2><table>
<thead>
<tr>
<th>函数</th>
<th>说明</th>
</tr>
</thead>
</table>
<p>简单打印日志：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印日志级别</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_logging</span>():</span></span><br><span class="line">    logging.debug(<span class="string">&#x27;Python debug&#x27;</span>)</span><br><span class="line">    logging.info(<span class="string">&#x27;Python info&#x27;</span>)</span><br><span class="line">    logging.warning(<span class="string">&#x27;Python warning&#x27;</span>)</span><br><span class="line">    logging.error(<span class="string">&#x27;Python Error&#x27;</span>)</span><br><span class="line">    logging.critical(<span class="string">&#x27;Python critical&#x27;</span>)</span><br><span class="line"></span><br><span class="line">test_logging()</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">WARNING:root:Python warning</span><br><span class="line">ERROR:root:Python Error</span><br><span class="line">CRITICAL:root:Python critical</span><br></pre></td></tr></table></figure>

<p>当指定一个日志级别之后，会记录大于或等于这个日志级别的日志信息，小于的将会被丢弃， ==默认情况下日志打印只显示大于等于 WARNING 级别的日志。==</p>
<h3 id="1-1-设置日志显示级别"><a href="#1-1-设置日志显示级别" class="headerlink" title="1.1 设置日志显示级别"></a><strong>1.1 设置日志显示级别</strong></h3><p>通过 logging.basicConfig() 可以设置 root 的日志级别，和日志输出格式。</p>
<p><strong>logging.basicConfig() 关键字参数</strong>：</p>
<table>
<thead>
<tr>
<th>关键字</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<p><strong>format 格式</strong></p>
<table>
<thead>
<tr>
<th>格式</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<p><strong>注意</strong>：Logging.basicConfig() 需要在开头就设置，在中间设置并无作用</p>
<p><strong>实例</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import logging</span><br><span class="line"></span><br><span class="line"># 打印日志级别</span><br><span class="line">def test():</span><br><span class="line">    logging.basicConfig(level=logging.DEBUG)</span><br><span class="line">    logging.debug(&#x27;Python debug&#x27;)</span><br><span class="line">    logging.info(&#x27;Python info&#x27;)</span><br><span class="line">    logging.warning(&#x27;Python warning&#x27;)</span><br><span class="line">    logging.error(&#x27;Python Error&#x27;)</span><br><span class="line">    logging.critical(&#x27;Python critical&#x27;)</span><br><span class="line">    logging.log(2,&#x27;test&#x27;)</span><br><span class="line">test()</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DEBUG:root:Python debug</span><br><span class="line">INFO:root:Python info</span><br><span class="line">WARNING:root:Python warning</span><br><span class="line">ERROR:root:Python Error</span><br><span class="line">CRITICAL:root:Python critical</span><br></pre></td></tr></table></figure>

<h3 id="1-2-将日志信息记录到文件"><a href="#1-2-将日志信息记录到文件" class="headerlink" title="1.2 将日志信息记录到文件"></a><strong>1.2 将日志信息记录到文件</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 日志信息记录到文件</span><br><span class="line">logging.basicConfig(filename=&#x27;F:/example.log&#x27;, level=logging.DEBUG)</span><br><span class="line">logging.debug(&#x27;This message should go to the log file&#x27;)</span><br><span class="line">logging.info(&#x27;So should this&#x27;)</span><br><span class="line">logging.warning(&#x27;And this, too&#x27;)</span><br></pre></td></tr></table></figure>

<p>在相应的路径下会有 example.log 日志文件，内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DEBUG:root:This message should go to the log file</span><br><span class="line">INFO:root:So should this</span><br><span class="line">WARNING:root:And this, too</span><br></pre></td></tr></table></figure>

<h3 id="1-3-多个模块记录日志信息"><a href="#1-3-多个模块记录日志信息" class="headerlink" title="1.3 多个模块记录日志信息"></a><strong>1.3 多个模块记录日志信息</strong></h3><p>如果程序包含多个模块，则用以下实例来显示日志信息： 实例中有两个模块，一个模块通过导入另一个模块的方式用日志显示另一个模块的信息：</p>
<p><strong>myapp.py 模块</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import logging</span><br><span class="line">import mylib</span><br><span class="line">def main():</span><br><span class="line">    logging.basicConfig(filename=&#x27;myapp.log&#x27;,level=logging.DEBUG)</span><br><span class="line">    logging.info(&#x27;Started&#x27;)</span><br><span class="line">    mylib.do_something()</span><br><span class="line">    logging.info(&#x27;Finished&#x27;)</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<p><strong>mylib.py 模块</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import logging</span><br><span class="line"></span><br><span class="line">def do_something():</span><br><span class="line">    logging.info(&#x27;Doing something&#x27;)</span><br></pre></td></tr></table></figure>

<p>执行 myapp.py 模块会打印相应日志，在文件 myapp.log 中显示信息如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">INFO:root:Started</span><br><span class="line">INFO:root:Doing something</span><br><span class="line">INFO:root:Finishe</span><br></pre></td></tr></table></figure>

<h3 id="1-4-显示信息的日期及更改显示消息格式"><a href="#1-4-显示信息的日期及更改显示消息格式" class="headerlink" title="1.4 显示信息的日期及更改显示消息格式"></a><strong>1.4 显示信息的日期及更改显示消息格式</strong></h3><p><strong>显示消息日期</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import logging</span><br><span class="line"># 显示消息时间</span><br><span class="line">logging.basicConfig(format=&#x27;%(asctime)s %(message)s&#x27;)</span><br><span class="line">logging.warning(&#x27;is when this event was logged.&#x27;)</span><br><span class="line"></span><br><span class="line">logging.basicConfig(format=&#x27;%(asctime)s %(message)s&#x27;, datefmt=&#x27;%m/%d/%Y %I:%M:%S %p&#x27;)</span><br><span class="line">logging.warning(&#x27;is when this event was logged.&#x27;)</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2019-10-16 18:57:45,988 is when this event was logged.</span><br><span class="line">2019-10-16 18:57:45,988 is when this event was logged.</span><br></pre></td></tr></table></figure>

<p><strong>更改显示消息格式</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import logging</span><br><span class="line"># 更改显示消息的格式</span><br><span class="line">logging.basicConfig(format=&#x27;%(levelname)s:%(message)s&#x27;,level=logging.DEBUG)</span><br><span class="line">logging.debug(&#x27;Python message format Debug&#x27;)</span><br><span class="line">logging.info(&#x27;Python message format Info&#x27;)</span><br><span class="line">logging.warning(&#x27;Python message format Warning&#x27;)</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DEBUG:Python message format Debug</span><br><span class="line">INFO:Python message format Info</span><br><span class="line">WARNING:Python message format Warning</span><br></pre></td></tr></table></figure>

<p>==注意==：显示结果只显示级别和具体信息，之前显示的 “根” 已经消失，重新定义的格式修改了默认输出方式。</p>
<h2 id="2、logging-模块四大组件"><a href="#2、logging-模块四大组件" class="headerlink" title="2、logging 模块四大组件"></a><strong>2、logging 模块四大组件</strong></h2><table>
<thead>
<tr>
<th>组件名称</th>
<th>对应类名</th>
<th>功能描述</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h3 id="2-1-日志器-Logger"><a href="#2-1-日志器-Logger" class="headerlink" title="2.1 日志器- Logger"></a><strong>2.1 日志器- Logger</strong></h3><p>Logger 持有日志记录器的方法，日志记录器不直接实例化，而是通过模块级函数 logger.getlogger (name) 来实例化,使用相同的名称多次调用 getLogger() 总是会返回对相同 Logger 对象的引用。</p>
<ul>
<li>  应用程序代码能直接调用日志接口。</li>
<li>  Logger最常用的操作有两类：配置和发送日志消息。</li>
<li>  初始化 logger = logging.getLogger(“endlesscode”)，获取 logger 对象，getLogger() 方法后面最好加上所要日志记录的模块名字，配置文件和打印日志格式中的 %(name)s 对应的是这里的模块名字，如果不指定name则返回root对象。</li>
<li>  logger.setLevel(logging.DEBUG)，Logging 中有 NOTSET &lt; DEBUG &lt; INFO &lt; WARNING &lt; ERROR &lt; CRITICAL这几种级别，日志会记录设置级别以上的日志</li>
<li>  多次使用相同的name调用 getLogger 方法返回同一个 looger 对象；</li>
</ul>
<p>Logger是一个树形层级结构，在使用接口 debug，info，warn，error，critical 之前必须创建 Logger 实例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">创建方法: logger = logging.getLogger(logger_name)</span><br></pre></td></tr></table></figure>

<p>创建Logger实例后，可以使用以下方法进行日志级别设置，增加处理器 Handler：</p>
<ul>
<li>  logger.setLevel(logging.ERROR) # 设置日志级别为 ERROR，即只有日志级别大于等于 ERROR 的日志才会输出</li>
<li>  logger.addHandler(handler_name) # 为 Logger 实例增加一个处理器</li>
<li>  logger.removeHandler(handler_name) # 为 Logger 实例删除一个处理器</li>
</ul>
<h3 id="2-2-处理器-Handler"><a href="#2-2-处理器-Handler" class="headerlink" title="2.2 处理器- Handler"></a><strong>2.2 处理器- Handler</strong></h3><p>Handler 处理器类型有很多种，比较常用的有三个，StreamHandler，FileHandler，NullHandler</p>
<p><strong>StreamHandler</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">创建方法：sh = logging.StreamHandler(stream=None)</span><br></pre></td></tr></table></figure>

<p>创建 StreamHandler 之后，可以通过使用以下方法设置日志级别，设置格式化器 Formatter，增加或删除过滤器 Filter：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ch.setLevel(logging.WARN) # 指定日志级别，低于WARN级别的日志将被忽略</span><br><span class="line"></span><br><span class="line">ch.setFormatter(formatter_name) # 设置一个格式化器formatter</span><br><span class="line"></span><br><span class="line">ch.addFilter(filter_name) # 增加一个过滤器，可以增加多个</span><br><span class="line"> </span><br><span class="line">ch.removeFilter(filter_name) # 删除一个过滤器</span><br></pre></td></tr></table></figure>

<h3 id="2-3-过滤器-Filter"><a href="#2-3-过滤器-Filter" class="headerlink" title="2.3 过滤器- Filter"></a><strong>2.3 过滤器- Filter</strong></h3><p>Handlers 和 Loggers 可以使用 Filters 来完成比级别更复杂的过滤。 Filter 基类只允许特定 Logger 层次以下的事件。 例如用 ‘A.B’ 初始化的 Filter 允许Logger ‘A.B’, ‘A.B.C’, ‘A.B.C.D’, ‘A.B.D’ 等记录的事件，logger‘A.BB’, ‘B.A.B’ 等就不行。 如果用空字符串来初始化，所有的事件都接受。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">创建方法: filter = logging.Filter(name=&#x27;&#x27;)</span><br></pre></td></tr></table></figure>

<h3 id="2-4-格式器-Formatter"><a href="#2-4-格式器-Formatter" class="headerlink" title="2.4 格式器- Formatter"></a><strong>2.4 格式器- Formatter</strong></h3><p>使用Formatter对象设置日志信息最后的规则、结构和内容，默认的时间格式为%Y-%m-%d %H:%M:%S。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">创建方法: formatter = logging.Formatter(fmt=None, datefmt=None)</span><br></pre></td></tr></table></figure>

<p>其中，fmt 是消息的格式化字符串，datefmt 是日期字符串。如果不指明 fmt，将使用 ‘%(message)s’ 。如果不指明 datefmt，将使用 ISO8601 日期格式。</p>
<h3 id="2-5-组件之间的关联关系"><a href="#2-5-组件之间的关联关系" class="headerlink" title="2.5 组件之间的关联关系"></a><strong>2.5 组件之间的关联关系</strong></h3><ul>
<li>  日志器（logger）需要通过处理器（handler）将日志信息输出到目标位置，不同的处理器（handler）可以将日志输出到不同的位置；</li>
<li>  日志器（logger）可以设置多个处理器（handler）将同一条日志记录输出到不同的位置；</li>
<li>  每个处理器（handler）都可以设置自己的过滤器（filter）实现日志过滤，从而只保留感兴趣的日志；</li>
<li>  每个处理器（handler）都可以设置自己的格式器（formatter）实现同一条日志以不同的格式输出到不同的地方。</li>
</ul>
<p>简明了说就是：日志器（logger）是入口，真正干活儿的是处理器（handler），处理器（handler）还可以通过过滤器（filter）和格式器（formatter）对要输出的日志内容做过滤和格式化等处理操作。</p>
<ul>
<li>  Logger 可以包含一个或多个 Handler 和 Filter</li>
<li>  Logger 与 Handler 或 Fitler 是一对多的关系</li>
<li>  一个 Logger 实例可以新增多 个 Handler，一个 Handler 可以新增多个格式化器或多个过滤器，而且日志级别将会继承。</li>
</ul>
<h2 id="二、Logging-日志工作流程"><a href="#二、Logging-日志工作流程" class="headerlink" title="二、Logging 日志工作流程"></a><strong>二、Logging 日志工作流程</strong></h2><h2 id="1、Logging-模块使用过程"><a href="#1、Logging-模块使用过程" class="headerlink" title="1、Logging 模块使用过程"></a><strong>1、Logging 模块使用过程</strong></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1）第一次导入 logging 模块或使用 reload 函数重新导入 logging 模块，logging 模块中的代码将被执行，这个过程中将产生 logging 日志系统的默认配置。</span><br><span class="line"></span><br><span class="line">2）自定义配置(可选),logging标准模块支持三种配置方式: dictConfig，fileConfig，listen。其中，dictConfig 是通过一个字典进行配置 Logger，Handler，Filter，Formatter；fileConfig 则是通过一个文件进行配置；而 listen 则监听一个网络端口，通过接收网络数据来进行配置。当然，除了以上集体化配置外，也可以直接调用 Logger，Handler 等对象中的方法在代码中来显式配置。</span><br><span class="line"></span><br><span class="line">3）使用 logging 模块的全局作用域中的 getLogger 函数来得到一个 Logger 对象实例(其参数即是一个字符串，表示 Logger 对象实例的名字，即通过该名字来得到相应的 Logger 对象实例)。</span><br><span class="line"></span><br><span class="line">4）使用 Logger 对象中的 debug，info，error，warn，critical 等方法记录日志信息。</span><br></pre></td></tr></table></figure>

<h2 id="2、Logging-模块处理流程"><a href="#2、Logging-模块处理流程" class="headerlink" title="2、Logging 模块处理流程"></a><strong>2、Logging 模块处理流程</strong></h2><p>流程描述：</p>
<ol>
<li> 判断日志的等级是否大于 Logger 对象的等级，如果大于，则往下执行，否则，流程结束。</li>
<li> 产生日志：第一步，判断是否有异常，如果有，则添加异常信息。 第二步，处理日志记录方法(如 debug，info 等)中的占位符，即一般的字符串格式化处理。</li>
<li> 使用注册到 Logger 对象中的 Filters 进行过滤。如果有多个过滤器，则依次过滤；只要有一个过滤器返回假，则过滤结束，且该日志信息将丢弃，不再处理，而处理流程也至此结束。否则，处理流程往下执行。</li>
<li> 在当前 Logger 对象中查找 Handlers，如果找不到任何 Handler，则往上到该 Logger 对象的父 Logger 中查找；如果找到一个或多个 Handler，则依次用 Handler 来处理日志信息。但在每个 Handler 处理日志信息过程中，会首先判断日志信息的等级是否大于该 Handler 的等级，如果大于，则往下执行(由 Logger 对象进入 Handler 对象中)，否则，处理流程结束。</li>
<li> 执行 Handler 对象中的 filter 方法，该方法会依次执行注册到该 Handler 对象中的 Filter。如果有一个 Filter 判断该日志信息为假，则此后的所有 Filter 都不再执行，而直接将该日志信息丢弃，处理流程结束。</li>
<li> 使用 Formatter 类格式化最终的输出结果。 注：Formatter 同上述第 2 步的字符串格式化不同，它会添加额外的信息，比如日志产生的时间，产生日志的源代码所在的源文件的路径等等。</li>
<li> 真正地输出日志信息(到网络，文件，终端，邮件等)。至于输出到哪个目的地，由 Handler 的种类来决定。</li>
</ol>
<h2 id="三、配置日志"><a href="#三、配置日志" class="headerlink" title="三、配置日志"></a><strong>三、配置日志</strong></h2><p>程序员可以通过三种方式配置日志记录：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1、使用配置方法的 Python 代码显式创建记录器，处理程序和格式化程序。</span><br><span class="line"></span><br><span class="line">2、创建日志记录配置文件并使用该 fileConfig() 功能读取它。</span><br><span class="line"></span><br><span class="line">3、创建配置信息字典并将其传递给 dictConfig()函数。</span><br></pre></td></tr></table></figure>

<p>下面使用 Python 代码配置一个非常简单的记录器，一个控制台处理程序和一个简单的格式化程序：</p>
<p><strong>logging.conf 配置文件</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[loggers]</span><br><span class="line">keys=root,simpleExample</span><br><span class="line"></span><br><span class="line">[handlers]</span><br><span class="line">keys=consoleHandler</span><br><span class="line"></span><br><span class="line">[formatters]</span><br><span class="line">keys=simpleFormatter</span><br><span class="line"></span><br><span class="line">[logger_root]</span><br><span class="line">level=DEBUG</span><br><span class="line">handlers=consoleHandler</span><br><span class="line"></span><br><span class="line">[logger_simpleExample]</span><br><span class="line">level=DEBUG</span><br><span class="line">handlers=consoleHandler</span><br><span class="line">qualname=simpleExample</span><br><span class="line">propagate=0</span><br><span class="line"></span><br><span class="line">[handler_consoleHandler]</span><br><span class="line">class=StreamHandler</span><br><span class="line">level=DEBUG</span><br><span class="line">formatter=simpleFormatter</span><br><span class="line">args=(sys.stdout,)</span><br><span class="line"></span><br><span class="line">[formatter_simpleFormatter]</span><br><span class="line">format=%(asctime)s - %(name)s - %(levelname)s - %(message)s</span><br><span class="line">datefmt=</span><br></pre></td></tr></table></figure>

<p><strong>config_logging.py 配置器</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import logging</span><br><span class="line"># create logger</span><br><span class="line">logger = logging.getLogger(&#x27;simple_example&#x27;)</span><br><span class="line">logger.setLevel(logging.DEBUG)</span><br><span class="line"></span><br><span class="line"># create console handler and set level to debug</span><br><span class="line">ch = logging.StreamHandler()</span><br><span class="line">ch.setLevel(logging.DEBUG)</span><br><span class="line"></span><br><span class="line"># create formatter</span><br><span class="line">formatter = logging.Formatter(&#x27;%(asctime)s - %(name)s - %(levelname)s - %(message)s&#x27;)</span><br><span class="line"></span><br><span class="line"># add formatter to ch</span><br><span class="line">ch.setFormatter(formatter)</span><br><span class="line"></span><br><span class="line"># add ch to logger</span><br><span class="line">logger.addHandler(ch)</span><br><span class="line"></span><br><span class="line"># &#x27;application&#x27; code</span><br><span class="line">logger.debug(&#x27;debug message&#x27;)</span><br><span class="line">logger.info(&#x27;info message&#x27;)</span><br><span class="line">logger.warning(&#x27;warn message&#x27;)</span><br><span class="line">logger.error(&#x27;error message&#x27;)</span><br><span class="line">logger.critical(&#x27;critical message&#x27;)</span><br></pre></td></tr></table></figure>

<p><strong>recorder 记录器</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import logging</span><br><span class="line">import logging.config</span><br><span class="line"></span><br><span class="line">logging.config.fileConfig(&#x27;logging.conf&#x27;)</span><br><span class="line"></span><br><span class="line"># create logger</span><br><span class="line">logger = logging.getLogger(&#x27;simpleExample&#x27;)</span><br><span class="line"></span><br><span class="line"># &#x27;application&#x27; code</span><br><span class="line">logger.debug(&#x27;debug message&#x27;)</span><br><span class="line">logger.info(&#x27;info message&#x27;)</span><br><span class="line">logger.warning(&#x27;warn message&#x27;)</span><br><span class="line">logger.error(&#x27;error message&#x27;)</span><br><span class="line">logger.critical(&#x27;critical message&#x27;)</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2019-10-16 19:45:34,440 - simple_example - DEBUG - debug message</span><br><span class="line">2019-10-16 19:45:34,440 - simple_example - INFO - info message</span><br><span class="line">2019-10-16 19:45:34,440 - simple_example - WARNING - warn message</span><br><span class="line">2019-10-16 19:45:34,440 - simple_example - ERROR - error message</span><br><span class="line">2019-10-16 19:45:34,441 - simple_example - CRITICAL - critical message</span><br></pre></td></tr></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><p>本章节给大家介绍了 Python 标准库中 Logging 模块的详细介绍与使用，对 Python 工程师使用该模块提供更好的支撑</p>
<p>参考： <a href="https://link.zhihu.com/?target=https://docs.python.org/3.6/library/logging.html?highlight=logging%23integration-with-the-warnings-module">https://docs.python.org/3.6/library/logging.html?highlight=logging#integration-with-the-warnings-module</a> <a href="https://link.zhihu.com/?target=https://www.jianshu.com/p/feb86c06c4f4">https://www.jianshu.com/p/feb86c06c4f4</a><br><a href="https://link.zhihu.com/?target=https://cloud.tencent.com/developer/article/1354396">https://cloud.tencent.com/devel</a></p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/" rel="tag">python</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-python语法" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2022/04/07/python%E8%AF%AD%E6%B3%95/" class="article-date">
      <time datetime="2022-04-07T08:10:44.000Z" itemprop="datePublished">2022-04-07</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/04/07/python%E8%AF%AD%E6%B3%95/">python语法</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
              <h2 id="矩阵的操作"><a href="#矩阵的操作" class="headerlink" title="矩阵的操作"></a>矩阵的操作</h2><h3 id="view-和reshape"><a href="#view-和reshape" class="headerlink" title="view()和reshape"></a>view()和reshape</h3><p>view操作后的变量还是连续的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]],</span><br><span class="line">                  [[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],[<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]]</span><br><span class="line">                 ])</span><br><span class="line">b = a.view(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">b = tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>],</span><br><span class="line">        [<span class="number">5</span>,  <span class="number">6</span> ,<span class="number">7</span>,	<span class="number">8</span>],</span><br><span class="line">        [<span class="number">9</span>,	<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br></pre></td></tr></table></figure>

<p>张量a的size是2x2x3，使用view函数后，先将这12个元素排成一列，然后将其依次填充至新的4x3的张量中：</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-acfb79d45817e3080ddcac7666d61f09_b.jpg" alt="img"></p>
<p>为了更细致，我们需要描述一下它们的具体操作流程（这并不是源码的流程，只是为了便于理解、记忆），因为二维比较直观，如果维度比较高的话，可能还是不够直观，心里老是有疑虑，还是以上面例子为例，张量a的每一个元素都有一个index，例如1的index是(0,0,0)，7的index是(1,0,0)，11的index是(1,1,1)……在拉成列向量排列时，排列规则是这样的，以(0,0,0)开始，维度从最后一维循环到第一维，在每一维内以升序将所有元素排成一列，即：</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-0c59291b64850293305ab879087ea32d_b.jpg" alt="img"></p>
<p>然后依据新的size对每个元素给予新的index，仍然是维度从最后一维循环到第一维，在每一维内升序，即:</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-ab9112d1c2f5bb21d1b3d164bffa5e95_b.jpg" alt="img"></p>
<blockquote>
<p>  torch的view()与reshape()方法都可以用来重塑tensor的shape，区别就是使用的条件不一样。view()方法只适用于满足连续性条件的tensor，并且该操作不会开辟新的内存空间，只是产生了对原存储空间的一个新别称和引用，返回值是视图。而reshape()方法的返回值既可以是视图，也可以是副本，当满足连续性条件时返回view，否则返回副本[ 此时等价于先调用contiguous()方法在使用view() ]。因此当不确能否使用view时，可以使用reshape。如果只是想简单地重塑一个tensor的shape，那么就是用reshape，但是如果需要考虑内存的开销而且要确保重塑后的tensor与之前的tensor共享存储空间，那就使用view()。</p>
</blockquote>
<h3 id="permute-和transpose"><a href="#permute-和transpose" class="headerlink" title="permute()和transpose()"></a>permute()和transpose()</h3><p>transpose()也是转置操作，与permute不同的是，transpose()每次只能进行二维转置，而permute()每次可转置多个维度().</p>
<p>接下来，说一下permute()，函数的参数为新的维度顺序，例如想交换第一维与第三维的index，则<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=tensor.permute&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:%2288311093%22%7D">tensor.permute</a>(2,1,0)，同样举一个简单第二维与第三维的例子，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]],[[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],[<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]]])</span><br><span class="line">b = a.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">b = tensor([[[ <span class="number">1</span>,  <span class="number">4</span>],</span><br><span class="line">         [ <span class="number">2</span>,  <span class="number">5</span>],</span><br><span class="line">         [ <span class="number">3</span>,  <span class="number">6</span>]],</span><br><span class="line">        [[ <span class="number">7</span>, <span class="number">10</span>],</span><br><span class="line">         [ <span class="number">8</span>, <span class="number">11</span>],</span><br><span class="line">         [ <span class="number">9</span>, <span class="number">12</span>]]])</span><br></pre></td></tr></table></figure>

<p><img src="E:\笔记\markdown\reference\picture\v2-6bdd87e65b64534339d3efad9ae4eae5_b.jpg" alt="img"></p>
<p>当维度比较大，交换维度的结果并不直观，我们还是说下permute到底做了什么，示意图如下。</p>
<p>需要说明一下，这里面的流程其实就是更改每个元素的index而已，上述例子中只是交换坐标，</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-ec4167d8c19713e232ab3f5ba35ed10d_b.jpg" alt="img"></p>
<p>然后按照新的index更改下各元素的位置并展示出即可。</p>
<h3 id="squeeze-和unsqueeze-函数功能"><a href="#squeeze-和unsqueeze-函数功能" class="headerlink" title="squeeze()和unsqueeze()函数功能"></a>squeeze()和unsqueeze()函数功能</h3><h4 id="1-squeeze-dim-："><a href="#1-squeeze-dim-：" class="headerlink" title="1.squeeze(dim)："></a>1.squeeze(dim)：</h4><p>给张量tensor降维，但不是啥张量都可以用这两个函数来降维。dim指定需要降维的维度，这个维度的值必须为1才能被降维。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a=torch.rand(<span class="number">2</span>,<span class="number">1</span>,<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(a.shape)</span><br><span class="line"><span class="built_in">print</span>(a.squeeze(<span class="number">1</span>).shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#torch.Size([2, 1, 4, 1])</span></span><br><span class="line"><span class="comment">#torch.Size([2, 4, 1])</span></span><br></pre></td></tr></table></figure>

<p>2.unsqueeze(dim)</p>
<p>与squeeze(dim)相反，在dim维度上添加一个维度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a=torch.rand(<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(a.shape)</span><br><span class="line"><span class="built_in">print</span>(a.unsqueeze(<span class="number">1</span>).shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#torch.Size([2, 4])</span></span><br><span class="line"><span class="comment">#torch.Size([2, 1, 4])</span></span><br></pre></td></tr></table></figure>





<h2 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h2><p>在 Python 中，* 和 ** 具有语法多义性，具体来说是有四类用法。</p>
          
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/" rel="tag">python</a></li></ul>
    </div>

      
        <p class="article-more-link">
          <a href="/2022/04/07/python%E8%AF%AD%E6%B3%95/#more">阅读全文 >></a>
        </p>
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-聚类方法总结" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2022/04/07/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/" class="article-date">
      <time datetime="2022-04-07T08:10:44.000Z" itemprop="datePublished">2022-04-07</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/04/07/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/">聚类</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <p>可以使用模块 <a target="_blank" rel="noopener" href="http://scikit-learn.org.cn/lists/3.html#sklearn.cluster%EF%BC%9A%E8%81%9A%E7%B1%BB"><code>sklearn.cluster</code></a> 对未标记的数据进行 <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Cluster_analysis">聚类(Clustering)</a> 。</p>
<p>每个聚类算法都有两个变体:一个是类，它实现了 <code>fit</code> 方法来学习训练数据上的簇，另一个是函数，给定训练数据，返回对应于不同簇的整数标签数组。对于类，可以在 <code>labels_</code> 属性中找到训练数据上的标签。</p>
<blockquote>
<p>  <strong>输入数据</strong></p>
<p>  特别需要注意的一点是，本模块实现的算法可以采用不同类型的矩阵作为输入。所有算法的调用接口都接受 shape<code>[n_samples, n_features]</code> 的标准数据矩阵。 这些矩阵可以通过使用 <a target="_blank" rel="noopener" href="http://scikit-learn.org.cn/lists/3.html#sklearn.feature_extraction%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96"><code>sklearn.feature_extraction</code></a> 模块中的类获得。对于 <a target="_blank" rel="noopener" href="https://scikit-learn.org.cn/view/370.html"><code>AffinityPropagation</code></a>, <a target="_blank" rel="noopener" href="https://scikit-learn.org.cn/view/391.html"><code>SpectralClustering</code></a> 和 <a target="_blank" rel="noopener" href="https://scikit-learn.org.cn/view/379.html"><code>DBSCAN</code></a> 也可以输入 shape <code>[n_samples, n_samples]</code> 的相似矩阵，可以通过 <a target="_blank" rel="noopener" href="http://scikit-learn.org.cn/lists/3.html#sklearn.metrics%EF%BC%9A%E6%8C%87%E6%A0%87"><code>sklearn.metrics.pairwise</code></a> 模块中的函数获得。</p>
</blockquote>
<h3 id="聚类方法概述"><a href="#聚类方法概述" class="headerlink" title="聚类方法概述"></a>聚类方法概述</h3>
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-pytorch使用汇总" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2022/04/07/pytorch%E4%BD%BF%E7%94%A8%E6%B1%87%E6%80%BB/" class="article-date">
      <time datetime="2022-04-07T04:44:13.000Z" itemprop="datePublished">2022-04-07</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2022/04/07/pytorch%E4%BD%BF%E7%94%A8%E6%B1%87%E6%80%BB/">pytorch使用汇总</a>
    </h1>
  

      </header>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
              <h2 id="日志输出"><a href="#日志输出" class="headerlink" title="日志输出"></a>日志输出</h2><p>利用logging模块在控制台实时打印并及时记录运行日志。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from config import  *</span><br><span class="line">import logging  # 引入logging模块</span><br><span class="line">import os.path</span><br><span class="line">class Logger:</span><br><span class="line">    def __init__(self,mode=&#x27;w&#x27;):</span><br><span class="line">        # 第一步，创建一个logger</span><br><span class="line">        self.logger = logging.getLogger()</span><br><span class="line">        self.logger.setLevel(logging.INFO)  # Log等级总开关</span><br><span class="line">        # 第二步，创建一个handler，用于写入日志文件</span><br><span class="line">        rq = time.strftime(&#x27;%Y%m%d%H%M&#x27;, time.localtime(time.time()))</span><br><span class="line">        log_path = os.getcwd() + &#x27;/Logs/&#x27;</span><br><span class="line">        log_name = log_path + rq + &#x27;.log&#x27;</span><br><span class="line">        logfile = log_name</span><br><span class="line">        fh = logging.FileHandler(logfile, mode=mode)</span><br><span class="line">        fh.setLevel(logging.DEBUG)  # 输出到file的log等级的开关</span><br><span class="line">        # 第三步，定义handler的输出格式</span><br><span class="line">        formatter = logging.Formatter(&quot;%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s&quot;)</span><br><span class="line">        fh.setFormatter(formatter)</span><br><span class="line">        # 第四步，将logger添加到handler里面</span><br><span class="line">        self.logger.addHandler(fh)</span><br><span class="line">        ch = logging.StreamHandler()</span><br><span class="line">        ch.setLevel(logging.INFO)  # 输出到console的log等级的开关</span><br><span class="line">        ch.setFormatter(formatter)</span><br><span class="line">        self.logger.addHandler(ch)</span><br></pre></td></tr></table></figure>



<h2 id="模型的保存和读取"><a href="#模型的保存和读取" class="headerlink" title="模型的保存和读取"></a>模型的保存和读取</h2><p>​    </p>
<table>
<thead>
<tr>
<th>保存</th>
<th align="left">（1）torch.save(net.state_dict(),保存路径) 。（2）多卡训练时，要用 torch.save(net.module.state_dict(),’./model/best.pth’)</th>
</tr>
</thead>
<tbody><tr>
<td>读取</td>
<td align="left">（1）net.load_state_dict(torch.load(‘best.pth’))。（2）并行时map_location=device.type在读取模型的时候一定要加上。即：    model.load_state_dict(torch.load(‘model/self_train_bestv2.pth’,map_location=’cuda’))</td>
</tr>
<tr>
<td></td>
<td align="left">torch.save(model.state_dict(),’checkpoint_0.tar’,_use_new_zipfile_serialization=False) #解决版本不兼容</td>
</tr>
</tbody></table>
          
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/pytorch/" rel="tag">pytorch</a></li></ul>
    </div>

      
        <p class="article-more-link">
          <a href="/2022/04/07/pytorch%E4%BD%BF%E7%94%A8%E6%B1%87%E6%80%BB/#more">阅读全文 >></a>
        </p>
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &amp;raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2021-2022  liusha
            </div>
            <div class="footer-right">
                回首向来萧瑟处，归去，也无风雨也无晴。
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_uv">
  本站访客数<span id="busuanzi_value_site_uv"></span>人次
</span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="本页阅读量"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>