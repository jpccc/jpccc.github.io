<!DOCTYPE html>
<html lang="zh-Hans">
<head>

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no" />
<meta name="author" content=" liusha" />



<meta name="description" content="The Illustrated Self-Supervised LearningI first got introduced to self-supervised learning in a talk by Yann Lecun, where he introduced the “cake analogy” to illustrate the importance of self-supervis">
<meta property="og:type" content="article">
<meta property="og:title" content="流沙">
<meta property="og:url" content="https://jpccc.github.io/2022/04/20/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="流沙">
<meta property="og:description" content="The Illustrated Self-Supervised LearningI first got introduced to self-supervised learning in a talk by Yann Lecun, where he introduced the “cake analogy” to illustrate the importance of self-supervis">
<meta property="og:locale">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/self-supervised-nlp-to-vision-16504267629161.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/supervised-manual-annotation-16504267629163.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/supervised-automated-16504267629165.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/self-supervised-workflow-16504267629167.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/self-supervised-finetuning-16504267629169.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/ss-colorization-data-gen-165042676291611.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/ss-image-colorization-165042676291613.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/ss-colorization-learning-165042676291615.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/ss-superresolution-training-gen-165042676291617.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/ss-srgan-architecture-165042676291619.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/ss-image-inpainting-data-gen-165042676291621.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/ss-inpainting-architecture-165042676291723.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/split-brain-autoencoder-165042676291725.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/split-brain-autoencoder-rgbhha-165042676291727.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/ss-image-jigsaw-data-165042676291729.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/ss-jigsaw-permutations-165042676291731.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/ss-jigsaw-permutation-64-165042676291733.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/ss-jigsaw-architecture-165042676291735.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/ss-context-prediction-gen-165042676291737.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/ss-context-prediction-architecture-165042676291739.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/ss-geometric-transformation-gen-165042676291741.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/ss-geometric-transformation-architecture-165042676291743.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/ss-image-clustering-gen-165042676291845.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/ss-deep-clustering-architecture-165042676291847.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/synthetic-imagery-data-165042676291849.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/ss-synthetic-image-architecture-165042676291851.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/ss-frame-order-data-gen-165042676291853.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/ss-temporal-order-architecture-165042676291855.png">
<meta property="article:published_time" content="2022-04-20T14:38:48.702Z">
<meta property="article:modified_time" content="2022-04-20T14:39:50.543Z">
<meta property="article:author" content=" liusha">
<meta property="article:tag" content="人工智能、机器学习、深度学习、数据挖掘、深度聚类">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="e:/笔记/markdown/reference/picture/self-supervised-nlp-to-vision-16504267629161.png">

<link rel="apple-touch-icon" href= "/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="流沙" type="application/atom+xml">



    <link rel="shortcut icon" href="/favicon.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">



<link rel="stylesheet" href="/css/style.css">




<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>流沙</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: false,
        isPost: true,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: true
    }
</script>


    <script> yiliaConfig.jquery_ui = [false]; </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/avatar.png" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/"></a></h1>
        </hgroup>

        

        
            <form id="search-form">
            <input type="text" id="local-search-input" name="q" placeholder="search..." class="search form-control" autocomplete="off" autocorrect="off" searchonload="" />
            <i class="fa fa-times" onclick="resetSearch()"></i>
            </form>
            <div id="local-search-result"></div>
            <p class='no-result'>No results found <i class='fa fa-spinner fa-pulse'></i></p>
        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                        <li>关于我</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="https://jpccc.github.io/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/%E9%9A%8F%E7%AC%94">随笔</a></li>
                        
                            <li><a href="/tags/">标签云</a></li>
                        
                            <li><a href="/about/">关于我</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="mailto:jiangpc21@mails.glu.edu.cn" title="Email"></a>
                            
                                <a class="fa GitHub" href="#" title="GitHub"></a>
                            
                                <a class="fa RSS" href="/atom.xml" title="RSS"></a>
                            
                                <a class="fa QQ" href="/1778013127" title="QQ"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/deepLearning/" rel="tag">deepLearning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/math/" rel="tag">math</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pytorch/" rel="tag">pytorch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/" rel="tag">统计学习</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://hexo.io">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://pages.github.com/">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="http://moxfive.xyz/">MOxFIVE</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">机器学习</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页"></a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/avatar.png" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页"></a></h1>
            </hgroup>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="https://jpccc.github.io/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/%E9%9A%8F%E7%AC%94">随笔</a></li>
                
                    <li><a href="/tags/">标签云</a></li>
                
                    <li><a href="/about/">关于我</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="mailto:jiangpc21@mails.glu.edu.cn" title="Email"></a>
                            
                                <a class="fa GitHub" target="_blank" href="#" title="GitHub"></a>
                            
                                <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a>
                            
                                <a class="fa QQ" target="_blank" href="/1778013127" title="QQ"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="标签" friends="友情链接" about="关于我"/>
</nav>
      <div class="body-wrap"><article id="post-无监督学习" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2022/04/20/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" class="article-date">
      <time datetime="2022-04-20T14:38:48.702Z" itemprop="datePublished">2022-04-20</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h1 id="The-Illustrated-Self-Supervised-Learning"><a href="#The-Illustrated-Self-Supervised-Learning" class="headerlink" title="The Illustrated Self-Supervised Learning"></a><a target="_blank" rel="noopener" href="https://amitness.com/2020/02/illustrated-self-supervised-learning/">The Illustrated Self-Supervised Learning</a></h1><p>I first got introduced to self-supervised learning in a <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=7I0Qt7GALVk&t=2639s">talk</a> by Yann Lecun, where he introduced the “cake analogy” to illustrate the importance of self-supervised learning. In the talk, he said:</p>
<blockquote>
<p>“If intelligence is a cake, the bulk of the cake is self-supervised learning, the icing on the cake is supervised learning, and the cherry on the cake is reinforcement learning (RL).”</p>
</blockquote>
<p>Though the analogy is <a target="_blank" rel="noopener" href="https://www.dropbox.com/s/fdw7q8mx3x4wr0c/2017_12_xx_NIPS-keynote-final.pdf?dl=0">debated</a>, we have seen the impact of self-supervised learning in the Natural Language Processing field where recent developments (Word2Vec, Glove, ELMO, BERT) have embraced self-supervision and achieved state of the art results.</p>
<p><img src="E:\笔记\markdown\reference\picture\self-supervised-nlp-to-vision-16504267629161.png" alt="img"></p>
<p>Curious to know the current state of self-supervised learning in the Computer Vision field, I read up on existing literature on self-supervised learning applied to computer vision through a <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.06162">recent survey paper</a> by Jing et. al.</p>
<p>In this post, I will explain what is self-supervised learning and summarize the patterns of problem formulation being used in self-supervised learning with visualizations.</p>
<h2 id="Why-Self-Supervised-Learning"><a href="#Why-Self-Supervised-Learning" class="headerlink" title="Why Self-Supervised Learning?"></a>Why Self-Supervised Learning?</h2><p>To apply supervised learning with deep neural networks, we need enough labeled data. To acquire that, human annotators manually label data which is both a time consuming and expensive process. There are also fields such as the medical field where getting enough data is a challenge itself. Thus, a major bottleneck in current supervised learning paradigm is the label generation part.</p>
<p><img src="E:\笔记\markdown\reference\picture\supervised-manual-annotation-16504267629163.png" alt="Manual Annotation in Supervised Learning"></p>
<h2 id="What-is-Self-Supervised-Learning"><a href="#What-is-Self-Supervised-Learning" class="headerlink" title="What is Self-Supervised Learning?"></a>What is Self-Supervised Learning?</h2><p>Self supervised learning is a method that poses the following question to formulate an unsupervised learning problem as a supervised one:</p>
<blockquote>
<p>Can we design the task in such a way that we can generate virtually unlimited labels from our existing images and use that to learn the representations?</p>
</blockquote>
<p><img src="E:\笔记\markdown\reference\picture\supervised-automated-16504267629165.png" alt="Automating manual labeling with Self Supervised Learning"></p>
<p>In self-supervised learning, we replace the human annotation block by creatively exploiting some property of data to set up a pseudo-supervised task. For example, here instead of labeling images as cat/dog, we could instead rotate them by 0/90/180/270 degrees and train a model to predict rotation. We can generate virtually unlimited training data from millions of images we have freely available on the internet.</p>
<p><img src="E:\笔记\markdown\reference\picture\self-supervised-workflow-16504267629167.png" alt="Self-supervised Learning Workflow Diagram"></p>
<p>Figure: End to End Workflow of Self-Supervised Learning</p>
<p>Once we learn representations from these millions of images, we can use transfer learning to fine-tune it on some supervised task like image classification of cats vs dogs with very few examples.</p>
<p><img src="E:\笔记\markdown\reference\picture\self-supervised-finetuning-16504267629169.png" alt="img"></p>
<h2 id="Survey-of-Self-Supervised-Learning-Methods"><a href="#Survey-of-Self-Supervised-Learning-Methods" class="headerlink" title="Survey of Self-Supervised Learning Methods"></a>Survey of Self-Supervised Learning Methods</h2><p>Let’s now understand the various approaches researchers have proposed to exploit image and video properties and apply self-supervised learning for representation learning.</p>
<h3 id="A-Self-Supervised-Learning-from-Image"><a href="#A-Self-Supervised-Learning-from-Image" class="headerlink" title="A. Self-Supervised Learning from Image"></a>A. Self-Supervised Learning from Image</h3><h4 id="Pattern-1-Reconstruction"><a href="#Pattern-1-Reconstruction" class="headerlink" title="Pattern 1: Reconstruction"></a>Pattern 1: Reconstruction</h4><h5 id="1-Image-Colorization"><a href="#1-Image-Colorization" class="headerlink" title="1. Image Colorization"></a>1. <a target="_blank" rel="noopener" href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#1-image-colorization"><strong>Image Colorization</strong></a></h5><p>Formulation:</p>
<blockquote>
<p>What if we prepared pairs of (grayscale, colorized) images by applying grayscale to millions of images we have freely available?</p>
</blockquote>
<p><img src="E:\笔记\markdown\reference\picture\ss-colorization-data-gen-165042676291611.png" alt="Data Generation for Image Colorization"></p>
<p>We could use an encoder-decoder architecture based on a fully convolutional neural network and compute the L2 loss between the predicted and actual color images.</p>
<p><img src="E:\笔记\markdown\reference\picture\ss-image-colorization-165042676291613.png" alt="Architecture for Image Colorization"></p>
<p>To solve this task, the model has to learn about different objects present in the image and related parts so that it can paint those parts in the same color. Thus, representations learned are useful for downstream tasks.<img src="E:\笔记\markdown\reference\picture\ss-colorization-learning-165042676291615.png" alt="Learning to colorize images"></p>
<p><strong>Papers:</strong><br>        <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1603.08511">Colorful Image Colorization</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1705.02999">Real-Time User-Guided Image Colorization with Learned Deep Priors</a> | <a target="_blank" rel="noopener" href="http://iizuka.cs.tsukuba.ac.jp/projects/colorization/en/">Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification</a></p>
<h5 id="2-Image-Superresolution"><a href="#2-Image-Superresolution" class="headerlink" title="2. Image Superresolution"></a>2. <a target="_blank" rel="noopener" href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#2-image-superresolution"><strong>Image Superresolution</strong></a></h5><p>Formulation:</p>
<blockquote>
<p>What if we prepared training pairs of (small, upscaled) images by downsampling millions of images we have freely available?</p>
</blockquote>
<p><img src="E:\笔记\markdown\reference\picture\ss-superresolution-training-gen-165042676291617.png" alt="Training Data for Superresolution"></p>
<p>GAN based models such as <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1609.04802">SRGAN</a> are popular for this task. A generator takes a low-resolution image and outputs a high-resolution image using a fully convolutional network. The actual and generated images are compared using both mean-squared-error and content loss to imitate human-like quality comparison. A binary-classification discriminator takes an image and classifies whether it’s an actual high-resolution image(1) or a fake generated superresolution image(0). This interplay between the two models leads to generator learning to produce images with fine details.</p>
<p><img src="E:\笔记\markdown\reference\picture\ss-srgan-architecture-165042676291619.png" alt="Architecture for SRGAN"></p>
<p>Both generator and discriminator learn semantic features that can be used for downstream tasks.</p>
<p><strong>Papers</strong>:<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1609.04802">Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</a></p>
<h5 id="3-Image-Inpainting"><a href="#3-Image-Inpainting" class="headerlink" title="3. Image Inpainting"></a>3. <a target="_blank" rel="noopener" href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#3-image-inpainting"><strong>Image Inpainting</strong></a></h5><p>Formulation:</p>
<blockquote>
<p>What if we prepared training pairs of (corrupted, fixed) images by randomly removing part of images?</p>
</blockquote>
<p><img src="E:\笔记\markdown\reference\picture\ss-image-inpainting-data-gen-165042676291621.png" alt="Training Data for Image Inpainting"></p>
<p>Similar to superresolution, we can leverage a GAN-based architecture where the Generator can learn to reconstruct the image while discriminator separates real and generated images.</p>
<p><img src="E:\笔记\markdown\reference\picture\ss-inpainting-architecture-165042676291723.png" alt="Architecture for Image Inpainting"></p>
<p>For downstream tasks, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1604.07379">Pathak et al.</a> have shown that semantic features learned by such a generator give 10.2% improvement over random initialization on the <a target="_blank" rel="noopener" href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html">PASCAL VOC 2012</a> semantic segmentation challenge while giving &lt;4% improvements over classification and object detection.</p>
<p><strong>Papers</strong>:<br>        <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1604.07379">Context encoders: Feature learning by inpainting</a></p>
<h5 id="4-Cross-Channel-Prediction"><a href="#4-Cross-Channel-Prediction" class="headerlink" title="4. Cross-Channel Prediction"></a>4. <a target="_blank" rel="noopener" href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#4-cross-channel-prediction"><strong>Cross-Channel Prediction</strong></a></h5><p>Formulation:</p>
<blockquote>
<p>What if we predict one channel of the image from the other channel and combine them to reconstruct the original image?</p>
</blockquote>
<p>Zhang et al. used this idea in their paper called “Split-Brain Autoencoder”. To understand the idea of the paper, let’s take an example of a color image of tomato.</p>
<p><img src="E:\笔记\markdown\reference\picture\split-brain-autoencoder-165042676291725.png" alt="img"></p>
<p>Example adapted from “Split-Brain Autoencoder” paper</p>
<p>For this color image, we can split it into grayscale and color channels. Then, for the grayscale channel, we predict the color channel and for the color channel part, we predict the grayscale channel. The two predicted channels $X_1$ and $X_2$ are combined to get back a reconstruction of the original image. We can compare this reconstruction to the original color image to get a loss and improve the model.</p>
<p>This same setup can be applied for images with depth as well where we use the color channels and the depth channels from a RGB-HHA image to predict each other and compare output image and original image.</p>
<p><img src="E:\笔记\markdown\reference\picture\split-brain-autoencoder-rgbhha-165042676291727.png" alt="img"></p>
<p>Example adapted from “Split-Brain Autoencoder” paper</p>
<p><strong>Papers</strong>:<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.09842">Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction</a></p>
<h4 id="Pattern-2-Common-Sense-Tasks"><a href="#Pattern-2-Common-Sense-Tasks" class="headerlink" title="Pattern 2: Common Sense Tasks"></a>Pattern 2: Common Sense Tasks</h4><h5 id="1-Image-Jigsaw-Puzzle"><a href="#1-Image-Jigsaw-Puzzle" class="headerlink" title="1. Image Jigsaw Puzzle"></a>1. <a target="_blank" rel="noopener" href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#1-image-jigsaw-puzzle"><strong>Image Jigsaw Puzzle</strong></a></h5><p>Formulation:</p>
<blockquote>
<p>What if we prepared training pairs of (shuffled, ordered) puzzles by randomly shuffling patches of images?</p>
</blockquote>
<p><img src="E:\笔记\markdown\reference\picture\ss-image-jigsaw-data-165042676291729.png" alt="Training Data For Image Jigsaw Puzzle"></p>
<p>Even with only 9 patches, there can be 362880 possible puzzles. To overcome this, only a subset of possible permutations is used such as 64 permutations with the highest hamming distance.</p>
<p><img src="E:\笔记\markdown\reference\picture\ss-jigsaw-permutations-165042676291731.png" alt="Number of Permutations in Image Jigsaw"></p>
<p>Suppose we use a permutation that changes the image as shown below. Let’s use the permutation number 64 from our total available 64 permutations.</p>
<p><img src="E:\笔记\markdown\reference\picture\ss-jigsaw-permutation-64-165042676291733.png" alt="Example of single permutation in jigsaw"></p>
<p>Now, to recover back the original patches, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1603.09246">Noroozi et al.</a> proposed a neural network called context-free network (CFN) as shown below. Here, the individual patches are passed through the same siamese convolutional layers that have shared weights. Then, the features are combined in a fully-connected layer. In the output, the model has to predict which permutation was used from the 64 possible classes. If we know the permutation, we can solve the puzzle.</p>
<p><img src="E:\笔记\markdown\reference\picture\ss-jigsaw-architecture-165042676291735.png" alt="Architecture for Image Jigsaw Task"></p>
<p>To solve the Jigsaw puzzle, the model needs to learn to identify how parts are assembled in an object, relative positions of different parts of objects and shape of objects. Thus, the representations are useful for downstream tasks in classification and detection.</p>
<p><strong>Papers</strong>:<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1603.09246">Unsupervised learning of visual representations by solving jigsaw puzzles</a></p>
<h5 id="2-Context-Prediction"><a href="#2-Context-Prediction" class="headerlink" title="2. Context Prediction"></a>2. <a target="_blank" rel="noopener" href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#2-context-prediction"><strong>Context Prediction</strong></a></h5><p>Formulation:</p>
<blockquote>
<p>What if we prepared training pairs of (image-patch, neighbor) by randomly taking an image patch and one of its neighbors around it from large, unlabeled image collection?</p>
</blockquote>
<p><img src="E:\笔记\markdown\reference\picture\ss-context-prediction-gen-165042676291737.png" alt="Training Data for Context Prediction"></p>
<p>To solve this pre-text task, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1505.05192">Doersch et al.</a> used an architecture similar to that of a jigsaw puzzle. We pass the patches through two siamese ConvNets to extract features, concatenate the features and do a classification over 8 classes denoting the 8 possible neighbor positions.</p>
<p><img src="E:\笔记\markdown\reference\picture\ss-context-prediction-architecture-165042676291739.png" alt="Architecture for Context Prediction"></p>
<p><strong>Papers</strong>:<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1505.05192">Unsupervised Visual Representation Learning by Context Prediction</a></p>
<h5 id="3-Geometric-Transformation-Recognition"><a href="#3-Geometric-Transformation-Recognition" class="headerlink" title="3. Geometric Transformation Recognition"></a>3. <a target="_blank" rel="noopener" href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#3-geometric-transformation-recognition"><strong>Geometric Transformation Recognition</strong></a></h5><p>Formulation:</p>
<blockquote>
<p>What if we prepared training pairs of (rotated-image, rotation-angle) by randomly rotating images by (0, 90, 180, 270) from large, unlabeled image collection?</p>
</blockquote>
<p><img src="E:\笔记\markdown\reference\picture\ss-geometric-transformation-gen-165042676291741.png" alt="Training Data for Geometric Transformation"></p>
<p>To solve this pre-text task, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.07728">Gidaris et al.</a> propose an architecture where a rotated image is passed through a ConvNet and the network has to classify it into 4 classes(0/90/270/360 degrees).<img src="E:\笔记\markdown\reference\picture\ss-geometric-transformation-architecture-165042676291743.png" alt="Architecture for Geometric Transformation Predction"></p>
<p>Though a very simple idea, the model has to understand location, types and pose of objects in an image to solve this task and as such, the representations learned are useful for downstream tasks.</p>
<p><strong>Papers</strong>:<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.07728">Unsupervised Representation Learning by Predicting Image Rotations</a></p>
<h4 id="Pattern-3-Automatic-Label-Generation"><a href="#Pattern-3-Automatic-Label-Generation" class="headerlink" title="Pattern 3: Automatic Label Generation"></a>Pattern 3: Automatic Label Generation</h4><h5 id="1-Image-Clustering"><a href="#1-Image-Clustering" class="headerlink" title="1. Image Clustering"></a>1. <a target="_blank" rel="noopener" href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#1-image-clustering"><strong>Image Clustering</strong></a></h5><p>Formulation:</p>
<blockquote>
<p>What if we prepared training pairs of (image, cluster-number) by performing clustering on large, unlabeled image collection?</p>
</blockquote>
<p><img src="E:\笔记\markdown\reference\picture\ss-image-clustering-gen-165042676291845.png" alt="Training Data for Image Clustering"></p>
<p>To solve this pre-text task, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.05520">Caron et al.</a> propose an architecture called deep clustering. Here, the images are first clustered and the clusters are used as classes. The task of the ConvNet is to predict the cluster label for an input image.<img src="E:\笔记\markdown\reference\picture\ss-deep-clustering-architecture-165042676291847.png" alt="Architecture for Deep Clustering"></p>
<p><strong>Papers</strong>:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://amitness.com/2020/04/deepcluster/">Deep clustering for unsupervised learning of visual features</a></li>
<li><a target="_blank" rel="noopener" href="https://amitness.com/2020/04/illustrated-self-labelling/">Self-labelling via simultaneous clustering and representation learning</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1608.08792">CliqueCNN: Deep Unsupervised Exemplar Learning</a></li>
</ul>
<h5 id="2-Synthetic-Imagery"><a href="#2-Synthetic-Imagery" class="headerlink" title="2. Synthetic Imagery"></a>2. <a target="_blank" rel="noopener" href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#2-synthetic-imagery"><strong>Synthetic Imagery</strong></a></h5><p>Formulation:</p>
<blockquote>
<p>What if we prepared training pairs of (image, properties) by generating synthetic images using game engines and adapting it to real images?</p>
</blockquote>
<p><img src="E:\笔记\markdown\reference\picture\synthetic-imagery-data-165042676291849.png" alt="Training Data for Sythetic Imagery"></p>
<p>To solve this pre-text task, <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1711.09082.pdf">Ren et al.</a> propose an architecture where weight-shared ConvNets are trained on both synthetic and real images and then a discriminator learns to classify whether ConvNet features fed to it is of a synthetic image or a real image. Due to adversarial nature, the shared representations between real and synthetic images get better.<img src="E:\笔记\markdown\reference\picture\ss-synthetic-image-architecture-165042676291851.png" alt="Architecture for Synthetic Image Training"></p>
<h3 id="B-Self-Supervised-Learning-From-Video"><a href="#B-Self-Supervised-Learning-From-Video" class="headerlink" title="B. Self-Supervised Learning From Video"></a>B. Self-Supervised Learning From Video</h3><h4 id="1-Frame-Order-Verification"><a href="#1-Frame-Order-Verification" class="headerlink" title="1. Frame Order Verification"></a>1. <a target="_blank" rel="noopener" href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#1-frame-order-verification"><strong>Frame Order Verification</strong></a></h4><p>Formulation:</p>
<blockquote>
<p>What if we prepared training pairs of (video frames, correct/incorrect order) by shuffling frames from videos of objects in motion?</p>
</blockquote>
<p><img src="E:\笔记\markdown\reference\picture\ss-frame-order-data-gen-165042676291853.png" alt="Training Data for Video Order"></p>
<p>To solve this pre-text task, <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1711.09082.pdf">Misra et al.</a> propose an architecture where video frames are passed through weight-shared ConvNets and the model has to figure out whether the frames are in the correct order or not. In doing so, the model learns not just spatial features but also takes into account temporal features.</p>
<p><img src="E:\笔记\markdown\reference\picture\ss-temporal-order-architecture-165042676291855.png" alt="Architecture for Frame Order Verification"></p>
<p><strong>Papers</strong>:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1603.08561">Shuffle and Learn: Unsupervised Learning using Temporal Order Verification</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.06646">Self-Supervised Video Representation Learning With Odd-One-Out Networks</a></li>
</ul>
<h2 id="Citation-Info-BibTex"><a href="#Citation-Info-BibTex" class="headerlink" title="Citation Info (BibTex)"></a><a target="_blank" rel="noopener" href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#citation-info-bibtex">Citation Info (BibTex)</a></h2><p>If you found this blog post useful, please consider citing it as:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@misc&#123;chaudhary2020selfsupervised,</span><br><span class="line">  title   = &#123;The Illustrated Self-Supervised Learning&#125;,</span><br><span class="line">  author  = &#123;Amit Chaudhary&#125;,</span><br><span class="line">  year    = 2020,</span><br><span class="line">  note    = &#123;\url&#123;https://amitness.com/2020/02/illustrated-self-supervised-learning&#125;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="References"><a href="#References" class="headerlink" title="References"></a><a target="_blank" rel="noopener" href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#references">References</a></h2><ul>
<li>Jing, et al. “<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.06162">Self-Supervised Visual Feature Learning with Deep Neural Networks: A Survey.</a>”</li>
</ul>

      
    </div>
    
  </div>
  
    
    <div class="copyright">
        <p><span>本文标题:</span><a href="/2022/04/20/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"></a></p>
        <p><span>文章作者:</span><a href="/" title="回到主页">流沙</a></p>
        <p><span>发布时间:</span>2022-04-20, 22:38:48</p>
        <p><span>最后更新:</span>2022-04-20, 22:39:50</p>
        <p>
            <span>原始链接:</span><a class="post-url" href="/2022/04/20/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" title="">https://jpccc.github.io/2022/04/20/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</a>
            <span class="copy-path" data-clipboard-text="原文: https://jpccc.github.io/2022/04/20/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/　　作者: " title="点击复制文章链接"><i class="fa fa-clipboard"></i></span>
            <script> var clipboard = new Clipboard('.copy-path'); </script>
        </p>
        <p>
            <span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="CC BY-NC-SA 4.0 International" target = "_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。
        </p>
    </div>



    <nav id="article-nav">
        
            <div id="article-nav-newer" class="article-nav-title">
                <a href="/2022/04/20/faster%20rcnn/">
                    
                </a>
            </div>
        
        
            <div id="article-nav-older" class="article-nav-title">
                <a href="/2022/04/20/%E8%87%AA%E7%9B%91%E7%9D%A3/">
                    
                </a>
            </div>
        
    </nav>

  
</article>

    <div id="toc" class="toc-article">
        <strong class="toc-title">文章目录</strong>
        
            <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#The-Illustrated-Self-Supervised-Learning"><span class="toc-number">1.</span> <span class="toc-text">The Illustrated Self-Supervised Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Why-Self-Supervised-Learning"><span class="toc-number">1.1.</span> <span class="toc-text">Why Self-Supervised Learning?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#What-is-Self-Supervised-Learning"><span class="toc-number">1.2.</span> <span class="toc-text">What is Self-Supervised Learning?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Survey-of-Self-Supervised-Learning-Methods"><span class="toc-number">1.3.</span> <span class="toc-text">Survey of Self-Supervised Learning Methods</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#A-Self-Supervised-Learning-from-Image"><span class="toc-number">1.3.1.</span> <span class="toc-text">A. Self-Supervised Learning from Image</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Pattern-1-Reconstruction"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">Pattern 1: Reconstruction</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-Image-Colorization"><span class="toc-number">1.3.1.1.1.</span> <span class="toc-text">1. Image Colorization</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-Image-Superresolution"><span class="toc-number">1.3.1.1.2.</span> <span class="toc-text">2. Image Superresolution</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-Image-Inpainting"><span class="toc-number">1.3.1.1.3.</span> <span class="toc-text">3. Image Inpainting</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-Cross-Channel-Prediction"><span class="toc-number">1.3.1.1.4.</span> <span class="toc-text">4. Cross-Channel Prediction</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Pattern-2-Common-Sense-Tasks"><span class="toc-number">1.3.1.2.</span> <span class="toc-text">Pattern 2: Common Sense Tasks</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-Image-Jigsaw-Puzzle"><span class="toc-number">1.3.1.2.1.</span> <span class="toc-text">1. Image Jigsaw Puzzle</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-Context-Prediction"><span class="toc-number">1.3.1.2.2.</span> <span class="toc-text">2. Context Prediction</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-Geometric-Transformation-Recognition"><span class="toc-number">1.3.1.2.3.</span> <span class="toc-text">3. Geometric Transformation Recognition</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Pattern-3-Automatic-Label-Generation"><span class="toc-number">1.3.1.3.</span> <span class="toc-text">Pattern 3: Automatic Label Generation</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-Image-Clustering"><span class="toc-number">1.3.1.3.1.</span> <span class="toc-text">1. Image Clustering</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-Synthetic-Imagery"><span class="toc-number">1.3.1.3.2.</span> <span class="toc-text">2. Synthetic Imagery</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#B-Self-Supervised-Learning-From-Video"><span class="toc-number">1.3.2.</span> <span class="toc-text">B. Self-Supervised Learning From Video</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Frame-Order-Verification"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">1. Frame Order Verification</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Citation-Info-BibTex"><span class="toc-number">1.4.</span> <span class="toc-text">Citation Info (BibTex)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#References"><span class="toc-number">1.5.</span> <span class="toc-text">References</span></a></li></ol></li></ol>
        
    </div>
    <style>
        .left-col .switch-btn,
        .left-col .switch-area {
            display: none;
        }
        .toc-level-3 i,
        .toc-level-3 ol {
            display: none !important;
        }
    </style>

    <input type="button" id="tocButton" value="隐藏目录"  title="点击按钮隐藏或者显示文章目录">

    <script>
        yiliaConfig.toc = ["隐藏目录", "显示目录", !!"false"];
    </script>



    
<div class="share">
    

    
</div>







    




    <div class="scroll" id="post-nav-button">
        
            <a href="/2022/04/20/faster%20rcnn/" title="上一篇: ">
                <i class="fa fa-angle-left"></i>
            </a>
        

        <a title="文章列表"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a>

        
            <a href="/2022/04/20/%E8%87%AA%E7%9B%91%E7%9D%A3/" title="下一篇: ">
                <i class="fa fa-angle-right"></i>
            </a>
        
    </div>

    <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2030/04/07/%E5%BF%83%E5%BE%97/">心得</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/20/RCNN/">RCNN</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/20/faster%20rcnn/">faster rcnn</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/20/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/">无监督学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/20/%E8%87%AA%E7%9B%91%E7%9D%A3/">自监督</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/11/%E5%9B%BE%E5%83%8F%E7%9A%84%E6%93%8D%E4%BD%9C/">图像的操作</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/07/%E7%89%B9%E8%89%B2%E5%8C%85/">特色包</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/07/python%E8%AF%AD%E6%B3%95/">python语法</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/07/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/">聚类</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/07/pytorch%E4%BD%BF%E7%94%A8%E6%B1%87%E6%80%BB/">pytorch使用汇总</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20tricks/">深度学习训练tricks</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/06/pytorch%E8%AE%A1%E7%AE%97%E5%9B%BE/">Understanding Graphs, Automatic Differentiation and Autograd</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/28/Jensen-inequality/">Jensen_inequality</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/28/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0part1/">统计学习数学基础-1</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/25/BN%E7%AE%97%E6%B3%95/">BN算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/25/%E4%BA%92%E4%BF%A1%E6%81%AF/">互信息</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/22/algebra/">algebra</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/17/hello-world/">Hello World</a></li></ul>




    <script>
        
    </script>
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2021-2022  liusha
            </div>
            <div class="footer-right">
                回首向来萧瑟处，归去，也无风雨也无晴。
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_uv">
  本站访客数<span id="busuanzi_value_site_uv"></span>人次
</span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="本页阅读量"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>