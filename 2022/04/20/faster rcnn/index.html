<!DOCTYPE html>
<html lang="zh-Hans">
<head>

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no" />
<meta name="author" content=" liusha" />



<meta name="description" content="RCNN- 将CNN引入目标检测的开山之作RCNN (论文：Rich feature hierarchies for accurate object detection and semantic segmentation) 是将CNN方法引入目标检测领域， 大大提高了目标检测效果，可以说改变了目标检测领域的主要研究思路， 紧随其后的系列文章：Fast RCNN, Faster RCNN 。 【论文">
<meta property="og:type" content="article">
<meta property="og:title" content="流沙">
<meta property="og:url" content="https://jpccc.github.io/2022/04/20/faster%20rcnn/index.html">
<meta property="og:site_name" content="流沙">
<meta property="og:description" content="RCNN- 将CNN引入目标检测的开山之作RCNN (论文：Rich feature hierarchies for accurate object detection and semantic segmentation) 是将CNN方法引入目标检测领域， 大大提高了目标检测效果，可以说改变了目标检测领域的主要研究思路， 紧随其后的系列文章：Fast RCNN, Faster RCNN 。 【论文">
<meta property="og:locale">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-32e78b7f2e29c3e4e159a52ed38a6f73_720w.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-616af8e96637b9b3280e71e05877db59_720w.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-0659a27df35fd2f62cd00127ca8d1a21_720w.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-6fe13f10a9cb286f06aa1e3e2a2b29bc_720w.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-e26ffc0835bc30dede8d82989ef9e178_720w.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-19c03377416e437a288e29bd27e97c14_720w.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-1738e9bdb129fea5d46d73218606aebd_720w.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-59449e8409b943f384c4cc3bf789d8b9_720w.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-03e65630d303565dba3a997911e72881_720w.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-002f73d5bb38dfe66e39ff472aca6c31_720w.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-4a8097e292784ffaff747417b71c863d_720w.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-728cc0822b07a6db24468698463efb89_720w.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-f67cd928e318ec00bc6047075c88e0b8_720w.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-3ef21dd028fd210f92107c1ded528045_720w.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-7e2c472157f6a4028db9f8ba3c0eb744_720w.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-da59abcbe56803aeeef24ffb5131ce83_720w.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-523707e94ccb850ca4c23cc94054a144_720w.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-62c008799df798656236258c64082340_720w.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-d68eaa673b48c3176eb48b3cb16a761f_720w.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-237603b04a4f5f801924219f4fdfad99_720w.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-8c5eddc9f856822aad5ae8d030ce1779_720w.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-7a4ce0c60b8fcac5eb7ffe365f99572e_720w.png">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-ced13fd5cc13bcccfcaf3afa20dce95e_b.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-d6b522d5eb9ff43c3bcd66c8448844d1_b.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-be21dff7c814eb9d470d35805f965ebb_b.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-95e27f746e16ee25da88871c3114ff76_b.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L(p, u, t^u , v) = Lcls(p, u) + λ[u ≥ 1]Lloc(t^u, v)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=Lcls">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=Lloc">
<meta property="og:image" content="https://www.zhihu.com/equation?tex= Lcls(p, u)=−log(p_u)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p_u">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165157845950121.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165157845950122.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/in%7Bx%2Cy%2Cw%2Ch%7D%7D%7Bsmooth_%7BL_i%7D(t_i%5Eu-v_i)%7D.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/end%7Bequation%7D-165157845950123.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-5fbec4583ca50103e33d421463d3a3a0_b.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-7b188e68f559382e0e1ddce4d4dfb837_b.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165157845950124.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165157845950125.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-3d11b57e6904c18bc8bd89efc4a06bda_b.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-8781d554a612c8a2c6686f4ac0df7a22_b.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-ff6a2918f64d1977fad99bb840facd7b_b.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-f0af16bde4eb2271920696e00ed5d465_b.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-c0172be282021a1029f7b72b51079ffe_720w.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-e64a99b38f411c337f538eb5f093bdf3_720w.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-dc78d0ad1012ad65040bdb7eb657f381_180x120.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-3c772e9ed555eb86a97ef9c08bf563c9_720w.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-1908feeaba591d28bee3c4a754cca282_720w.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-8d72777321cbf1336b79d839b6c7f9fc_720w.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture%7D.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-7abead97efcc46a3ee5b030a2151643f_720w.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-c93db71cc8f4f4fd8cfb4ef2e2cef4f4_720w.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/tag%7B1%7D.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-4b15828dfee19be726835b671748cc4d_720w.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-1ab4b6c3dd607a5035b5203c76b078f3_720w.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-93021a3c03d66456150efa1da95416d3_720w.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-16504664534921.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-16504664534922.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-16504664534933.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-16504664534934.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-16504664534935.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-ea7e6e48662bfa68ec73bdf32f36bb85_720w.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/tag%7B2%7D.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/tag%7B3%7D.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/tag%7B4%7D.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/tag%7B5%7D.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-16504664534936.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-16504664534936.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-16504664534937.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-16504664534938.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/tag%7B6%7D.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/phi(A).svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-16504664534949.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349410.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-16504669589211.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349411.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/tag%7B7%7D.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/tag%7B8%7D.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349412.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349413.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/tag%7B9%7D.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/tag%7B10%7D.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349414.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-16504669589212.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-8241c8076d60156248916fe2f1a5674a_720w.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/tag%7B11%7D.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349415.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349416.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349417.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349418.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349519.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349519.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-1e43500c7cc9a9de211d737bc347ced9_720w.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349519.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-e525342cbde476a11c48a6be393f226c_720w.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/text%7Bpooled_h%7D.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/text%7Bpooled_h%7D.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-e3108dc5cdd76b871e21a4cb64001b5c_720w.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-9377a45dc8393d546b7b52a491414ded_720w.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-38594a97f33ff56fc72542a20a78116d_720w.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-f56d3209f9a7d5f27d77ead7489ab70f_720w.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-ddfcf3dc29976e384b047418aec9002d_720w.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-c39aef1d06e08e4e0cec96b10f50a779_720w.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/tag%7B12%7D.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349520.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349521.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349522.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349523.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349524.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349525.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349526.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349522.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/text%7Bcls%7D.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/text%7Breg%7D.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/text%7Bcls%7D%3D256.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/text%7Breg%7D%3D2400.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/approx10.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/text%7Breg%7D-165046645349627.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/tag%7B13%7D.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/tag%7B14%7D.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349628.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349629.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/text%7Bcls%7D.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-16504669589223.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-16504669589224.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349525.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349526.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-16504669589224.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/text%7Breg%7D.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-1ac5f8a2899ee413464ecf7866f8f840_720w.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-fbece817952865689187e68f0af86792_720w.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-9f4e9c49a8e59e08abe70f8ba9b14fef_ipico.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349630.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/frac%7B1%7D%7B16%7D.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349416.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-9d67146e0cb10397d8c2170794412608_720w.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349415.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349416.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349417.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349418.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-82196feb7b528d76411feb90bfec2af4_720w.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349731.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349732.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/equation-165046645349733.svg+xml">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-dc78d0ad1012ad65040bdb7eb657f381_180x120.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-2ea98126ebc05e35be28efe598a021ed_180x120.jpg">
<meta property="og:image" content="e:/笔记/markdown/reference/picture/v2-b7f1350a3179674d93d60d8ad9b52c0f_180x120.jpg">
<meta property="article:published_time" content="2022-04-20T14:54:10.912Z">
<meta property="article:modified_time" content="2022-05-05T06:40:44.510Z">
<meta property="article:author" content=" liusha">
<meta property="article:tag" content="人工智能、机器学习、深度学习、数据挖掘、深度聚类">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="e:/笔记/markdown/reference/picture/v2-32e78b7f2e29c3e4e159a52ed38a6f73_720w.png">

<link rel="apple-touch-icon" href= "/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="流沙" type="application/atom+xml">



    <link rel="shortcut icon" href="/favicon.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">



<link rel="stylesheet" href="/css/style.css">




<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>流沙</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: false,
        isPost: true,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: true
    }
</script>


    <script> yiliaConfig.jquery_ui = [false]; </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/avatar.png" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/"></a></h1>
        </hgroup>

        

        
            <form id="search-form">
            <input type="text" id="local-search-input" name="q" placeholder="search..." class="search form-control" autocomplete="off" autocorrect="off" searchonload="" />
            <i class="fa fa-times" onclick="resetSearch()"></i>
            </form>
            <div id="local-search-result"></div>
            <p class='no-result'>No results found <i class='fa fa-spinner fa-pulse'></i></p>
        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                        <li>关于我</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="https://jpccc.github.io/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/%E9%9A%8F%E7%AC%94">随笔</a></li>
                        
                            <li><a href="/tags/">标签云</a></li>
                        
                            <li><a href="/about/">关于我</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="mailto:jiangpc21@mails.glu.edu.cn" title="Email"></a>
                            
                                <a class="fa GitHub" href="#" title="GitHub"></a>
                            
                                <a class="fa RSS" href="/atom.xml" title="RSS"></a>
                            
                                <a class="fa QQ" href="/1778013127" title="QQ"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/deepLearning/" rel="tag">deepLearning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/math/" rel="tag">math</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/" rel="tag">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pytorch/" rel="tag">pytorch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%AE%97%E6%B3%95/" rel="tag">算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/" rel="tag">统计学习</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://hexo.io">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://pages.github.com/">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="http://moxfive.xyz/">MOxFIVE</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">机器学习</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页"></a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/avatar.png" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页"></a></h1>
            </hgroup>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="https://jpccc.github.io/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/%E9%9A%8F%E7%AC%94">随笔</a></li>
                
                    <li><a href="/tags/">标签云</a></li>
                
                    <li><a href="/about/">关于我</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="mailto:jiangpc21@mails.glu.edu.cn" title="Email"></a>
                            
                                <a class="fa GitHub" target="_blank" href="#" title="GitHub"></a>
                            
                                <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a>
                            
                                <a class="fa QQ" target="_blank" href="/1778013127" title="QQ"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="标签" friends="友情链接" about="关于我"/>
</nav>
      <div class="body-wrap"><article id="post-faster rcnn" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2022/04/20/faster%20rcnn/" class="article-date">
      <time datetime="2022-04-20T14:54:10.912Z" itemprop="datePublished">2022-04-20</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h1 id="RCNN-将CNN引入目标检测的开山之作"><a href="#RCNN-将CNN引入目标检测的开山之作" class="headerlink" title="RCNN- 将CNN引入目标检测的开山之作"></a>RCNN- 将CNN引入目标检测的开山之作</h1><p><a href="https://link.zhihu.com/?target=https://github.com/rbgirshick/rcnn">RCNN</a> (论文：Rich feature hierarchies for accurate object detection and semantic segmentation) 是将CNN方法引入目标检测领域， 大大提高了目标检测效果，可以说改变了目标检测领域的主要研究思路， 紧随其后的系列文章：<a href="https://link.zhihu.com/?target=https://github.com/rbgirshick/fast-rcnn">Fast RCNN</a>, <a href="https://link.zhihu.com/?target=https://github.com/ShaoqingRen/faster_rcnn">Faster RCNN</a> 。</p>
<p>【论文主要特点】（相对传统方法的改进）</p>
<ul>
<li>速度： 经典的目标检测算法使用滑动窗法依次判断所有可能的区域。本文则(采用Selective Search方法)预先提取一系列较可能是物体的候选区域，之后仅在这些候选区域上(采用CNN)提取特征，进行判断。</li>
<li>训练集： 经典的目标检测算法在区域中提取人工设定的特征。本文则采用深度网络进行特征提取。使用两个数据库： 一个较大的识别库（ImageNet ILSVC 2012）：标定每张图片中物体的类别。一千万图像，1000类。 一个较小的检测库（PASCAL VOC 2007）：标定每张图片中，物体的类别和位置，一万图像，20类。 本文使用识别库进行预训练得到CNN（有监督预训练），而后用检测库调优参数，最后在检测库上评测。</li>
</ul>
<p>看到这里也许你已经对很多名词很困惑，下面会解释。先来看看它的基本流程：</p>
<h2 id="【基本流程-】"><a href="#【基本流程-】" class="headerlink" title="【基本流程 ===================================】"></a>【基本流程 ===================================】</h2><p>RCNN算法分为4个步骤</p>
<ol>
<li>候选区域生成： 一张图像生成1K~2K个候选区域 （采用Selective Search 方法）</li>
<li>特征提取： 对每个候选区域，使用深度卷积网络提取特征 （CNN）</li>
<li>类别判断： 特征送入每一类的SVM 分类器，判别是否属于该类</li>
<li>位置精修： 使用回归器精细修正候选框位置</li>
</ol>
<p><img src="E:\笔记\markdown\reference\picture\v2-32e78b7f2e29c3e4e159a52ed38a6f73_720w.png" alt="img"></p>
<h2 id="【基础知识-】"><a href="#【基础知识-】" class="headerlink" title="【基础知识 ===================================】"></a>【基础知识 ===================================】</h2><h3 id="Selective-Search-主要思想"><a href="#Selective-Search-主要思想" class="headerlink" title="Selective Search 主要思想:"></a><strong><a href="https://link.zhihu.com/?target=http://koen.me/research/pub/uijlings-ijcv2013-draft.pdf">Selective Search</a> 主要思想:</strong></h3><ol>
<li>使用一种过分割手段，将图像分割成小区域 (1k~2k 个)</li>
<li>查看现有小区域，按照合并规则合并可能性最高的相邻两个区域。重复直到整张图像合并成一个区域位置</li>
<li>输出所有曾经存在过的区域，所谓候选区域</li>
</ol>
<p>其中合并规则如下： 优先合并以下四种区域：</p>
<ul>
<li>颜色（颜色直方图）相近的</li>
<li>纹理（梯度直方图）相近的</li>
<li>合并后总面积小的： 保证合并操作的尺度较为均匀，避免一个大区域陆续“吃掉”其他小区域 （例：设有区域a-b-c-d-e-f-g-h。较好的合并方式是：ab-cd-ef-gh -&gt; abcd-efgh -&gt; abcdefgh。 不好的合并方法是：ab-c-d-e-f-g-h -&gt;abcd-e-f-g-h -&gt;abcdef-gh -&gt; abcdefgh）</li>
<li>合并后，总面积在其BBOX中所占比例大的： 保证合并后形状规则。<br><img src="E:\笔记\markdown\reference\picture\v2-616af8e96637b9b3280e71e05877db59_720w.png" alt="img"></li>
</ul>
<p>上述四条规则只涉及区域的颜色直方图、梯度直方图、面积和位置。合并后的区域特征可以直接由子区域特征计算而来，速度较快。</p>
<h3 id="有监督预训练与无监督预训练"><a href="#有监督预训练与无监督预训练" class="headerlink" title="有监督预训练与无监督预训练:"></a><strong>有监督预训练与无监督预训练:</strong></h3><p>(1)无监督预训练(Unsupervised pre-training)</p>
<p>预训练阶段的样本不需要人工标注数据，所以就叫做无监督预训练。</p>
<p>(2)有监督预训练(Supervised pre-training)</p>
<p>所谓的有监督预训练也可以把它称之为迁移学习。比如你已经有一大堆标注好的人脸年龄分类的图片数据，训练了一个CNN，用于人脸的年龄识别。然后当你遇到新的项目任务时：人脸性别识别，那么这个时候你可以利用已经训练好的年龄识别CNN模型，去掉最后一层，然后其它的网络层参数就直接复制过来，继续进行训练，让它输出性别。这就是所谓的迁移学习，说的简单一点就是把一个任务训练好的参数，拿到另外一个任务，作为神经网络的初始参数值,这样相比于你直接采用随机初始化的方法，精度可以有很大的提高。</p>
<p>对于目标检测问题： 图片分类标注好的训练数据非常多，但是物体检测的标注数据却很少，如何用少量的标注数据，训练高质量的模型，这就是文献最大的特点，这篇论文采用了迁移学习的思想： 先用了ILSVRC2012这个训练数据库（这是一个图片分类训练数据库），先进行网络图片<strong>分类</strong>训练。这个数据库有大量的标注数据，共包含了1000种类别物体，因此预训练阶段CNN模型的输出是1000个神经元（当然也直接可以采用Alexnet训练好的模型参数）。</p>
<h3 id="重叠度（IOU）"><a href="#重叠度（IOU）" class="headerlink" title="重叠度（IOU）:"></a><strong>重叠度（IOU）:</strong></h3><p>物体检测需要定位出物体的bounding box，就像下面的图片一样，我们不仅要定位出车辆的bounding box 我们还要识别出bounding box 里面的物体就是车辆。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-0659a27df35fd2f62cd00127ca8d1a21_720w.png" alt="img"></p>
<p>对于bounding box的定位精度，有一个很重要的概念： 因为我们算法不可能百分百跟人工标注的数据完全匹配，因此就存在一个定位精度评价公式：IOU。 它定义了两个bounding box的重叠度，如下图所示</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-6fe13f10a9cb286f06aa1e3e2a2b29bc_720w.png" alt="img"></p>
<p><img src="E:\笔记\markdown\reference\picture\v2-e26ffc0835bc30dede8d82989ef9e178_720w.png" alt="img"></p>
<p>就是矩形框A、B的重叠面积占A、B并集的面积比例。</p>
<h3 id="非极大值抑制（NMS）："><a href="#非极大值抑制（NMS）：" class="headerlink" title="非极大值抑制（NMS）："></a><strong>非极大值抑制（</strong>NMS<strong>）：</strong></h3><p>RCNN会从一张图片中找出n个可能是物体的矩形框，然后为每个矩形框做类别分类概率：</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-19c03377416e437a288e29bd27e97c14_720w.png" alt="img"></p>
<p>就像上面的图片一样，定位一个车辆，最后算法就找出了一堆的方框，我们需要判别哪些矩形框是没用的。非极大值抑制的方法是：先假设有6个矩形框，根据分类器的类别分类概率做排序，假设从小到大属于车辆的概率 分别为A、B、C、D、E、F。</p>
<p>(1)从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;</p>
<p>(2)假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。</p>
<p>(3)从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。</p>
<p>就这样一直重复，找到所有被保留下来的矩形框。</p>
<p>非极大值抑制（NMS）顾名思义就是抑制不是极大值的元素，搜索局部的极大值。这个局部代表的是一个邻域，邻域有两个参数可变，一是邻域的维数，二是邻域的大小。这里不讨论通用的NMS算法，而于在目标检测中用于提取分数最高的窗口的。例如在行人检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到NMS来选取那些邻域里分数最高（是行人的概率最大），并且抑制那些分数低的窗口。</p>
<h3 id="VOC物体检测任务"><a href="#VOC物体检测任务" class="headerlink" title="VOC物体检测任务:"></a><strong>VOC物体检测任务:</strong></h3><p>相当于一个竞赛，里面包含了20个物体类别：<a href="https://link.zhihu.com/?target=http://host.robots.ox.ac.uk/pascal/VOC/voc2012/examples/index.html">PASCAL VOC2011 Example Images</a> 还有一个背景，总共就相当于21个类别，因此一会设计fine-tuning CNN的时候，我们softmax分类输出层为21个神经元。</p>
<h2 id="【各个阶段详解-】"><a href="#【各个阶段详解-】" class="headerlink" title="【各个阶段详解 ===================================】"></a>【各个阶段详解 ===================================】</h2><p>总体思路再回顾：</p>
<p>首先对每一个输入的图片产生近2000个不分种类的候选区域（region proposals），然后使用CNNs从每个候选框中提取一个固定长度的特征向量（4096维度），接着对每个取出的特征向量使用特定种类的线性SVM进行分类。也就是总个过程分为三个程序：<strong>a、找出候选框；b、利用CNN提取特征向量；c、利用SVM进行特征向量分类。</strong></p>
<p><img src="E:\笔记\markdown\reference\picture\v2-1738e9bdb129fea5d46d73218606aebd_720w.png" alt="img"></p>
<h3 id="候选框搜索阶段："><a href="#候选框搜索阶段：" class="headerlink" title="候选框搜索阶段："></a><strong>候选框搜索阶段：</strong></h3><p>当我们输入一张图片时，我们要搜索出所有可能是物体的区域，这里采用的就是前面提到的Selective Search方法，通过这个算法我们搜索出2000个候选框。然后从上面的总流程图中可以看到，搜出的候选框是矩形的，而且是大小各不相同。然而CNN对输入图片的大小是有固定的，如果把搜索到的矩形选框不做处理，就扔进CNN中，肯定不行。因此对于每个输入的候选框都需要缩放到固定的大小。下面我们讲解要怎么进行缩放处理，为了简单起见我们假设下一阶段CNN所需要的输入图片大小是个正方形图片227*227。因为我们经过selective search 得到的是矩形框，paper试验了两种不同的处理方法：</p>
<p>(1)各向异性缩放</p>
<p>这种方法很简单，就是不管图片的长宽比例，管它是否扭曲，进行缩放就是了，全部缩放到CNN输入的大小227*227，如下图(D)所示；</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-59449e8409b943f384c4cc3bf789d8b9_720w.png" alt="img"></p>
<p>(2)各向同性缩放</p>
<p>因为图片扭曲后，估计会对后续CNN的训练精度有影响，于是作者也测试了“各向同性缩放”方案。有两种办法</p>
<p>A、先扩充后裁剪： 直接在原始图片中，把bounding box的边界进行扩展延伸成正方形，然后再进行裁剪；如果已经延伸到了原始图片的外边界，那么就用bounding box中的颜色均值填充；如上图(B)所示;</p>
<p>B、先裁剪后扩充：先把bounding box图片裁剪出来，然后用固定的背景颜色填充成正方形图片(背景颜色也是采用bounding box的像素颜色均值),如上图(C)所示;</p>
<p>对于上面的异性、同性缩放，文献还有个padding处理，上面的示意图中第1、3行就是结合了padding=0,第2、4行结果图采用padding=16的结果。经过最后的试验，作者发现采用各向异性缩放、padding=16的精度最高。</p>
<p>（备注：候选框的搜索策略作者也考虑过使用一个滑动窗口的方法，然而由于更深的网络，更大的输入图片和滑动步长，使得使用滑动窗口来定位的方法充满了挑战。）</p>
<h3 id="CNN特征提取阶段："><a href="#CNN特征提取阶段：" class="headerlink" title="CNN特征提取阶段："></a><strong>CNN特征提取阶段：</strong></h3><p><strong>1、算法实现</strong></p>
<p>a、网络结构设计阶段</p>
<p>网络架构两个可选方案：第一选择经典的Alexnet；第二选择VGG16。经过测试Alexnet精度为58.5%，VGG16精度为66%。VGG这个模型的特点是选择比较小的卷积核、选择较小的跨步，这个网络的精度高，不过计算量是Alexnet的7倍。后面为了简单起见，我们就直接选用Alexnet，并进行讲解；Alexnet特征提取部分包含了5个卷积层、2个全连接层，在Alexnet中p5层神经元个数为9216、 f6、f7的神经元个数都是4096，通过这个网络训练完毕后，最后提取特征每个输入候选框图片都能得到一个4096维的特征向量。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-03e65630d303565dba3a997911e72881_720w.png" alt="img"></p>
<p><img src="E:\笔记\markdown\reference\picture\v2-002f73d5bb38dfe66e39ff472aca6c31_720w.png" alt="img"></p>
<p>b、网络有监督预训练阶段 （图片数据库：ImageNet ILSVC ）</p>
<p>参数初始化部分：物体检测的一个难点在于，物体标签训练数据少，如果要直接采用随机初始化CNN参数的方法，那么目前的训练数据量是远远不够的。这种情况下，最好的是采用某些方法，把参数初始化了，然后在进行有监督的参数微调，这里文献采用的是有监督的预训练。所以paper在设计网络结构的时候，是直接用Alexnet的网络，然后连参数也是直接采用它的参数，作为初始的参数值，然后再fine-tuning训练。网络优化求解时采用随机梯度下降法，学习率大小为0.001；</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-4a8097e292784ffaff747417b71c863d_720w.png" alt="img"></p>
<p>C、fine-tuning阶段 （图片数据库： PASCAL VOC）</p>
<p>我们接着采用 selective search 搜索出来的候选框 （PASCAL VOC 数据库中的图片） 继续对上面预训练的CNN模型进行fine-tuning训练。假设要检测的物体类别有N类，那么我们就需要把上面预训练阶段的CNN模型的最后一层给替换掉，替换成N+1个输出的神经元(加1，表示还有一个背景) (20 + 1bg = 21)，然后这一层直接采用参数随机初始化的方法，其它网络层的参数不变；接着就可以开始继续SGD训练了。开始的时候，SGD学习率选择0.001，在每次训练的时候，我们batch size大小选择128，其中32个事正样本、96个事负样本。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-728cc0822b07a6db24468698463efb89_720w.png" alt="img"></p>
<p>关于正负样本问题：</p>
<p>一张照片我们得到了2000个候选框。然而人工标注的数据一张图片中就只标注了正确的bounding box，我们搜索出来的2000个矩形框也不可能会出现一个与人工标注完全匹配的候选框。因此在CNN阶段我们需要用IOU为2000个bounding box打标签。如果用selective search挑选出来的候选框与物体的人工标注矩形框（PASCAL VOC的图片都有人工标注）的重叠区域IoU大于0.5，那么我们就把这个候选框标注成物体类别（正样本），否则我们就把它当做背景类别（负样本）。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-f67cd928e318ec00bc6047075c88e0b8_720w.png" alt="img"></p>
<p>（备注： 如果不针对特定任务进行fine-tuning，而是把CNN当做特征提取器，卷积层所学到的特征其实就是基础的共享特征提取层，就类似于SIFT算法一样，可以用于提取各种图片的特征，而f6、f7所学习到的特征是用于针对特定任务的特征。打个比方：对于人脸性别识别来说，一个CNN模型前面的卷积层所学习到的特征就类似于学习人脸共性特征，然后全连接层所学习的特征就是针对性别分类的特征了）</p>
<p><strong>2. 疑惑点</strong>： CNN训练的时候，本来就是对bounding box的物体进行识别分类训练，在训练的时候最后一层softmax就是分类层。那么为什么作者闲着没事干要先用CNN做特征提取（提取fc7层数据），然后再把提取的特征用于训练svm分类器？</p>
<p>这个是因为svm训练和cnn训练过程的正负样本定义方式各有不同，导致最后采用CNN softmax输出比采用svm精度还低。事情是这样的，cnn在训练的时候，对训练数据做了比较宽松的标注，比如一个bounding box可能只包含物体的一部分，那么我也把它标注为正样本，用于训练cnn；采用这个方法的主要原因在于因为CNN容易过拟合，所以需要大量的训练数据，所以在CNN训练阶段我们是对Bounding box的位置限制条件限制的比较松(IOU只要大于0.5都被标注为正样本了)；然而svm训练的时候，因为svm适用于少样本训练，所以对于训练样本数据的IOU要求比较严格，我们只有当bounding box把整个物体都包含进去了，我们才把它标注为物体类别，然后训练svm，具体请看下文。</p>
<h3 id="SVM训练、测试阶段"><a href="#SVM训练、测试阶段" class="headerlink" title="SVM训练、测试阶段"></a><strong>SVM训练、测试阶段</strong></h3><p>训练阶段：</p>
<p>这是一个二分类问题，我么假设我们要检测车辆。我们知道只有当bounding box把整量车都包含在内，那才叫正样本；如果bounding box 没有包含到车辆，那么我们就可以把它当做负样本。但问题是当我们的检测窗口只有部分包含物体，那该怎么定义正负样本呢？作者测试了IOU阈值各种方案数值0,0.1,0.2,0.3,0.4,0.5。最后通过训练发现，如果选择IOU阈值为0.3效果最好（选择为0精度下降了4个百分点，选择0.5精度下降了5个百分点）,即当重叠度小于0.3的时候，我们就把它标注为负样本。一旦CNN f7层特征被提取出来，那么我们将为每个物体类训练一个svm分类器。当我们用CNN提取2000个候选框，可以得到2000**4096这样的特征向量矩阵，然后我们只需要把这样的一个矩阵与svm权值矩阵4096*N点乘(N为分类类别数目，因为我们训练的N个svm，每个svm包含了4096个权值w)，就可以得到结果了。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-3ef21dd028fd210f92107c1ded528045_720w.png" alt="img"></p>
<p>得到的特征输入到SVM进行分类看看这个feature vector所对应的region proposal是需要的物体还是无关的实物(background) 。</p>
<p> 排序，canny边界检测之后就得到了我们需要的bounding-box。</p>
<blockquote>
<p>再回顾总结一下：整个系统分为三个部分：1.产生不依赖与特定类别的region proposals，这些region proposals定义了一个整个检测器可以获得的候选目标2.一个大的卷积神经网络，对每个region产生一个固定长度的特征向量3.一系列特定类别的线性SVM分类器。</p>
</blockquote>
<p>位置精修： 目标检测问题的衡量标准是重叠面积：许多看似准确的检测结果，往往因为候选框不够准确，重叠面积很小。故需要一个位置精修步骤。 回归器：对每一类目标，使用一个线性脊回归器进行精修。正则项λ=10000。 输入为深度网络pool5层的4096维特征，输出为xy方向的缩放和平移。 训练样本：判定为本类的候选框中和真值重叠面积大于0.6的候选框。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-7e2c472157f6a4028db9f8ba3c0eb744_720w.png" alt="img"></p>
<p>测试阶段：</p>
<p>使用selective search的方法在测试图片上提取2000个region propasals ，将每个region proposals归一化到227x227，然后在CNN中正向传播，将最后一层得到的特征提取出来。然后对于每一个类别，使用为这一类训练的SVM分类器对提取的特征向量进行打分，得到测试图片中对于所有region proposals的对于这一类的分数，再使用贪心的非极大值抑制（NMS）去除相交的多余的框。再对这些框进行canny边缘检测，就可以得到bounding-box(then B-BoxRegression)。</p>
<p>（非极大值抑制（NMS）先计算出每一个bounding box的面积，然后根据score进行排序，把score最大的bounding box作为选定的框，计算其余bounding box与当前最大score与box的IoU，去除IoU大于设定的阈值的bounding box。然后重复上面的过程，直至候选bounding box为空，然后再将score小于一定阈值的选定框删除得到这一类的结果（然后继续进行下一个分类）。作者提到花费在region propasals和提取特征的时间是13s/张-GPU和53s/张-CPU，可以看出时间还是很长的，不能够达到及时性。</p>
<p>本文主要整理自以下文章：</p>
<ul>
<li><a href="https://link.zhihu.com/?target=http://blog.csdn.net/u011534057/article/details/51240387">RCNN学习笔记(0):rcnn简介</a></li>
<li><a href="https://link.zhihu.com/?target=http://blog.csdn.net/u011534057/article/details/51218218">RCNN学习笔记(1):Rich feature hierarchies for accurate object detection and semantic segmentation</a></li>
<li>《Rich feature hierarchies for Accurate Object Detection and Segmentation》</li>
<li>《Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition》</li>
</ul>
<h1 id="SPPnet"><a href="#SPPnet" class="headerlink" title="SPPnet"></a>SPPnet</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>沿着上一篇RCNN的思路，我们继续探索目标检测的痛点，其中RCNN使用CNN作为特征提取器，首次使得目标检测跨入深度学习的阶段。但是RCNN对于每一个区域候选都需要首先将图片放缩到固定的尺寸（224*224），然后为每个区域候选提取CNN特征。容易看出这里面存在的一些性能瓶颈：</p>
<ul>
<li>速度瓶颈：重复为每个region proposal提取特征是极其费时的，Selective Search对于每幅图片产生2K左右个region proposal，也就是意味着一幅图片需要经过2K次的完整的CNN计算得到最终的结果。</li>
<li>性能瓶颈：对于所有的region proposal防缩到固定的尺寸会导致我们不期望看到的几何形变，而且由于速度瓶颈的存在，不可能采用多尺度或者是大量的数据增强去训练模型。</li>
</ul>
<p>但是为什么CNN需要固定的输入呢？CNN网络可以分解为卷积网络部分以及全连接网络部分。我们知道卷积网络的参数主要是卷积核，完全能够适用任意大小的输入，并且能够产生任意大小的输出。但是全连接层部分不同，全连接层部分的参数是神经元对于所有输入的连接权重，也就是说输入尺寸不固定的话，全连接层参数的个数都不能固定。</p>
<p>何凯明团队的SPPNet给出的解决方案是，既然只有全连接层需要固定的输入，那么我们在全连接层前加入一个网络层，让他对任意的输入产生固定的输出不就好了吗？一种常见的想法是对于最后一层卷积层的输出pooling一下，但是这个pooling窗口的尺寸及步伐设置为相对值，也就是输出尺寸的一个比例值，这样对于任意输入经过这层后都能得到一个固定的输出。SPPnet在这个想法上继续加入SPM的思路，SPM其实在传统的机器学习特征提取中很常用，主要思路就是对于一副图像分成若干尺度的一些块，比如一幅图像分成1份，4份，8份等。然后对于每一块提取特征然后融合在一起，这样就可以兼容多个尺度的特征啦。SPPNet首次将这种思想应用在CNN中，对于卷积层特征我们也先给他分成不同的尺寸，然后每个尺寸提取一个固定维度的特征，最后拼接这些特征不就是一个固定维度的输入了吗？</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-da59abcbe56803aeeef24ffb5131ce83_720w.png" alt="img"></p>
<p>上面这个图可以看出SPPnet和RCNN的区别，首先是输入不需要放缩到指定大小。其次是增加了一个空间金字塔池化层，还有最重要的一点是每幅图片只需要提取一次特征。</p>
<p>通过上述方法虽然解决了CNN输入任意大小图片的问题，但是还是需要重复为每个region proposal提取特征啊，能不能我们直接根据region proposal定位到他在卷积层特征的位置，然后直接对于这部分特征处理呢？答案是肯定的，我们将在下一章节介绍。</p>
<h2 id="网络细节"><a href="#网络细节" class="headerlink" title="网络细节"></a>网络细节</h2><ul>
<li><strong>卷积层特征图</strong></li>
</ul>
<p><img src="E:\笔记\markdown\reference\picture\v2-523707e94ccb850ca4c23cc94054a144_720w.png" alt="img"></p>
<p>SPPNet通过可视化Conv5层特征，发现卷积特征其实保存了空间位置信息（数学推理中更容易发现这点），并且每一个卷积核负责提取不同的特征，比如C图175、55卷积核的特征，其中175负责提取窗口特征，55负责提取圆形的类似于车轮的特征。我们可以通过传统的方法聚集这些特征，例如词袋模型或是空间金字塔的方法。</p>
<ul>
<li><strong>空间金字塔池化层</strong></li>
</ul>
<p><img src="E:\笔记\markdown\reference\picture\v2-62c008799df798656236258c64082340_720w.png" alt="img"></p>
<p>上图的空间金字塔池化层是SPPNet的核心，其主要目的是对于任意尺寸的输入产生固定大小的输出。思路是对于任意大小的feature map首先分成16、4、1个块，然后在每个块上最大池化，池化后的特征拼接得到一个固定维度的输出。以满足全连接层的需要。不过因为不是针对于目标检测的，所以输入的图像为一整副图像。</p>
<ul>
<li><strong>SPPNet应用于图像分类</strong></li>
</ul>
<p>SPPNet的能够接受任意尺寸图片的输入，但是训练难点在于所有的深度学习框架都需要固定大小的输入，因此SPPNet做出了多阶段多尺寸训练方法。在每一个epoch的时候，我们先将图像放缩到一个size，然后训练网络。训练完整后保存网络的参数，然后resize 到另外一个尺寸，并在之前权值的基础上再次训练模型。相比于其他的CNN网络，SPPNet的优点是可以方便地进行多尺寸训练，而且对于同一个尺度，其特征也是个空间金字塔的特征，综合了多个特征的空间多尺度信息。</p>
<ul>
<li><strong>SPPNet应用于目标检测</strong></li>
</ul>
<p><img src="E:\笔记\markdown\reference\picture\v2-d68eaa673b48c3176eb48b3cb16a761f_720w.png" alt="img"></p>
<p>SPPNet理论上可以改进任何CNN网络，通过空间金字塔池化，使得CNN的特征不再是单一尺度的。但是SPPNet更适用于处理目标检测问题，首先是网络可以介绍任意大小的输入，也就是说能够很方便地多尺寸训练。其次是空间金字塔池化能够对于任意大小的输入产生固定的输出，这样使得一幅图片的多个region proposal提取一次特征成为可能。SPPNet的做法是：</p>
<ol>
<li>首先通过selective search产生一系列的region proposal，参见：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27467369">目标检测（1）-Selective Search - 知乎专栏</a></li>
<li>然后训练多尺寸识别网络用以提取区域特征，其中处理方法是每个尺寸的最短边大小在尺寸集合中：</li>
</ol>
<p><img src="E:\笔记\markdown\reference\picture\v2-237603b04a4f5f801924219f4fdfad99_720w.png" alt="img"></p>
<p>训练的时候通过上面提到的多尺寸训练方法，也就是在每个epoch中首先训练一个尺寸产生一个model，然后加载这个model并训练第二个尺寸，直到训练完所有的尺寸。空间金字塔池化使用的尺度为：1<em>1，2</em>2，3<em>3，6</em>6，一共是50个bins。</p>
<p>3.在测试时，每个region proposal选择能使其包含的像素个数最接近224*224的尺寸，提取相 应特征。</p>
<p>由于我们的空间金字塔池化可以接受任意大小的输入，因此对于每个region proposal将其映射到feature map上，然后仅对这一块feature map进行空间金字塔池化就可以得到固定维度的特征用以训练CNN了。关于从region proposal映射到feature map的细节我们待会儿去说。</p>
<p>4.训练SVM，BoundingBox回归</p>
<p>这部分和RCNN完全一致，参见：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27473413">目标检测（2）-RCNN - 知乎专栏</a></p>
<ul>
<li><strong>实验结果</strong></li>
</ul>
<p>其中单一尺寸训练结果低于RCNN1.2%，但是速度是其102倍，5个尺寸的训练结果与RCNN相当，其速度为RCNN的38倍。</p>
<ul>
<li><strong>如何从一个region proposal 映射到feature map的位置？</strong></li>
</ul>
<p>SPPNet通过角点尽量将图像像素映射到feature map感受野的中央，假设每一层的padding都是p/2，p为卷积核大小。对于feature map的一个像素（x’,y’），其实际感受野为：（Sx‘，Sy’），其中S为之前所有层步伐的乘积。然后对于region proposal的位置，我们获取左上右下两个点对应的feature map的位置，然后取特征就好了。左上角映射为：</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-8c5eddc9f856822aad5ae8d030ce1779_720w.png" alt="img"></p>
<p>右下角映射为：</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-7a4ce0c60b8fcac5eb7ffe365f99572e_720w.png" alt="img"></p>
<p>当然，如果padding大小不一致，那么就需要计算相应的偏移值啦。</p>
<h2 id="存在的不足"><a href="#存在的不足" class="headerlink" title="存在的不足"></a>存在的不足</h2><p>和RCNN一样，SPP也需要训练CNN提取特征，然后训练SVM分类这些特征。需要巨大的存储空间，并且分开训练也很复杂。而且selective search的方法提取特征是在CPU上进行的，相对于GPU来说还是比较慢的。针对这些问题的改进，我们将在Fast RCNN以及Faster RCNN中介绍，敬请期待。</p>
<h1 id="Fast-RCNN"><a href="#Fast-RCNN" class="headerlink" title="Fast RCNN"></a>Fast RCNN</h1><p>Fast-RCNN是基于RCNN和SPPnet的进一步改进。Fast-RCNN针对RCNN的以下三点缺点做了改进：</p>
<ol>
<li>RCNN是多阶段的模型，使用selective search算法筛选候选区域-&gt;<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:%22338211515%22%7D">卷积神经网络</a>提取特征值-&gt;SVM判断类型，非常繁琐。</li>
<li>RCNN训练非常消耗时间和内存。</li>
<li>RCNN的识别速度很慢。</li>
</ol>
<h2 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h2><p><img src="E:\笔记\markdown\reference\picture\v2-ced13fd5cc13bcccfcaf3afa20dce95e_b.jpg" alt="img">Fast-RCNN整体结构图</p>
<p><strong>从上图可以看到，相比起RCNN，Fast-RCNN使用全连接层替代了SVM来识别物体，并且Fast-RCNN摒弃了以前每一个候选区域分别放入卷积神经网络进行特征提取的方法，将整个图片直接放入卷积神经网络提取特征，避免了重复计算，提高了检测的速度。</strong></p>
<p>上面这个图片可能有点抽象，下面这个图片摘自<a href="https://link.zhihu.com/?target=https://blog.csdn.net/shenxiaolu1984/article/details/51036677">博客</a>，更加清晰的展示了网络的细节：</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-d6b522d5eb9ff43c3bcd66c8448844d1_b.jpg" alt="img"></p>
<p>​                                                                                                                Fast-RCNN网络结构图</p>
<ul>
<li>一张（224,224,3）的图片进入网络（VGG16）进行<strong>提取特征。</strong></li>
<li>进入ROI Pooling层</li>
<li>再经过两个output都为4096维的全连接层</li>
<li>分别经过output各为21和84维的全连接层（并列的，前者是分类输出，有21个种类；后者是回归输出，输出边框的位置[x,y,w,h], 21*4=84）</li>
</ul>
<p>下面这张来自<a href="https://link.zhihu.com/?target=https://www.robots.ox.ac.uk/~tvg/publications/talks/fast-rcnn-slides.pdf">Fast-RCNN的PPT</a>图可能更加清晰一些：</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-be21dff7c814eb9d470d35805f965ebb_b.jpg" alt="img">Fast-RCNN结构图</p>
<p>从上图可以看到，卷积不再是对每个region proposal（候选区域）进行，而是直接对整张图像，这样减少了很多重复计算。原来RCNN是对每个候选区域分别做卷积，因为一张图像中有2000左右的候选区域，肯定相互之间的重叠率很高，因此产生重复计算。</p>
<p><strong>每一个候选区域都是输入图片的一张子图，那么每一个候选区域都对应<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=%E7%89%B9%E5%BE%81%E5%9B%BE&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:%22338211515%22%7D">特征图</a>中的一个部分。</strong></p>
<h2 id="ROI"><a href="#ROI" class="headerlink" title="ROI"></a>ROI</h2><p>使用Selective Search算法对输入图片进行提取候选区域，候选区域的大小不一，所以对应特征图的区域也不一样，而全连接层的输入是固定的，这样就无法直接输入到全连接层中。如何将其变化成统一的格式呢？</p>
<p>ROI Pooling的作用是对不同大小的候选区域，从最后卷积层输出的特征图提取大小固定的特征图。</p>
<p>使用ROI Pooling层将每个候选区域均匀分成M×N块，对每块进行Max Pooling。将所有输出值组合起来便形成固定大小为H×W的特征图。这样就可以将特征图上大小不一的候选区域转变为大小统一的数据，送入下一层。图片来自<a href="https://link.zhihu.com/?target=https://blog.csdn.net/shenxiaolu1984/article/details/51036677">博客</a>。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-95e27f746e16ee25da88871c3114ff76_b.jpg" alt="img">ROI Pooling示意图</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><strong>总损失</strong>如下：</p>
<img src='https://www.zhihu.com/equation?tex=L(p, u, t^u , v) = Lcls(p, u) + λ[u ≥ 1]Lloc(t^u, v)'>

<p>损失函数分为两个部分：<img src='https://www.zhihu.com/equation?tex=Lcls'> 表示分类损失，<img src='https://www.zhihu.com/equation?tex=Lloc'> 表示定位损失(回归损失).</p>
<p>λ用于判断背景，如果分类不是背景，λ=1</p>
<p>分类如果是背景λ=0，则不考虑定位损失,即：</p>
<p>$\begin{equation} L=\left{ \begin{aligned} Lcls+λLloc&amp; &amp; {u不为背景}\ Lcls &amp; &amp; {u为背景}\ \end{aligned} \right. \end{equation}$</p>
<p><strong>分类损失：</strong></p>
<p>对于分类的全连接神经网络，它将产生由Softmax产生的概率p=(p0,p1…,pk)。这里K=20,所以共有21个概率值。</p>
<p><img src='https://www.zhihu.com/equation?tex= Lcls(p, u)=−log(p_u)'>，其中<img src='https://www.zhihu.com/equation?tex=p_u'> 是真实类型预测的那个概率。也就是说这里只计算一个 ，其他概率值不算。</p>
<p><strong>定位损失：</strong></p>
<p>对于bounding-box回归的全连接网络，它产生的是位置信息（x,y,w,h），分类神经网络每一个概率值，它都对应有一个位置信息。所以网络最后输出维度为：21*4=84。 </p>
<p><img src="E:\笔记\markdown\reference\picture\equation-165157845950121.svg+xml" alt="[公式]"> 表示第k个种类所对应的位置信息。</p>
<p><img src="E:\笔记\markdown\reference\picture\equation-165157845950122.svg+xml" alt="[公式]"> 表示真实的位置信息</p>
<p><img src="E:\笔记\markdown\reference\picture\in{x%2Cy%2Cw%2Ch}}{smooth_{L_i}(t_i^u-v_i)}.svg+xml" alt="[公式]"> </p>
<p>其中smooth函数:</p>
<p><img src="E:\笔记\markdown\reference\picture\end{equation}-165157845950123.svg+xml" alt="[公式]"> </p>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>看一下训练时候的整体结构图：</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-5fbec4583ca50103e33d421463d3a3a0_b.jpg" alt="img"></p>
<p>在调优训练时，每一个mini-batch中首先加入N张完整图片，而后加入从N张图片中选取的R个候选框。这R个候选框可以复用N张图片前5个阶段的网络特征。<strong>实际选择N=2， R=128</strong></p>
<p><strong>训练数据构成</strong></p>
<p>N张完整图片以50%概率水平翻转。</p>
<p>R个候选框的构成方式如下：</p>
<table>
<thead>
<tr>
<th>类别</th>
<th>比例</th>
<th>方式</th>
</tr>
</thead>
<tbody><tr>
<td>前景</td>
<td>25%</td>
<td>与某个真值重叠在[0.5,1]的候选框</td>
</tr>
<tr>
<td>背景</td>
<td>75%</td>
<td>与真值重叠的最大值在[0.1,0.5)的候选框</td>
</tr>
</tbody></table>
<h2 id="SVD加速"><a href="#SVD加速" class="headerlink" title="SVD加速"></a>SVD加速</h2><p>分类和位置调整都是通过全连接层实现的，假设全连接层参数为W，尺寸u × v 一次前向传播即为：Y=Wx,计算复次数为u × v</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-7b188e68f559382e0e1ddce4d4dfb837_b.jpg" alt="img">网络中各个部分计算所占用的时间</p>
<p>从上图可以看出，两个全连接层（fc6、fc7）占用了44.9%的时间，非常消耗时间。</p>
<p>为此，论文中对W进行SVD分解，并用前t个特征值近似：</p>
<p><img src="E:\笔记\markdown\reference\picture\equation-165157845950124.svg+xml" alt="[公式]"> </p>
<p>原来的前向传播分解成两步： <img src="E:\笔记\markdown\reference\picture\equation-165157845950125.svg+xml" alt="[公式]"> </p>
<p>计算复杂度变为u × t + v × t=(u + v) × t</p>
<p>在实现时，相当于把一个全连接层拆分成两个，中间以一个<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=%E4%BD%8E%E7%BB%B4%E6%95%B0%E6%8D%AE&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:%22338211515%22%7D">低维数据</a>相连。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-3d11b57e6904c18bc8bd89efc4a06bda_b.jpg" alt="img"></p>
<p>使用SVD加速后，各部分使用时间如下所示：</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-8781d554a612c8a2c6686f4ac0df7a22_b.jpg" alt="img"></p>
<p>​                                                                使用SVD加速后，网络中各个部分计算所占用的时间</p>
<p>可以看到加速确实有效果了。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="E:\笔记\markdown\reference\picture\v2-ff6a2918f64d1977fad99bb840facd7b_b.jpg" alt="img"></p>
<p><img src="E:\笔记\markdown\reference\picture\v2-f0af16bde4eb2271920696e00ed5d465_b.jpg" alt="img"></p>
<h2 id="参考连接："><a href="#参考连接：" class="headerlink" title="参考连接："></a>参考连接：</h2><p><a href="https://link.zhihu.com/?target=https://www.robots.ox.ac.uk/~tvg/publications/talks/fast-rcnn-slides.pdf">https://www.robots.ox.ac.uk/~tvg/publications/talks/fast-rcnn-slides.pdf</a></p>
<p><a href="https://link.zhihu.com/?target=https://blog.csdn.net/shenxiaolu1984/article/details/51036677">https://blog.csdn.net/s</a>henxiaolu1984/article/details/51036677</p>
<p><a href="https://link.zhihu.com/?target=https://blog.csdn.net/u014380165/article/details/72851319">Fast RCNN算法详解_AI之路-CSDN博客_fast rcnn</a></p>
<p><a href="https://link.zhihu.com/?target=https://www.cnblogs.com/CZiFan/p/9903518.html">Fast R-CNN（理解） - CZiFan - 博客园</a></p>
<p><a href="https://link.zhihu.com/?target=https://blog.csdn.net/u014380165/article/details/72851319">Fast RCNN算法详解_AI之路-CSDN博客_fast rcnn</a></p>
<h1 id="Faster-RCNN"><a href="#Faster-RCNN" class="headerlink" title="Faster RCNN"></a>Faster RCNN</h1><p>经过R-CNN和Fast RCNN的积淀，Ross B. Girshick在2016年提出了新的Faster RCNN，在结构上，Faster RCNN已经将特征抽取(feature extraction)，proposal提取，bounding box regression(rect refine)，classification都整合在了一个网络中，使得综合性能有较大提高，在检测速度方面尤为明显。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-c0172be282021a1029f7b72b51079ffe_720w.jpg" alt="img">图1 Faster RCNN基本结构（来自原论文）</p>
<p>依作者看来，如图1，Faster RCNN其实可以分为4个主要内容：</p>
<ol>
<li>Conv layers。作为一种CNN网络目标检测方法，Faster RCNN首先使用一组基础的conv+relu+pooling层提取image的feature maps。该feature maps被共享用于后续RPN层和全连接层。</li>
<li>Region Proposal Networks。RPN网络用于生成region proposals。该层通过softmax判断anchors属于positive或者negative，再利用bounding box regression修正anchors获得精确的proposals。</li>
<li>Roi Pooling。该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。</li>
<li>Classification。利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。</li>
</ol>
<p>所以本文以上述4个内容作为切入点介绍Faster R-CNN网络。</p>
<p>图2展示了python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构，可以清晰的看到该网络对于一副任意大小PxQ的图像：</p>
<ul>
<li>首先缩放至固定大小MxN，然后将MxN图像送入网络；</li>
<li>而Conv layers中包含了13个conv层+13个relu层+4个pooling层；</li>
<li>RPN网络首先经过3x3卷积，再分别生成positive anchors和对应bounding box regression偏移量，然后计算出proposals；</li>
<li>而Roi Pooling层则利用proposals从feature maps中提取proposal feature送入后续全连接和softmax网络作classification（即分类proposal到底是什么object）。</li>
</ul>
<p><img src="E:\笔记\markdown\reference\picture\v2-e64a99b38f411c337f538eb5f093bdf3_720w.jpg" alt="img">图2 faster_rcnn_test.pt网络结构 （pascal_voc/VGG16/faster_rcnn_alt_opt/faster_rcnn_test.pt）</p>
<p><em>本文不会讨论任何关于R-CNN家族的历史，分析清楚最新的Faster R-CNN就够了，并不需要追溯到那么久。实话说我也不了解R-CNN，更不关心。有空不如看看新算法。</em></p>
<p>新出炉的pytorch官方Faster RCNN代码导读：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/145842317">捋一捋pytorch官方FasterRCNN代码1308 赞同 · 21 评论文章<img src="E:\笔记\markdown\reference\picture\v2-dc78d0ad1012ad65040bdb7eb657f381_180x120.jpg" alt="img"></a></p>
<h2 id="1-Conv-layers"><a href="#1-Conv-layers" class="headerlink" title="1 Conv layers"></a>1 Conv layers</h2><p>Conv layers包含了conv，pooling，relu三种层。以python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构为例，如图2，Conv layers部分共有13个conv层，13个relu层，4个pooling层。这里有一个非常容易被忽略但是又无比重要的信息，在Conv layers中：</p>
<ol>
<li>所有的conv层都是：kernel_size=3，pad=1，stride=1</li>
<li>所有的pooling层都是：kernel_size=2，pad=0，stride=2</li>
</ol>
<p>为何重要？在Faster RCNN Conv layers中对所有的卷积都做了扩边处理（ pad=1，即填充一圈0），导致原图变为 (M+2)x(N+2)大小，再做3x3卷积后输出MxN 。正是这种设置，导致Conv layers中的conv层不改变输入和输出矩阵大小。如图3：</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-3c772e9ed555eb86a97ef9c08bf563c9_720w.jpg" alt="img">图3 卷积示意图</p>
<p>类似的是，Conv layers中的pooling层kernel_size=2，stride=2。这样每个经过pooling层的MxN矩阵，都会变为(M/2)x(N/2)大小。综上所述，在整个Conv layers中，conv和relu层不改变输入输出大小，只有pooling层使输出长宽都变为输入的1/2。</p>
<p>那么，一个MxN大小的矩阵经过Conv layers固定变为(M/16)x(N/16)！这样Conv layers生成的feature map中都可以和原图对应起来。</p>
<h2 id="2-Region-Proposal-Networks-RPN"><a href="#2-Region-Proposal-Networks-RPN" class="headerlink" title="2 Region Proposal Networks(RPN)"></a>2 Region Proposal Networks(RPN)</h2><p>经典的检测方法生成检测框都非常耗时，如OpenCV adaboost使用滑动窗口+图像金字塔生成检测框；或如R-CNN使用SS(Selective Search)方法生成检测框。而Faster RCNN则抛弃了传统的滑动窗口和SS方法，直接使用RPN生成检测框，这也是Faster R-CNN的巨大优势，能极大提升检测框的生成速度。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-1908feeaba591d28bee3c4a754cca282_720w.jpg" alt="img">图4 RPN网络结构</p>
<p>上图4展示了RPN网络的具体结构。可以看到RPN网络实际分为2条线，上面一条通过softmax分类anchors获得positive和negative分类，下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。而最后的Proposal层则负责综合positive anchors和对应bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能。</p>
<h3 id="2-1-多通道图像卷积基础知识介绍"><a href="#2-1-多通道图像卷积基础知识介绍" class="headerlink" title="2.1 多通道图像卷积基础知识介绍"></a>2.1 多通道图像卷积基础知识介绍</h3><p>在介绍RPN前，还要多解释几句基础知识，已经懂的看官老爷跳过就好。</p>
<ol>
<li>对于单通道图像+单卷积核做卷积，第一章中的图3已经展示了；</li>
<li>对于多通道图像+多卷积核做卷积，计算方式如下：</li>
</ol>
<p><img src="E:\笔记\markdown\reference\picture\v2-8d72777321cbf1336b79d839b6c7f9fc_720w.jpg" alt="img">图5 多通道卷积计算方式</p>
<p>如图5，输入有3个通道，同时有2个卷积核。对于每个卷积核，先在输入3个通道分别作卷积，再将3个通道结果加起来得到卷积输出。所以对于某个卷积层，无论输入图像有多少个通道，输出图像通道数总是等于卷积核数量！</p>
<p>对多通道图像做1x1卷积，其实就是将输入图像于每个通道乘以卷积系数后加在一起，即相当于把原图像中本来各个独立的通道“联通”在了一起。</p>
<h3 id="2-2-anchors"><a href="#2-2-anchors" class="headerlink" title="2.2 anchors"></a>2.2 anchors</h3><p>提到RPN网络，就不能不说anchors。所谓anchors，实际上就是一组由rpn/generate_anchors.py生成的矩形。直接运行作者demo中的generate_anchors.py可以得到以下输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[[ -84.  -40.   99.   55.]</span><br><span class="line"> [-176.  -88.  191.  103.]</span><br><span class="line"> [-360. -184.  375.  199.]</span><br><span class="line"> [ -56.  -56.   71.   71.]</span><br><span class="line"> [-120. -120.  135.  135.]</span><br><span class="line"> [-248. -248.  263.  263.]</span><br><span class="line"> [ -36.  -80.   51.   95.]</span><br><span class="line"> [ -80. -168.   95.  183.]</span><br><span class="line"> [-168. -344.  183.  359.]]</span><br></pre></td></tr></table></figure>

<p>其中每行的4个值 <img src="E:\笔记\markdown\reference\picture\equation.svg+xml" alt="[公式]"> 表矩形左上和右下角点坐标。9个矩形共有3种形状，长宽比为大约为 <img src="E:\笔记\markdown\reference\picture}.svg+xml" alt="[公式]"> 三种，如图6。实际上通过anchors就引入了检测中常用到的多尺度方法。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-7abead97efcc46a3ee5b030a2151643f_720w.jpg" alt="img">图6 anchors示意图</p>
<p>注：关于上面的anchors size，其实是根据检测图像设置的。在python demo中，会把任意大小的输入图像reshape成800x600（即图2中的M=800，N=600）。再回头来看anchors的大小，anchors中长宽1:2中最大为352x704，长宽2:1中最大736x384，基本是cover了800x600的各个尺度和形状。</p>
<p>那么这9个anchors是做什么的呢？借用Faster RCNN论文中的原图，如图7，遍历Conv layers计算获得的feature maps，为每一个点都配备这9种anchors作为初始的检测框。这样做获得检测框很不准确，不用担心，后面还有2次bounding box regression可以修正检测框位置。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-c93db71cc8f4f4fd8cfb4ef2e2cef4f4_720w.jpg" alt="img">图7</p>
<p>解释一下上面这张图的数字。</p>
<ol>
<li>在原文中使用的是ZF model中，其Conv Layers中最后的conv5层num_output=256，对应生成256张特征图，所以相当于feature map每个点都是256-dimensions</li>
<li>在conv5之后，做了rpn_conv/3x3卷积且num_output=256，相当于每个点又融合了周围3x3的空间信息（猜测这样做也许更鲁棒？反正我没测试），同时256-d不变（如图4和图7中的红框）</li>
<li>假设在conv5 feature map中每个点上有k个anchor（默认k=9），而每个anhcor要分positive和negative，所以每个点由256d feature转化为cls=2•k scores；而每个anchor都有(x, y, w, h)对应4个偏移量，所以reg=4•k coordinates</li>
<li>补充一点，全部anchors拿去训练太多了，训练程序会在合适的anchors中<strong>随机</strong>选取128个postive anchors+128个negative anchors进行训练（什么是合适的anchors下文5.1有解释）</li>
</ol>
<p>注意，在本文讲解中使用的VGG conv5 num_output=512，所以是512d，其他类似。</p>
<p><strong>其实RPN最终就是在原图尺度上，设置了密密麻麻的候选Anchor。然后用cnn去判断哪些Anchor是里面有目标的positive anchor，哪些是没目标的negative anchor。所以，仅仅是个二分类而已！</strong></p>
<p>那么Anchor一共有多少个？原图800x600，VGG下采样16倍，feature map每个点设置9个Anchor，所以：</p>
<p><img src="E:\笔记\markdown\reference\picture\tag{1}.svg+xml" alt="[公式]"></p>
<p>其中ceil()表示向上取整，是因为VGG输出的feature map size= 50*38。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-4b15828dfee19be726835b671748cc4d_720w.jpg" alt="img">图8 Gernerate Anchors</p>
<h3 id="2-3-softmax判定positive与negative"><a href="#2-3-softmax判定positive与negative" class="headerlink" title="2.3 softmax判定positive与negative"></a>2.3 softmax判定positive与negative</h3><p>一副MxN大小的矩阵送入Faster RCNN网络后，到RPN网络变为(M/16)x(N/16)，不妨设 W=M/16，H=N/16。在进入reshape与softmax之前，先做了1x1卷积，如图9：</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-1ab4b6c3dd607a5035b5203c76b078f3_720w.jpg" alt="img">图9 RPN中判定positive/negative网络结构</p>
<p>该1x1卷积的caffe prototxt定义如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;rpn_cls_score&quot;</span><br><span class="line">  type: &quot;Convolution&quot;</span><br><span class="line">  bottom: &quot;rpn/output&quot;</span><br><span class="line">  top: &quot;rpn_cls_score&quot;</span><br><span class="line">  convolution_param &#123;</span><br><span class="line">    num_output: 18   # 2(positive/negative) * 9(anchors)</span><br><span class="line">    kernel_size: 1 pad: 0 stride: 1</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到其num_output=18，也就是经过该卷积的输出图像为WxHx18大小（注意第二章开头提到的卷积计算方式）。这也就刚好对应了feature maps每一个点都有9个anchors，同时每个anchors又有可能是positive和negative，所有这些信息都保存WxHx(9*2)大小的矩阵。为何这样做？后面接softmax分类获得positive anchors，也就相当于初步提取了检测目标候选区域box（一般认为目标在positive anchors中）。</p>
<p>那么为何要在softmax前后都接一个reshape layer？其实只是为了便于softmax分类，至于具体原因这就要从caffe的实现形式说起了。在caffe基本数据结构blob中以如下形式保存数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">blob=[batch_size, channel，height，width]</span><br></pre></td></tr></table></figure>

<p>对应至上面的保存positive/negative anchors的矩阵，其在caffe blob中的存储形式为[1, 2x9, H, W]。而在softmax分类时需要进行positive/negative二分类，所以reshape layer会将其变为[1, 2, 9xH, W]大小，即单独“腾空”出来一个维度以便softmax分类，之后再reshape回复原状。贴一段caffe softmax_loss_layer.cpp的reshape函数的解释，非常精辟：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;Number of labels must match number of predictions; &quot;</span></span><br><span class="line"><span class="string">&quot;e.g., if softmax axis == 1 and prediction shape is (N, C, H, W), &quot;</span></span><br><span class="line"><span class="string">&quot;label count (number of labels) must be N*H*W, &quot;</span></span><br><span class="line"><span class="string">&quot;with integer values in &#123;0, 1, ..., C-1&#125;.&quot;</span>;</span><br></pre></td></tr></table></figure>

<p>综上所述，RPN网络中利用anchors和softmax初步提取出positive anchors作为候选区域（另外也有实现用sigmoid代替softmax，输出[1, 1, 9xH, W]后接sigmoid进行positive/negative二分类，原理一样）。</p>
<h3 id="2-4-bounding-box-regression原理"><a href="#2-4-bounding-box-regression原理" class="headerlink" title="2.4 bounding box regression原理"></a>2.4 bounding box regression原理</h3><p>如图9所示绿色框为飞机的Ground Truth(GT)，红色为提取的positive anchors，即便红色的框被分类器识别为飞机，但是由于红色的框定位不准，这张图相当于没有正确的检测出飞机。所以我们希望采用一种方法对红色的框进行微调，使得positive anchors和GT更加接近。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-93021a3c03d66456150efa1da95416d3_720w.jpg" alt="img">图10</p>
<p>对于窗口一般使用四维向量 <img src="E:\笔记\markdown\reference\picture\equation-16504664534921.svg+xml" alt="[公式]"> 表示，分别表示窗口的中心点坐标和宽高。对于图 11，红色的框A代表原始的positive Anchors，绿色的框G代表目标的GT，我们的目标是寻找一种关系，使得输入原始的anchor A经过映射得到一个跟真实窗口G更接近的回归窗口G’，即：</p>
<ul>
<li>给定anchor <img src="E:\笔记\markdown\reference\picture\equation-16504664534922.svg+xml" alt="[公式]"> 和 <img src="E:\笔记\markdown\reference\picture\equation-16504664534933.svg+xml" alt="[公式]"></li>
<li>寻找一种变换<strong>F，</strong>使得：<img src="E:\笔记\markdown\reference\picture\equation-16504664534934.svg+xml" alt="[公式]">，其中<img src="E:\笔记\markdown\reference\picture\equation-16504664534935.svg+xml" alt="[公式]"></li>
</ul>
<p><img src="E:\笔记\markdown\reference\picture\v2-ea7e6e48662bfa68ec73bdf32f36bb85_720w.jpg" alt="img">图11</p>
<p>那么经过何种变换<strong>F</strong>才能从图10中的anchor A变为G’呢？ 比较简单的思路就是:</p>
<ul>
<li>先做平移</li>
</ul>
<p><img src="E:\笔记\markdown\reference\picture\tag{2}.svg+xml" alt="[公式]"></p>
<p><img src="E:\笔记\markdown\reference\picture\tag{3}.svg+xml" alt="[公式]"></p>
<ul>
<li>再做缩放</li>
</ul>
<p><img src="E:\笔记\markdown\reference\picture\tag{4}.svg+xml" alt="[公式]"></p>
<p><img src="E:\笔记\markdown\reference\picture\tag{5}.svg+xml" alt="[公式]"></p>
<p>观察上面4个公式发现，需要学习的是 <img src="E:\笔记\markdown\reference\picture\equation-16504664534936.svg+xml" alt="[公式]"> 这四个变换。当输入的anchor A与GT相差较小时，可以认为这种变换是一种线性变换， 那么就可以用线性回归来建模对窗口进行微调（注意，只有当anchors A和GT比较接近时，才能使用线性回归模型，否则就是复杂的非线性问题了）。</p>
<p>接下来的问题就是如何通过线性回归获得 <img src="E:\笔记\markdown\reference\picture\equation-16504664534936.svg+xml" alt="[公式]"> 了。线性回归就是给定输入的特征向量X, 学习一组参数W, 使得经过线性回归后的值跟真实值Y非常接近，即<img src="E:\笔记\markdown\reference\picture\equation-16504664534937.svg+xml" alt="[公式]">。对于该问题，输入X是cnn feature map，定义为Φ；同时还有训练传入A与GT之间的变换量，即<img src="E:\笔记\markdown\reference\picture\equation-16504664534938.svg+xml" alt="[公式]">。输出是<img src="E:\笔记\markdown\reference\picture\equation.svg+xml" alt="[公式]">四个变换。那么目标函数可以表示为：</p>
<p><img src="E:\笔记\markdown\reference\picture\tag{6}.svg+xml" alt="[公式]"></p>
<p>其中 <img src="E:\笔记\markdown\reference\picture\phi(A).svg+xml" alt="[公式]"> 是对应anchor的feature map组成的特征向量， <img src="E:\笔记\markdown\reference\picture\equation-16504664534949.svg+xml" alt="[公式]"> 是需要学习的参数， <img src="E:\笔记\markdown\reference\picture\equation-165046645349410.svg+xml" alt="[公式]"> 是得到的预测值（*表示 x，y，w，h，也就是每一个变换对应一个上述目标函数）。为了让预测值 <img src="E:\笔记\markdown\reference\picture\equation-16504669589211.svg+xml" alt="[公式]"> 与真实值 <img src="E:\笔记\markdown\reference\picture\equation-165046645349411.svg+xml" alt="[公式]"> 差距最小，设计L1损失函数：</p>
<p><img src="E:\笔记\markdown\reference\picture\tag{7}.svg+xml" alt="[公式]"></p>
<p>函数优化目标为：</p>
<p><img src="E:\笔记\markdown\reference\picture\tag{8}.svg+xml" alt="[公式]"></p>
<p>为了方便描述，这里以L1损失为例介绍，而真实情况中一般使用soomth-L1损失。</p>
<p>需要说明，只有在GT与需要回归框位置比较接近时，才可近似认为上述线性变换成立。<br>说完原理，对应于Faster RCNN原文，positive anchor与ground truth之间的平移量 <img src="E:\笔记\markdown\reference\picture\equation-165046645349412.svg+xml" alt="[公式]"> 与尺度因子 <img src="E:\笔记\markdown\reference\picture\equation-165046645349413.svg+xml" alt="[公式]"> 如下：</p>
<p><img src="E:\笔记\markdown\reference\picture\tag{9}.svg+xml" alt="[公式]"></p>
<p><img src="E:\笔记\markdown\reference\picture\tag{10}.svg+xml" alt="[公式]"></p>
<p>对于训练bouding box regression网络回归分支，输入是cnn feature Φ，监督信号是Anchor与GT的差距 <img src="E:\笔记\markdown\reference\picture\equation-165046645349414.svg+xml" alt="[公式]">，即训练目标是：输入 Φ的情况下使网络输出与监督信号尽可能接近。那么当bouding box regression工作时，再输入Φ时，回归网络分支的输出就是每个Anchor的平移量和变换尺度 <img src="E:\笔记\markdown\reference\picture\equation-16504669589212.svg+xml" alt="[公式]">，显然即可用来修正Anchor位置了。</p>
<h3 id="2-5-对proposals进行bounding-box-regression"><a href="#2-5-对proposals进行bounding-box-regression" class="headerlink" title="2.5 对proposals进行bounding box regression"></a>2.5 对proposals进行bounding box regression</h3><p>在了解bounding box regression后，再回头来看RPN网络第二条线路，如图12。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-8241c8076d60156248916fe2f1a5674a_720w.jpg" alt="img">图12 RPN中的bbox reg</p>
<p>先来看一看上图11中1x1卷积的caffe prototxt定义：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;rpn_bbox_pred&quot;</span><br><span class="line">  type: &quot;Convolution&quot;</span><br><span class="line">  bottom: &quot;rpn/output&quot;</span><br><span class="line">  top: &quot;rpn_bbox_pred&quot;</span><br><span class="line">  convolution_param &#123;</span><br><span class="line">    num_output: 36   # 4 * 9(anchors)</span><br><span class="line">    kernel_size: 1 pad: 0 stride: 1</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到其 num_output=36，即经过该卷积输出图像为WxHx36，在caffe blob存储为[1, 4x9, H, W]，这里相当于feature maps每个点都有9个anchors，每个anchors又都有4个用于回归的</p>
<p><img src="E:\笔记\markdown\reference\picture\tag{11}.svg+xml" alt="[公式]"></p>
<p>变换量。</p>
<p>回到图8，VGG输出 <img src="E:\笔记\markdown\reference\picture\equation-165046645349415.svg+xml" alt="[公式]"> 的特征，对应设置 <img src="E:\笔记\markdown\reference\picture\equation-165046645349416.svg+xml" alt="[公式]"> 个anchors，而RPN输出：</p>
<ol>
<li>大小为<img src="E:\笔记\markdown\reference\picture\equation-165046645349417.svg+xml" alt="[公式]"> 的positive/negative softmax分类特征矩阵</li>
<li>大小为 <img src="E:\笔记\markdown\reference\picture\equation-165046645349418.svg+xml" alt="[公式]"> 的regression坐标回归特征矩阵</li>
</ol>
<p>恰好满足RPN完成positive/negative分类+bounding box regression坐标回归.</p>
<h3 id="2-6-Proposal-Layer"><a href="#2-6-Proposal-Layer" class="headerlink" title="2.6 Proposal Layer"></a>2.6 Proposal Layer</h3><p>Proposal Layer负责综合所有 <img src="E:\笔记\markdown\reference\picture\equation-165046645349519.svg+xml" alt="[公式]"> 变换量和positive anchors，计算出精准的proposal，送入后续RoI Pooling Layer。还是先来看看Proposal Layer的caffe prototxt定义：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &#x27;proposal&#x27;</span><br><span class="line">  type: &#x27;Python&#x27;</span><br><span class="line">  bottom: &#x27;rpn_cls_prob_reshape&#x27;</span><br><span class="line">  bottom: &#x27;rpn_bbox_pred&#x27;</span><br><span class="line">  bottom: &#x27;im_info&#x27;</span><br><span class="line">  top: &#x27;rois&#x27;</span><br><span class="line">  python_param &#123;</span><br><span class="line">    module: &#x27;rpn.proposal_layer&#x27;</span><br><span class="line">    layer: &#x27;ProposalLayer&#x27;</span><br><span class="line">    param_str: &quot;&#x27;feat_stride&#x27;: 16&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Proposal Layer有3个输入：positive vs negative anchors分类器结果rpn_cls_prob_reshape，对应的bbox reg的 <img src="E:\笔记\markdown\reference\picture\equation-165046645349519.svg+xml" alt="[公式]"> 变换量rpn_bbox_pred，以及im_info；另外还有参数feat_stride=16，这和图4是对应的。</p>
<p>首先解释im_info。对于一副任意大小PxQ图像，传入Faster RCNN前首先reshape到固定MxN，im_info=[M, N, scale_factor]则保存了此次缩放的所有信息。然后经过Conv Layers，经过4次pooling变为WxH=(M/16)x(N/16)大小，其中feature_stride=16则保存了该信息，用于计算anchor偏移量。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-1e43500c7cc9a9de211d737bc347ced9_720w.jpg" alt="img">图13</p>
<p>Proposal Layer forward（caffe layer的前传函数）按照以下顺序依次处理：</p>
<ol>
<li>生成anchors，利用<img src="E:\笔记\markdown\reference\picture\equation-165046645349519.svg+xml" alt="[公式]">对所有的anchors做bbox regression回归（这里的anchors生成和训练时完全一致）</li>
<li>按照输入的positive softmax scores由大到小排序anchors，提取前pre_nms_topN(e.g. 6000)个anchors，即提取修正位置后的positive anchors</li>
<li>限定超出图像边界的positive anchors为图像边界，防止后续roi pooling时proposal超出图像边界（见文章底部QA部分图21）</li>
<li>剔除尺寸非常小的positive anchors</li>
<li>对剩余的positive anchors进行NMS（nonmaximum suppression）</li>
<li>Proposal Layer有3个输入：positive和negative anchors分类器结果rpn_cls_prob_reshape，对应的bbox reg的(e.g. 300)结果作为proposal输出</li>
</ol>
<p>之后输出proposal=[x1, y1, x2, y2]，注意，由于在第三步中将anchors映射回原图判断是否超出边界，所以这里输出的proposal是对应MxN输入图像尺度的，这点在后续网络中有用。另外我认为，严格意义上的检测应该到此就结束了，后续部分应该属于识别了。</p>
<p>RPN网络结构就介绍到这里，总结起来就是：<br><strong>生成anchors -&gt; softmax分类器提取positvie anchors -&gt; bbox reg回归positive anchors -&gt; Proposal Layer生成proposals</strong></p>
<h2 id="3-RoI-pooling"><a href="#3-RoI-pooling" class="headerlink" title="3 RoI pooling"></a>3 RoI pooling</h2><p>而RoI Pooling层则负责收集proposal，并计算出proposal feature maps，送入后续网络。从图2中可以看到Rol pooling层有2个输入：</p>
<ol>
<li>原始的feature maps</li>
<li>RPN输出的proposal boxes（大小各不相同）</li>
</ol>
<h3 id="3-1-为何需要RoI-Pooling"><a href="#3-1-为何需要RoI-Pooling" class="headerlink" title="3.1 为何需要RoI Pooling"></a>3.1 为何需要RoI Pooling</h3><p>先来看一个问题：对于传统的CNN（如AlexNet和VGG），当网络训练好后输入的图像尺寸必须是固定值，同时网络输出也是固定大小的vector or matrix。如果输入图像大小不定，这个问题就变得比较麻烦。有2种解决办法：</p>
<ol>
<li>从图像中crop一部分传入网络</li>
<li>将图像warp成需要的大小后传入网络</li>
</ol>
<p><img src="E:\笔记\markdown\reference\picture\v2-e525342cbde476a11c48a6be393f226c_720w.jpg" alt="img">图14 crop与warp破坏图像原有结构信息</p>
<p>两种办法的示意图如图14，可以看到无论采取那种办法都不好，要么crop后破坏了图像的完整结构，要么warp破坏了图像原始形状信息。</p>
<p>回忆RPN网络生成的proposals的方法：对positive anchors进行bounding box regression，那么这样获得的proposals也是大小形状各不相同，即也存在上述问题。所以Faster R-CNN中提出了RoI Pooling解决这个问题。不过RoI Pooling确实是从<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/1406.4729">Spatial Pyramid Pooling</a>发展而来，但是限于篇幅这里略去不讲，有兴趣的读者可以自行查阅相关论文。</p>
<h3 id="3-2-RoI-Pooling原理"><a href="#3-2-RoI-Pooling原理" class="headerlink" title="3.2 RoI Pooling原理"></a>3.2 RoI Pooling原理</h3><p>分析之前先来看看RoI Pooling Layer的caffe prototxt的定义：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;roi_pool5&quot;</span><br><span class="line">  type: &quot;ROIPooling&quot;</span><br><span class="line">  bottom: &quot;conv5_3&quot;</span><br><span class="line">  bottom: &quot;rois&quot;</span><br><span class="line">  top: &quot;pool5&quot;</span><br><span class="line">  roi_pooling_param &#123;</span><br><span class="line">    pooled_w: 7</span><br><span class="line">    pooled_h: 7</span><br><span class="line">    spatial_scale: 0.0625 # 1/16</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其中有新参数pooled_w和pooled_h，另外一个参数spatial_scale认真阅读的读者肯定已经知道知道用途。RoI Pooling layer forward过程：</p>
<ul>
<li>由于proposal是对应MxN尺度的，所以首先使用spatial_scale参数将其映射回(M/16)x(N/16)大小的feature map尺度；</li>
<li>再将每个proposal对应的feature map区域水平分为 <img src="E:\笔记\markdown\reference\picture\text{pooled_h}.svg+xml" alt="[公式]"> 的网格；</li>
<li>对网格的每一份都进行max pooling处理。</li>
</ul>
<p>这样处理后，即使大小不同的proposal输出结果都是 <img src="E:\笔记\markdown\reference\picture\text{pooled_h}.svg+xml" alt="[公式]"> 固定大小，实现了固定长度输出。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-e3108dc5cdd76b871e21a4cb64001b5c_720w.jpg" alt="img">图15 proposal示意图</p>
<h2 id="4-Classification"><a href="#4-Classification" class="headerlink" title="4 Classification"></a>4 Classification</h2><p>Classification部分利用已经获得的proposal feature maps，通过full connect层与softmax计算每个proposal具体属于那个类别（如人，车，电视等），输出cls_prob概率向量；同时再次利用bounding box regression获得每个proposal的位置偏移量bbox_pred，用于回归更加精确的目标检测框。Classification部分网络结构如图16。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-9377a45dc8393d546b7b52a491414ded_720w.jpg" alt="img">图16 Classification部分网络结构图</p>
<p>从RoI Pooling获取到7x7=49大小的proposal feature maps后，送入后续网络，可以看到做了如下2件事：</p>
<ol>
<li>通过全连接和softmax对proposals进行分类，这实际上已经是识别的范畴了</li>
<li>再次对proposals进行bounding box regression，获取更高精度的rect box</li>
</ol>
<p>这里来看看全连接层InnerProduct layers，简单的示意图如图17，</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-38594a97f33ff56fc72542a20a78116d_720w.jpg" alt="img">图17 全连接层示意图</p>
<p>其计算公式如下：</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-f56d3209f9a7d5f27d77ead7489ab70f_720w.jpg" alt="img"></p>
<p>其中W和bias B都是预先训练好的，即大小是固定的，当然输入X和输出Y也就是固定大小。所以，这也就印证了之前Roi Pooling的必要性。到这里，我想其他内容已经很容易理解，不在赘述了。</p>
<h2 id="5-Faster-RCNN训练"><a href="#5-Faster-RCNN训练" class="headerlink" title="5 Faster RCNN训练"></a>5 Faster RCNN训练</h2><p>Faster R-CNN的训练，是在已经训练好的model（如VGG_CNN_M_1024，VGG，ZF）的基础上继续进行训练。实际中训练过程分为6个步骤：</p>
<ol>
<li>在已经训练好的model上，训练RPN网络，对应stage1_rpn_train.pt</li>
<li>利用步骤1中训练好的RPN网络，收集proposals，对应rpn_test.pt</li>
<li>第一次训练Fast RCNN网络，对应stage1_fast_rcnn_train.pt</li>
<li>第二训练RPN网络，对应stage2_rpn_train.pt</li>
<li>再次利用步骤4中训练好的RPN网络，收集proposals，对应rpn_test.pt</li>
<li>第二次训练Fast RCNN网络，对应stage2_fast_rcnn_train.pt</li>
</ol>
<p>可以看到训练过程类似于一种“迭代”的过程，不过只循环了2次。至于只循环了2次的原因是应为作者提到：”A similar alternating training can be run for more iterations, but we have observed negligible improvements”，即循环更多次没有提升了。接下来本章以上述6个步骤讲解训练过程。</p>
<p>下面是一张训练过程流程图，应该更加清晰：</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-ddfcf3dc29976e384b047418aec9002d_720w.jpg" alt="img"></p>
<h3 id="5-1-训练RPN网络"><a href="#5-1-训练RPN网络" class="headerlink" title="5.1 训练RPN网络"></a>5.1 训练RPN网络</h3><p>在该步骤中，首先读取RBG提供的预训练好的model（本文使用VGG），开始迭代训练。来看看stage1_rpn_train.pt网络结构，如图19。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-c39aef1d06e08e4e0cec96b10f50a779_720w.jpg" alt="img">图19 stage1_rpn_train.pt（考虑图片大小，Conv Layers中所有的层都画在一起了，如红圈所示，后续图都如此处理）</p>
<p>与检测网络类似的是，依然使用Conv Layers提取feature maps。整个网络使用的Loss如下：</p>
<p><img src="E:\笔记\markdown\reference\picture\tag{12}.svg+xml" alt="[公式]"></p>
<p>上述公式中 <img src="E:\笔记\markdown\reference\picture\equation-165046645349520.svg+xml" alt="[公式]"> 表示anchors index， <img src="E:\笔记\markdown\reference\picture\equation-165046645349521.svg+xml" alt="[公式]"> 表示positive softmax probability，<img src="E:\笔记\markdown\reference\picture\equation-165046645349522.svg+xml" alt="[公式]">代表对应的GT predict概率（即当第i个anchor与GT间IoU&gt;0.7，认为是该anchor是positive，<img src="E:\笔记\markdown\reference\picture\equation-165046645349523.svg+xml" alt="[公式]">；反之IoU&lt;0.3时，认为是该anchor是negative，<img src="E:\笔记\markdown\reference\picture\equation-165046645349524.svg+xml" alt="[公式]">；至于那些0.3&lt;IoU&lt;0.7的anchor则不参与训练）；<img src="E:\笔记\markdown\reference\picture\equation-165046645349525.svg+xml" alt="[公式]">代表predict bounding box，<img src="E:\笔记\markdown\reference\picture\equation-165046645349526.svg+xml" alt="[公式]">代表对应positive anchor对应的GT box。可以看到，整个Loss分为2部分：</p>
<ol>
<li>cls loss，即rpn_cls_loss层计算的softmax loss，用于分类anchors为positive与negative的网络训练</li>
<li>reg loss，即rpn_loss_bbox层计算的soomth L1 loss，用于bounding box regression网络训练。注意在该loss中乘了 <img src="E:\笔记\markdown\reference\picture\equation-165046645349522.svg+xml" alt="[公式]"> ，相当于只关心positive anchors的回归（其实在回归中也完全没必要去关心negative）。</li>
</ol>
<p>由于在实际过程中，<img src="E:\笔记\markdown\reference\picture\text{cls}.svg+xml" alt="[公式]">和<img src="E:\笔记\markdown\reference\picture\text{reg}.svg+xml" alt="[公式]">差距过大，用参数λ平衡二者（如<img src="E:\笔记\markdown\reference\picture\text{cls}%3D256.svg+xml" alt="[公式]">，<img src="E:\笔记\markdown\reference\picture\text{reg}%3D2400.svg+xml" alt="[公式]">时设置 <img src="E:\笔记\markdown\reference\picture\approx10.svg+xml" alt="[公式]"> ），使总的网络Loss计算过程中能够均匀考虑2种Loss。这里比较重要是 <img src="E:\笔记\markdown\reference\picture\text{reg}-165046645349627.svg+xml" alt="[公式]"> 使用的soomth L1 loss，计算公式如下：</p>
<p><img src="E:\笔记\markdown\reference\picture\tag{13}.svg+xml" alt="[公式]"></p>
<p><img src="E:\笔记\markdown\reference\picture\tag{14}.svg+xml" alt="[公式]"></p>
<p>了解数学原理后，反过来看图18：</p>
<ol>
<li>在RPN训练阶段，rpn-data（python AnchorTargetLayer）层会按照和test阶段Proposal层完全一样的方式生成Anchors用于训练</li>
<li>对于rpn_loss_cls，输入的rpn_cls_scors_reshape和rpn_labels分别对应 <img src="E:\笔记\markdown\reference\picture\equation-165046645349628.svg+xml" alt="[公式]"> 与 <img src="E:\笔记\markdown\reference\picture\equation-165046645349629.svg+xml" alt="[公式]"> ， <img src="E:\笔记\markdown\reference\picture\text{cls}.svg+xml" alt="[公式]"> 参数隐含在<img src="E:\笔记\markdown\reference\picture\equation-16504669589223.svg+xml" alt="[公式]">与<img src="E:\笔记\markdown\reference\picture\equation-16504669589224.svg+xml" alt="[公式]">的caffe blob的大小中</li>
<li>对于rpn_loss_bbox，输入的rpn_bbox_pred和rpn_bbox_targets分别对应 <img src="E:\笔记\markdown\reference\picture\equation-165046645349525.svg+xml" alt="[公式]"> 与 <img src="E:\笔记\markdown\reference\picture\equation-165046645349526.svg+xml" alt="[公式]"> ，rpn_bbox_inside_weigths对应 <img src="E:\笔记\markdown\reference\picture\equation-16504669589224.svg+xml" alt="[公式]">，rpn_bbox_outside_weigths未用到（从smooth_L1_Loss layer代码中可以看到），而 <img src="E:\笔记\markdown\reference\picture\text{reg}.svg+xml" alt="[公式]"> 同样隐含在caffe blob大小中</li>
</ol>
<p>这样，公式与代码就完全对应了。特别需要注意的是，在训练和检测阶段生成和存储anchors的顺序完全一样，这样训练结果才能被用于检测！</p>
<h3 id="5-2-通过训练好的RPN网络收集proposals"><a href="#5-2-通过训练好的RPN网络收集proposals" class="headerlink" title="5.2 通过训练好的RPN网络收集proposals"></a>5.2 通过训练好的RPN网络收集proposals</h3><p>在该步骤中，利用之前的RPN网络，获取proposal rois，同时获取positive softmax probability，如图20，然后将获取的信息保存在python pickle文件中。该网络本质上和检测中的RPN网络一样，没有什么区别。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-1ac5f8a2899ee413464ecf7866f8f840_720w.jpg" alt="img">图20 rpn_test.pt</p>
<h3 id="5-3-训练Faster-RCNN网络"><a href="#5-3-训练Faster-RCNN网络" class="headerlink" title="5.3 训练Faster RCNN网络"></a>5.3 训练Faster RCNN网络</h3><p>读取之前保存的pickle文件，获取proposals与positive probability。从data层输入网络。然后：</p>
<ol>
<li>将提取的proposals作为rois传入网络，如图21蓝框</li>
<li>计算bbox_inside_weights+bbox_outside_weights，作用与RPN一样，传入soomth_L1_loss layer，如图21绿框</li>
</ol>
<p>这样就可以训练最后的识别softmax与最终的bounding box regression了。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-fbece817952865689187e68f0af86792_720w.jpg" alt="img">图21 stage1_fast_rcnn_train.pt</p>
<p>之后的stage2训练都是大同小异，不再赘述了。Faster R-CNN还有一种end-to-end的训练方式，可以一次完成train，有兴趣请自己看作者GitHub吧。</p>
<p><a href="https://link.zhihu.com/?target=https://github.com/rbgirshick/py-faster-rcnn">rbgirshick py-faster-rcnngithub.com/rbgirshick/py-faster-rcnn<img src="E:\笔记\markdown\reference\picture\v2-9f4e9c49a8e59e08abe70f8ba9b14fef_ipico.jpg" alt="img"></a></p>
<h2 id="QA"><a href="#QA" class="headerlink" title="QA"></a>QA</h2><p>此篇文章初次成文于2016年内部学习分享，再后来经多次修正和完善成为现在的样子。感谢大家一直以来的支持，现在总结常见疑问回答如下：</p>
<ul>
<li>为什么Anchor坐标中有负数</li>
</ul>
<p>回顾anchor生成步骤：首先生成9个base anchor，然后通过坐标偏移在 <img src="E:\笔记\markdown\reference\picture\equation-165046645349630.svg+xml" alt="[公式]"> 大小的 <img src="E:\笔记\markdown\reference\picture\frac{1}{16}.svg+xml" alt="[公式]"> 下采样FeatureMap每个点都放上这9个base anchor，就形成了 <img src="E:\笔记\markdown\reference\picture\equation-165046645349416.svg+xml" alt="[公式]"> 个anhcors。至于这9个base anchor坐标是什么其实并不重要，不同代码实现也许不同。</p>
<p>显然这里面有一部分边缘anchors会超出图像边界，而真实中不会有超出图像的目标，所以会有clip anchor步骤。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-9d67146e0cb10397d8c2170794412608_720w.jpg" alt="img">图21 clip anchor</p>
<ul>
<li>Anchor到底与网络输出如何对应</li>
</ul>
<p>VGG输出 <img src="E:\笔记\markdown\reference\picture\equation-165046645349415.svg+xml" alt="[公式]"> 的特征，对应设置 <img src="E:\笔记\markdown\reference\picture\equation-165046645349416.svg+xml" alt="[公式]"> 个anchors，而RPN输出 <img src="E:\笔记\markdown\reference\picture\equation-165046645349417.svg+xml" alt="[公式]"> 的分类特征矩阵和 <img src="E:\笔记\markdown\reference\picture\equation-165046645349418.svg+xml" alt="[公式]"> 的坐标回归特征矩阵。</p>
<p><img src="E:\笔记\markdown\reference\picture\v2-82196feb7b528d76411feb90bfec2af4_720w.jpg" alt="img">图22 anchor与网络输出如何对应方式</p>
<p>其实在实现过程中，每个点的 <img src="E:\笔记\markdown\reference\picture\equation-165046645349731.svg+xml" alt="[公式]"> 个分类特征与 <img src="E:\笔记\markdown\reference\picture\equation-165046645349732.svg+xml" alt="[公式]"> 回归特征，与 <img src="E:\笔记\markdown\reference\picture\equation-165046645349733.svg+xml" alt="[公式]"> 个anchor逐个对应即可，这实际是一种“人为设置的逻辑映射”。当然，也可以不这样设置，但是无论如何都需要保证<strong>在训练和测试过程中映射方式必须一致</strong>。</p>
<ul>
<li>为何有ROI Pooling还要把输入图片resize到固定大小的MxN</li>
</ul>
<p>由于引入ROI Pooling，从原理上说Faster R-CNN确实能够检测任意大小的图片。但是由于在训练的时候需要使用大batch训练网络，而不同大小输入拼batch在实现的时候代码较为复杂，而且当时以Caffe为代表的第一代深度学习框架也不如Tensorflow和PyTorch灵活，所以作者选择了把输入图片resize到固定大小的800x600。这应该算是历史遗留问题。</p>
<p>另外很多问题，都是属于具体实现问题，真诚的建议读者阅读代码自行理解。</p>
<h2 id="拓展"><a href="#拓展" class="headerlink" title="拓展"></a>拓展</h2><p>关于torchvision中的FasterRCNN代码：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/145842317">捋一捋pytorch官方FasterRCNN代码1308 赞同 · 21 评论文章<img src="E:\笔记\markdown\reference\picture\v2-dc78d0ad1012ad65040bdb7eb657f381_180x120.jpg" alt="img"></a></p>
<p>Faster RCNN在文字检测中的应用：CTPN</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/34757009">场景文字检测—CTPN原理与实现678 赞同 · 54 评论文章<img src="E:\笔记\markdown\reference\picture\v2-2ea98126ebc05e35be28efe598a021ed_180x120.jpg" alt="img"></a></p>
<p>Faster RCNN在高德导航中的应用：</p>
<p><a href="https://link.zhihu.com/?target=https://mp.weixin.qq.com/s/IJUMCOBhgXHv7VC1YT4q_g">机器学习在交通标志检测与精细分类中的应用mp.weixin.qq.com/s/IJUMCOBhgXHv7VC1YT4q_g<img src="E:\笔记\markdown\reference\picture\v2-b7f1350a3179674d93d60d8ad9b52c0f_180x120.jpg" alt="img"></a></p>

      
    </div>
    
  </div>
  
    
    <div class="copyright">
        <p><span>本文标题:</span><a href="/2022/04/20/faster%20rcnn/"></a></p>
        <p><span>文章作者:</span><a href="/" title="回到主页">流沙</a></p>
        <p><span>发布时间:</span>2022-04-20, 22:54:10</p>
        <p><span>最后更新:</span>2022-05-05, 14:40:44</p>
        <p>
            <span>原始链接:</span><a class="post-url" href="/2022/04/20/faster%20rcnn/" title="">https://jpccc.github.io/2022/04/20/faster%20rcnn/</a>
            <span class="copy-path" data-clipboard-text="原文: https://jpccc.github.io/2022/04/20/faster%20rcnn/　　作者: " title="点击复制文章链接"><i class="fa fa-clipboard"></i></span>
            <script> var clipboard = new Clipboard('.copy-path'); </script>
        </p>
        <p>
            <span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="CC BY-NC-SA 4.0 International" target = "_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。
        </p>
    </div>



    <nav id="article-nav">
        
            <div id="article-nav-newer" class="article-nav-title">
                <a href="/2022/04/20/RCNN/">
                    
                </a>
            </div>
        
        
            <div id="article-nav-older" class="article-nav-title">
                <a href="/2022/04/20/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/">
                    
                </a>
            </div>
        
    </nav>

  
</article>

    <div id="toc" class="toc-article">
        <strong class="toc-title">文章目录</strong>
        
            <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#RCNN-%E5%B0%86CNN%E5%BC%95%E5%85%A5%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%9A%84%E5%BC%80%E5%B1%B1%E4%B9%8B%E4%BD%9C"><span class="toc-number">1.</span> <span class="toc-text">RCNN- 将CNN引入目标检测的开山之作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E3%80%90%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B-%E3%80%91"><span class="toc-number">1.1.</span> <span class="toc-text">【基本流程 &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;】</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E3%80%90%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-%E3%80%91"><span class="toc-number">1.2.</span> <span class="toc-text">【基础知识 &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;】</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Selective-Search-%E4%B8%BB%E8%A6%81%E6%80%9D%E6%83%B3"><span class="toc-number">1.2.1.</span> <span class="toc-text">Selective Search 主要思想:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%89%E7%9B%91%E7%9D%A3%E9%A2%84%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-number">1.2.2.</span> <span class="toc-text">有监督预训练与无监督预训练:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%8D%E5%8F%A0%E5%BA%A6%EF%BC%88IOU%EF%BC%89"><span class="toc-number">1.2.3.</span> <span class="toc-text">重叠度（IOU）:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9D%9E%E6%9E%81%E5%A4%A7%E5%80%BC%E6%8A%91%E5%88%B6%EF%BC%88NMS%EF%BC%89%EF%BC%9A"><span class="toc-number">1.2.4.</span> <span class="toc-text">非极大值抑制（NMS）：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#VOC%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E4%BB%BB%E5%8A%A1"><span class="toc-number">1.2.5.</span> <span class="toc-text">VOC物体检测任务:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E3%80%90%E5%90%84%E4%B8%AA%E9%98%B6%E6%AE%B5%E8%AF%A6%E8%A7%A3-%E3%80%91"><span class="toc-number">1.3.</span> <span class="toc-text">【各个阶段详解 &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;】</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%80%99%E9%80%89%E6%A1%86%E6%90%9C%E7%B4%A2%E9%98%B6%E6%AE%B5%EF%BC%9A"><span class="toc-number">1.3.1.</span> <span class="toc-text">候选框搜索阶段：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CNN%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E9%98%B6%E6%AE%B5%EF%BC%9A"><span class="toc-number">1.3.2.</span> <span class="toc-text">CNN特征提取阶段：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SVM%E8%AE%AD%E7%BB%83%E3%80%81%E6%B5%8B%E8%AF%95%E9%98%B6%E6%AE%B5"><span class="toc-number">1.3.3.</span> <span class="toc-text">SVM训练、测试阶段</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#SPPnet"><span class="toc-number">2.</span> <span class="toc-text">SPPnet</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">2.1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%86%E8%8A%82"><span class="toc-number">2.2.</span> <span class="toc-text">网络细节</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%98%E5%9C%A8%E7%9A%84%E4%B8%8D%E8%B6%B3"><span class="toc-number">2.3.</span> <span class="toc-text">存在的不足</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Fast-RCNN"><span class="toc-number">3.</span> <span class="toc-text">Fast RCNN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B4%E4%BD%93%E7%BB%93%E6%9E%84"><span class="toc-number">3.1.</span> <span class="toc-text">整体结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ROI"><span class="toc-number">3.2.</span> <span class="toc-text">ROI</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">3.3.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">3.4.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SVD%E5%8A%A0%E9%80%9F"><span class="toc-number">3.5.</span> <span class="toc-text">SVD加速</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">3.6.</span> <span class="toc-text">实验结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%BF%9E%E6%8E%A5%EF%BC%9A"><span class="toc-number">3.7.</span> <span class="toc-text">参考连接：</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Faster-RCNN"><span class="toc-number">4.</span> <span class="toc-text">Faster RCNN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Conv-layers"><span class="toc-number">4.1.</span> <span class="toc-text">1 Conv layers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Region-Proposal-Networks-RPN"><span class="toc-number">4.2.</span> <span class="toc-text">2 Region Proposal Networks(RPN)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E5%A4%9A%E9%80%9A%E9%81%93%E5%9B%BE%E5%83%8F%E5%8D%B7%E7%A7%AF%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E4%BB%8B%E7%BB%8D"><span class="toc-number">4.2.1.</span> <span class="toc-text">2.1 多通道图像卷积基础知识介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-anchors"><span class="toc-number">4.2.2.</span> <span class="toc-text">2.2 anchors</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-softmax%E5%88%A4%E5%AE%9Apositive%E4%B8%8Enegative"><span class="toc-number">4.2.3.</span> <span class="toc-text">2.3 softmax判定positive与negative</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-bounding-box-regression%E5%8E%9F%E7%90%86"><span class="toc-number">4.2.4.</span> <span class="toc-text">2.4 bounding box regression原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-%E5%AF%B9proposals%E8%BF%9B%E8%A1%8Cbounding-box-regression"><span class="toc-number">4.2.5.</span> <span class="toc-text">2.5 对proposals进行bounding box regression</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-Proposal-Layer"><span class="toc-number">4.2.6.</span> <span class="toc-text">2.6 Proposal Layer</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-RoI-pooling"><span class="toc-number">4.3.</span> <span class="toc-text">3 RoI pooling</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E4%B8%BA%E4%BD%95%E9%9C%80%E8%A6%81RoI-Pooling"><span class="toc-number">4.3.1.</span> <span class="toc-text">3.1 为何需要RoI Pooling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-RoI-Pooling%E5%8E%9F%E7%90%86"><span class="toc-number">4.3.2.</span> <span class="toc-text">3.2 RoI Pooling原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Classification"><span class="toc-number">4.4.</span> <span class="toc-text">4 Classification</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Faster-RCNN%E8%AE%AD%E7%BB%83"><span class="toc-number">4.5.</span> <span class="toc-text">5 Faster RCNN训练</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E8%AE%AD%E7%BB%83RPN%E7%BD%91%E7%BB%9C"><span class="toc-number">4.5.1.</span> <span class="toc-text">5.1 训练RPN网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E9%80%9A%E8%BF%87%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84RPN%E7%BD%91%E7%BB%9C%E6%94%B6%E9%9B%86proposals"><span class="toc-number">4.5.2.</span> <span class="toc-text">5.2 通过训练好的RPN网络收集proposals</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E8%AE%AD%E7%BB%83Faster-RCNN%E7%BD%91%E7%BB%9C"><span class="toc-number">4.5.3.</span> <span class="toc-text">5.3 训练Faster RCNN网络</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#QA"><span class="toc-number">4.6.</span> <span class="toc-text">QA</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8B%93%E5%B1%95"><span class="toc-number">4.7.</span> <span class="toc-text">拓展</span></a></li></ol></li></ol>
        
    </div>
    <style>
        .left-col .switch-btn,
        .left-col .switch-area {
            display: none;
        }
        .toc-level-3 i,
        .toc-level-3 ol {
            display: none !important;
        }
    </style>

    <input type="button" id="tocButton" value="隐藏目录"  title="点击按钮隐藏或者显示文章目录">

    <script>
        yiliaConfig.toc = ["隐藏目录", "显示目录", !!"false"];
    </script>



    
<div class="share">
    

    
</div>







    




    <div class="scroll" id="post-nav-button">
        
            <a href="/2022/04/20/RCNN/" title="上一篇: ">
                <i class="fa fa-angle-left"></i>
            </a>
        

        <a title="文章列表"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a>

        
            <a href="/2022/04/20/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" title="下一篇: ">
                <i class="fa fa-angle-right"></i>
            </a>
        
    </div>

    <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2030/04/07/%E5%BF%83%E5%BE%97/">心得</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/20/RCNN/">RCNN</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/20/faster%20rcnn/">faster rcnn</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/20/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/">无监督学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/20/%E8%87%AA%E7%9B%91%E7%9D%A3/">自监督</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/11/%E5%9B%BE%E5%83%8F%E7%9A%84%E6%93%8D%E4%BD%9C/">图像的操作</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/07/%E7%89%B9%E8%89%B2%E5%8C%85/">特色包</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/07/python%E8%AF%AD%E6%B3%95/">python语法</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/07/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/">聚类</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/07/pytorch%E4%BD%BF%E7%94%A8%E6%B1%87%E6%80%BB/">pytorch使用汇总</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20tricks/">深度学习训练tricks</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/04/06/pytorch%E8%AE%A1%E7%AE%97%E5%9B%BE/">Understanding Graphs, Automatic Differentiation and Autograd</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/28/Jensen-inequality/">Jensen_inequality</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/28/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0part1/">统计学习数学基础-1</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/25/BN%E7%AE%97%E6%B3%95/">BN算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/25/%E4%BA%92%E4%BF%A1%E6%81%AF/">互信息</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/22/algebra/">algebra</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/17/hello-world/">Hello World</a></li></ul>




    <script>
        
    </script>
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2021-2022  liusha
            </div>
            <div class="footer-right">
                回首向来萧瑟处，归去，也无风雨也无晴。
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_uv">
  本站访客数<span id="busuanzi_value_site_uv"></span>人次
</span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="本页阅读量"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>