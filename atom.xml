<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>流沙</title>
  
  <subtitle>Artificial Intelligence</subtitle>
  <link href="https://jpccc.github.io/atom.xml" rel="self"/>
  
  <link href="https://jpccc.github.io/"/>
  <updated>2022-04-06T01:32:53.828Z</updated>
  <id>https://jpccc.github.io/</id>
  
  <author>
    <name> liusha</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Understanding Graphs, Automatic Differentiation and Autograd</title>
    <link href="https://jpccc.github.io/2022/04/06/pytorch%E8%AE%A1%E7%AE%97%E5%9B%BE/"/>
    <id>https://jpccc.github.io/2022/04/06/pytorch%E8%AE%A1%E7%AE%97%E5%9B%BE/</id>
    <published>2022-04-05T16:08:00.000Z</published>
    <updated>2022-04-06T01:32:53.828Z</updated>
    
    <content type="html"><![CDATA[<div align=center><p><img src="https://jpccc.github.io/resource/pytorch/full_graph.png" alt="img"></p></div><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>PyTorch is one of the foremost python deep learning libraries out there. It’s the go to choice for deep learning research, and as each days passes by, more and more companies and research labs are adopting this library.</p><p>In this series of tutorials, we will be introducing you to PyTorch, and how to make the best use of the libraries as well the ecosystem of tools built around it. We’ll first cover the basic building blocks, and then move onto how you can quickly prototype custom architectures. We will finally conclude with a couple of posts on how to scale your code, and how to debug your code if things go awry.</p><p>This is Part 1 of our PyTorch 101 series.</p><span id="more"></span><ol><li><a href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/">Understanding Graphs, Automatic Differentiation and Autograd</a></li><li><a href="https://blog.paperspace.com/pytorch-101-building-neural-networks/">Building Your First Neural Network</a></li><li><a href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/blog.paperspace.com/pytorch-101-advanced/">Going Deep with PyTorch</a></li><li><a href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/blog.paperspace.com/pytorch-memory-multi-gpu-debugging/">Memory Management and Using Multiple GPUs</a></li><li><a href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/">Understanding Hooks</a></li></ol><p>You can get all the code in this post, (and other posts as well) in the Github repo <a href="https://github.com/Paperspace/PyTorch-101-Tutorial-Series">here</a>.</p><hr><h2 id="Prerequisites"><a href="#Prerequisites" class="headerlink" title="Prerequisites"></a>Prerequisites</h2><ol><li>Chain rule</li><li>Basic Understanding of Deep Learning</li><li>PyTorch 1.0</li></ol><hr><p>You can get all the code in this post, (and other posts as well) in the Github repo <a href="https://github.com/Paperspace/PyTorch-101-Tutorial-Series">here</a>.</p><h2 id="Automatic-Differentiation"><a href="#Automatic-Differentiation" class="headerlink" title="Automatic Differentiation"></a><strong>Automatic</strong> Differentiation</h2><p>A lot of tutorial series on PyTorch would start begin with a rudimentary discussion of what the basic structures are. However, I’d like to instead start by discussing automatic differentiation first.</p><p>Automatic Differentiation is a building block of not only PyTorch, but every DL library out there. In my opinion, PyTorch’s automatic differentiation engine, called <em>Autograd</em> is a brilliant tool to understand how automatic differentiation works. This will not only help you understand PyTorch better, but also other DL libraries.</p><p>Modern neural network architectures can have millions of learnable parameters. From a computational point of view, training a neural network consists of two phases:</p><ol><li>A forward pass to compute the value of the loss function.</li><li>A backward pass to compute the gradients of the learnable parameters.</li></ol><p>The forward pass is pretty straight forward. The output of one layer is the input to the next and so forth.</p><p>Backward pass is a bit more complicated since it requires us to use the chain rule to compute the gradients of weights w.r.t to the loss function.</p><h2 id="A-toy-example"><a href="#A-toy-example" class="headerlink" title="A toy example"></a>A toy example</h2><p>Let us take an very simple neural network consisting of just 5 neurons. Our neural network looks like the following.</p><div align=center><p><img src="https://jpccc.github.io//resource/pytorch/full_graph-16491739555532.png" alt="img"></p></div><p>The following equations describe our neural network.</p><p>$$<br>b=w1∗a\\<br>c=w2∗a\\<br>d=w3∗b+w4∗c\\<br>L=10−d\\<br>$$</p><p>Let us compute the gradients for each of the learnable parameters ww.</p><p>$$<br>\frac{\partial{L}}{\partial{w_4}} = \frac{\partial{L}}{\partial{d}}\frac{\partial{d}}{\partial{w_4}} \\<br>\frac{\partial{L}}{\partial{w_3}} = \frac{\partial{L}}{\partial{d}}\frac{\partial{d}}{\partial{w_3}}\\<br>\frac{\partial{L}}{\partial{w_2}} = \frac{\partial{L}}{\partial{d}} * \frac{\partial{d}}{\partial{c}} * \frac{\partial{c}}{\partial{w_2}}\\<br>\frac{\partial{L}}{\partial{w_1}} = \frac{\partial{L}}{\partial{d}}* \frac{\partial{d}}{\partial{b}} * \frac{\partial{b}}{\partial{w_1}}\\<br>$$</p><p>All these gradients have been computed by applying the chain rule. Note that all the individual gradients on the right hand side of the equations mentioned above can be computed directly since the <em>numerators</em> of the gradients are explicit functions of the <em>denominators.</em></p><hr><h2 id="Computation-Graphs"><a href="#Computation-Graphs" class="headerlink" title="Computation Graphs"></a>Computation Graphs</h2><p>We could manually compute the gradients of our network as it was very simple. Imagine, what if you had a network with 152 layers. Or, if the network had multiple branches.</p><p>When we design software to implement neural networks, we want to come up with a way that can allow us to seamlessly compute the gradients, regardless of the architecture type so that the programmer doesn’t have to manually compute gradients when changes are made to the network.</p><p>We galvanize(激励,启发) this idea in form of a data structure called a <strong>Computation graph</strong>. A computation graph looks very similar to the diagram of the graph that we made in the image above. However, the nodes in a computation graph are basically <strong>operators</strong>. These operators are basically the mathematical operators except for one case, where we need to represent creation of a user-defined variable.</p><p>Notice that we have also denoted(表示…) the leaf variables <strong>a,w1,w2,w3,w4</strong> in the graph for sake of clarity. However, it should noted that they are not a part of the computation graph. What they represent in our graph is the special case for user-defined variables which we just covered as an exception.</p><div align=center><p><img src="https://jpccc.github.io/resource/pytorch/computation_graph.png" alt="img"></p></div><p>The variables, <em>b,c</em> and <em>d</em> are created as a result of mathematical operations, whereas variables <em>a, w1, w2, w3</em> and <em>w4</em> are initialised by the user itself. Since, they are not created by any mathematical operator, nodes corresponding to their creation is represented by their name itself. This is true for all the <em>leaf</em> nodes in the graph.</p><hr><h2 id="Computing-the-gradients"><a href="#Computing-the-gradients" class="headerlink" title="Computing the gradients"></a>Computing the gradients</h2><p>Now, we are ready to describe how we will compute gradients using a computation graph.</p><p>Each node of the computation graph, with the exception of(除了…外) leaf nodes, can be considered as a function which takes some inputs and produces an output. Consider the node of the graph which produces variable <em>d</em> from w4cand w3b. Therefore we can write,</p><p>$$<br>d=f(w_3b,w_4c)<br>$$</p><div align=center><p><img src="https://jpccc.github.io/resource/pytorch/d_mini.png" alt="img"></p></div><p>​                                                                                            <center>    d is output of function f(x,y) = x + y  </center></p><p>Now, we can easily compute the gradient of the ff with respect to it’s inputs, $\frac{\partial{f}}{\partial{w_3b}}$ and $\frac{\partial{f}}{\partial{w_4b}}$ (which are both 1). Now, label the edges coming into the nodes with their respective gradients like the following image.</p><div align=center><p><img src="https://jpccc.github.io/resource/pytorch/d_mini_grad.png" alt="img"></p></div><p>​                                                                                                            <center>    Local Gradients </center></p><p>We do it for the entire graph. The graph looks like this.</p><div align=center><p><img src="https://jpccc.github.io/resource/pytorch/full_graph-16491739555532.png" alt="img"></p></div><center>Back propagation in an Computational Graph</center><p>Following we describe the algorithm for computing derivative(微分) of any node in this graph with respect to the loss, LL. Let’s say we want to compute the derivative, $\frac{\partial{f}}{\partial{w_4}}$</p><ol><li>We first trace down all possible paths from <em>d</em> to $w_4$.</li><li>There is only one such path.</li><li>We multiply all the edges long this path.</li></ol><p>If you see, the product is precisely the same expression we derived using chain rule. If there is more than one path to a variable from <em>L</em> then, we multiply the edges along each path and then add them together. For example,$\frac{\partial{L}}{\partial{a}}$  is computed as</p><p>$$<br>\frac{\partial{L}}{\partial{w_4}} = \frac{\partial{L}}{\partial{d}}*\frac{\partial{d}}{\partial{b}}*\frac{\partial{b}}{\partial{a}} + \frac{\partial{L}}{\partial{d}}*\frac{\partial{d}}{\partial{c}}*\frac{\partial{c}}{\partial{a}}<br>$$</p><h2 id="PyTorch-Autograd"><a href="#PyTorch-Autograd" class="headerlink" title="PyTorch Autograd"></a>PyTorch Autograd</h2><p>Now we get what a computational graph is, let’s get back to PyTorch and understand how the above is implemented in PyTorch.</p><p><em><strong>(注意：对谁求导，对应的导数就保存在对应变量中)</strong></em></p><h3 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h3><p><code>Tensor</code> is a data structure which is a fundamental building block of PyTorch. <code>Tensor</code>s are pretty much like numpy arrays, except that unlike numpy, tensors are designed to take advantage of parallel computation capabilities of a GPU. A lot of Tensor syntax(语法) is similar to that of numpy arrays.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]:  <span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: tsr = torch.Tensor(<span class="number">3</span>,<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: tsr</span><br><span class="line">Out[<span class="number">3</span>]: </span><br><span class="line">tensor([[ <span class="number">0.0000e+00</span>,  <span class="number">0.0000e+00</span>,  <span class="number">8.4452e-29</span>, -<span class="number">1.0842e-19</span>,  <span class="number">1.2413e-35</span>],</span><br><span class="line">        [ <span class="number">1.4013e-45</span>,  <span class="number">1.2416e-35</span>,  <span class="number">1.4013e-45</span>,  <span class="number">2.3331e-35</span>,  <span class="number">1.4013e-45</span>],</span><br><span class="line">        [ <span class="number">1.0108e-36</span>,  <span class="number">1.4013e-45</span>,  <span class="number">8.3641e-37</span>,  <span class="number">1.4013e-45</span>,  <span class="number">1.0040e-36</span>]])</span><br></pre></td></tr></table></figure><p>One it’s own, <code>Tensor</code> is just like a numpy <code>ndarray</code>. A data structure that can let you do fast linear algebra options. If you want PyTorch to create a graph corresponding to these operations, you will have to set the <code>requires_grad</code> attribute of the <code>Tensor</code> to True.</p><p>The API can be a bit confusing here. There are multiple ways to initialise tensors in PyTorch. While some ways can let you explicitly define that the <code>requires_grad</code> in the constructor itself, others require you to set it manually after creation of the Tensor.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; t1 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>) </span><br><span class="line"></span><br><span class="line">&gt;&gt; t2 = torch.FloatTensor(<span class="number">3</span>,<span class="number">3</span>) <span class="comment"># No way to specify requires_grad while initiating </span></span><br><span class="line">&gt;&gt; t2.requires_grad = <span class="literal">True</span></span><br></pre></td></tr></table></figure><p><code>requires_grad</code> is contagious. It means that when a <code>Tensor</code> is created by operating on other <code>Tensor</code>s, the <code>requires_grad</code> of the resultant <code>Tensor</code> would be set <code>True</code> given at least one of the tensors used for creation has it’s <code>requires_grad</code> set to <code>True</code>.</p><p>Each <code>Tensor</code> has a something an attribute called <code>grad_fn</code><em>,</em> which refers to the <strong>mathematical operator that create the variable</strong>. If <code>requires_grad</code> is set to False, <code>grad_fn</code> would be None.</p><p>In our example where, $d=f(w_3b,w_4c)$, <em>d</em>‘s grad function would be the addition operator, since <em>f</em> adds it’s to input together. Notice, addition operator is also the node in our graph that output’s <em>d</em>. If our <code>Tensor</code> is a leaf node (initialised by the user), then the <code>grad_fn</code> is also None.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line">a = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">w1 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w2 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w3 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w4 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">b = w1*a </span><br><span class="line">c = w2*a</span><br><span class="line"></span><br><span class="line">d = w3*b + w4*c </span><br><span class="line"></span><br><span class="line">L = <span class="number">10</span> - d</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The grad fn for a is&quot;</span>, a.grad_fn)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The grad fn for d is&quot;</span>, d.grad_fn)</span><br></pre></td></tr></table></figure><p>If you run the code above, you get the following output.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">The grad fn <span class="keyword">for</span> a <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">The grad fn <span class="keyword">for</span> d <span class="keyword">is</span> &lt;AddBackward0 <span class="built_in">object</span> at <span class="number">0x1033afe48</span>&gt;</span><br></pre></td></tr></table></figure><p>One can use the member function <code>is_leaf</code> to determine whether a variable is a leaf <code>Tensor</code> or not.</p><h3 id="Function"><a href="#Function" class="headerlink" title="Function"></a>Function</h3><p>All mathematical operations in PyTorch are implemented by the <em>torch.nn.Autograd.Function</em> class. This class has two important member functions we need to look at.</p><ul><li>The first is it’s <em>forward</em>  function, which simply computes the output using it’s inputs.</li></ul><ul><li>The <code>backward</code> function takes the incoming gradient coming from the the part of the network in front of it. As you can see, the gradient to be backpropagated from a function <em>f</em> is basically the <strong>gradient that is backpropagated to f from the layers in front of it</strong> multiplied by <strong>the local gradient of the output of f with respect to it’s inputs</strong>（链式规则）. This is exactly what the <code>backward</code> function does.</li></ul><p>Let’s again understand with our example of<br>$$<br>d=f(w_3b,w_4c)<br>$$</p><ol><li><em>d</em> is our <code>Tensor</code> here. It’s <code>grad_fn</code> is <code>&lt;ThAddBackward&gt;</code><em>.</em> This is basically the addition operation since the function that creates <em>d</em> adds inputs.</li><li>The <code>forward</code> function of the it’s <code>grad_fn</code> receives the inputs $w_3b$ <em>and</em> $w_4c$ and adds them. This value is basically stored in the <em>d</em>.</li><li>The <code>backward</code> function of the <code>&lt;ThAddBackward&gt;</code> basically takes the the <strong>incoming gradient</strong> from the further layers as the input. This is basically $\frac{\partial{L}}{\partial{d}}$ coming along the edge leading from <em>L</em> to <em>d.</em> This gradient is also the gradient of <em>L</em> w.r.t to <em>d</em> and is stored in <code>grad</code> attribute of the <code>d</code>. It can be accessed by calling <code>d.grad</code><em>.</em></li><li>It then takes computes the local gradients $\frac{\partial{d}}{\partial{w_4c}}$and$\frac{\partial{d}}{\partial{w_3b}}$.</li><li>Then the backward function multiplies the incoming gradient with the <strong>locally computed gradients</strong> respectively and <em><strong>“<em><strong>sends</strong></em>“</strong></em> the gradients to it’s inputs by invoking the backward method of the <code>grad_fn</code> of their inputs.</li><li>For example, the <code>backward</code> function of <code>&lt;ThAddBackward&gt;</code> associated with <em>d</em> invokes(援引，调用) backward function of the <em>grad_fn</em> of the $w_4∗c$∗(Here, $w_4∗c$ is a intermediate Tensor, and it’s <em>grad_fn</em> is <code>&lt;ThMulBackward&gt;</code>. At time of invocation of the <code>backward</code> function, the gradient $\frac{\partial{L}}{\partial{d}}*\frac{\partial{d}}{\partial{w_4c}}$ is passed as the input.</li><li>Now, for the variable $w_4∗c$, $\frac{\partial{L}}{\partial{d}}*\frac{\partial{d}}{\partial{w_4c}}$ becomes the incoming gradient, $\frac{\partial{L}}{\partial{d}}$ was for $d$ in step 3 and the process repeats.</li></ol><p>Algorithmically, here’s how back propagation happens with a computation graph. (Not the actual implementation, only representative)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def backward (incoming_gradients):</span><br><span class="line">self.Tensor.grad = incoming_gradients</span><br><span class="line"></span><br><span class="line">for inp in self.inputs:</span><br><span class="line">if inp.grad_fn is not None:</span><br><span class="line">new_incoming_gradients = //</span><br><span class="line">  incoming_gradient * local_grad(self.Tensor, inp)</span><br><span class="line"></span><br><span class="line">inp.grad_fn.backward(new_incoming_gradients)</span><br><span class="line">else:</span><br><span class="line">pass</span><br></pre></td></tr></table></figure><p>Here, <code>self.Tensor</code> is basically the <code>Tensor</code> created by Autograd.Function, which was <em>d</em> in our example.</p><p>Incoming gradients and local gradients have been described above.</p><hr><p>In order to compute derivatives in our neural network, we generally call <code>backward</code> on the <code>Tensor</code> representing our loss. Then, we backtrack through the graph starting from node representing the <code>grad_fn</code> of our loss.</p><p>As described above, the <code>backward</code> function is recursively called through out the graph as we backtrack. Once, we reach a leaf node, since the <code>grad_fn</code> is None, but stop backtracking through that path.</p><p>One thing to note here is that PyTorch gives an error if you call <code>backward()</code> on vector-valued Tensor. This means you can only call <code>backward</code> on a scalar valued Tensor. In our example, if we assume <code>a</code> to be a vector valued Tensor, and call <code>backward</code> on L, it will throw up an error.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line">a = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">w1 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w2 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w3 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w4 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">b = w1*a </span><br><span class="line">c = w2*a</span><br><span class="line"></span><br><span class="line">d = w3*b + w4*c </span><br><span class="line"></span><br><span class="line">L = (<span class="number">10</span> - d)</span><br><span class="line"></span><br><span class="line">L.backward()</span><br></pre></td></tr></table></figure><p>Running the above snippet results in the following error.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RuntimeError: grad can be implicitly created only <span class="keyword">for</span> scalar outputs</span><br></pre></td></tr></table></figure><p>This is because gradients can be computed with respect to scalar values by definition. You can’t exactly differentiate a vector with respect to another vector. The mathematical entity used for such cases is called a <strong>Jacobian,</strong> the discussion of which is beyond the scope of this article.</p><p>There are two ways to overcome this.</p><p>If you just make a small change in the above code setting <code>L</code> to be the sum of all the errors, our problem will be solved.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line">a = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">w1 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w2 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w3 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w4 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">b = w1*a </span><br><span class="line">c = w2*a</span><br><span class="line"></span><br><span class="line">d = w3*b + w4*c </span><br><span class="line"></span><br><span class="line"><span class="comment"># Replace L = (10 - d) by </span></span><br><span class="line">L = (<span class="number">10</span> -d).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">L.backward()</span><br></pre></td></tr></table></figure><p>Once that’s done, you can access the gradients by calling the <code>grad</code> attribute of <code>Tensor</code>.</p><p><strong>Second way is</strong>, for some reason have to absolutely call <code>backward</code> on a vector function, you can pass a <code>torch.ones</code> of size of shape of the tensor you are trying to call backward with.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Replace L.backward() with </span></span><br><span class="line">L.backward(torch.ones(L.shape))</span><br></pre></td></tr></table></figure><p>Notice how <code>backward</code> used to take incoming gradients as it’s input. Doing the above makes the <code>backward</code> think that incoming gradient are just Tensor of ones of same size as L, and it’s able to back propagate.</p><p>In this way, we can have gradients for every <code>Tensor</code> , and we can update them using Optimisation algorithm of our choice.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w1 = w1 - learning_rate * w1.grad</span><br></pre></td></tr></table></figure><p>And so on.</p><h2 id="How-are-PyTorch’s-graphs-different-from-TensorFlow-graphs"><a href="#How-are-PyTorch’s-graphs-different-from-TensorFlow-graphs" class="headerlink" title="How are PyTorch’s graphs different from TensorFlow graphs"></a>How are PyTorch’s graphs different from TensorFlow graphs</h2><p>PyTorch creates something called a <strong>Dynamic Computation Graph,</strong> which means that the graph is generated on the fly.</p><p>Until the <code>forward</code> function of a Variable is called, there exists no node for the <code>Tensor</code> <em>(<em>it’s <code>grad_fn</code></em>)</em> in the graph.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)   <span class="comment">#No graph yet, as a is a leaf</span></span><br><span class="line"></span><br><span class="line">w1 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)  <span class="comment">#Same logic as above</span></span><br><span class="line"></span><br><span class="line">b = w1*a   <span class="comment">#Graph with node `mulBackward` is created.</span></span><br></pre></td></tr></table></figure><p>The graph is created as a result of <code>forward</code> function of many <em>Tensors</em> being invoked. Only then, the buffers for the non-leaf nodes allocated for the graph and intermediate values (used for computing gradients later.  When you call <code>backward</code>, as the gradients are computed, these buffers (for non-leaf variables) are essentially freed, and the graph is <em>destroyed</em> ( In a sense, you can’t backpropagate through it since the buffers holding values to compute the gradients are gone).</p><p>Next time, you will call <code>forward</code> on the same set of tensors, <strong>the leaf node buffers from the previous run will be shared, while the non-leaf nodes buffers will be created again.</strong></p><p>If you call <code>backward</code> more than once on a graph with non-leaf nodes, you’ll be met with the following error.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=<span class="literal">True</span> when calling backward the first time.</span><br></pre></td></tr></table></figure><p>This is because the non-leaf buffers gets destroyed the first time <code>backward()</code> is called and hence, there’s no path to navigate to the leaves when <code>backward</code> is invoked the second time. You can undo this non-leaf buffer destroying behaviour by adding <code>retain_graph = True</code> argument to the <code>backward</code> function.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss.backward(retain_graph = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>If you do the above, you will be able to backpropagate again through the same graph and the gradients will be accumulated, i.e. the next you backpropagate, the gradients will be added to those already stored in the previous back pass.</p><hr><p>This is in contrast to the <em><strong>Static Computation Graphs</strong></em>, used by TensorFlow where the graph is declared <strong>before</strong> running the program. Then the graph is “run” by feeding values to the predefined graph.</p><p>The dynamic graph paradigm allows you to make changes to your network architecture <em>during</em> runtime, as a graph is created only when a piece of code is run.</p><p>This means a graph may be redefined during the lifetime for a program since you don’t have to define it beforehand.</p><p>This, however, is not possible with static graphs where graphs are created before running the program, and merely executed later.</p><p>Dynamic graphs also make debugging way easier since it’s easier to locate the source of your error.</p><h2 id="Some-Tricks-of-Trade"><a href="#Some-Tricks-of-Trade" class="headerlink" title="Some Tricks of Trade"></a>Some Tricks of Trade</h2><h3 id="requires-grad"><a href="#requires-grad" class="headerlink" title="requires_grad"></a>requires_grad</h3><p>This is an attribute of the <code>Tensor</code> class. By default, it’s False. It comes handy when you have to freeze some layers, and stop them from updating parameters while training. You can simply set the <code>requires_grad</code> to False, and these <code>Tensors</code> won’t participate in the computation graph.</p><div align=center><p><img src="https://jpccc.github.io/resource/pytorch/image-4.png" alt="img"></p></div><p>Thus, no gradient would be propagated to them, or to those layers which depend upon these layers for gradient flow <code>requires_grad</code>. When set to True, <code>requires_grad</code> is contagious meaning even if one operand of an operation has <code>requires_grad</code> set to True, so will the result.</p><h3 id="torch-no-grad"><a href="#torch-no-grad" class="headerlink" title="torch.no_grad()"></a>torch.no_grad()</h3><p>When we are computing gradients, we need to cache input values, and intermediate features as they maybe required to compute the gradient later.</p><p>The gradient of $ b=w_1∗a$ w.r.t it’s inputs w1w1 and aa is aa and w1w1 respectively. We need to store these values for gradient computation during the backward pass. This affects the memory footprint of the network.</p><p>While, we are performing inference, we don’t compute gradients, and thus, don’t need to store these values. Infact, no graph needs to be create during inference as it will lead to useless consumption of memory.</p><p>PyTorch offers a context manager, called <code>torch.no_grad</code> for this purpose.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad:</span><br><span class="line">inference code goes here </span><br></pre></td></tr></table></figure><p>No graph is defined for operations executed under this context manager.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Understanding how <em>Autograd</em> and computation graphs works can make life with PyTorch a whole lot easier. With our foundations rock solid, the next posts will detail how to create custom complex architectures, how to create custom data pipelines and more interesting stuff.</p><h2 id="Further-Reading"><a href="#Further-Reading" class="headerlink" title="Further Reading"></a>Further Reading</h2><ol><li><a href="https://www.khanacademy.org/math/differential-calculus/dc-chain">Chain Rule</a></li><li><a href="http://neuralnetworksanddeeplearning.com/chap2.html">Backpropagation</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;div align=center&gt;

&lt;p&gt;&lt;img src=&quot;https://jpccc.github.io/resource/pytorch/full_graph.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;PyTorch is one of the foremost python deep learning libraries out there. It’s the go to choice for deep learning research, and as each days passes by, more and more companies and research labs are adopting this library.&lt;/p&gt;
&lt;p&gt;In this series of tutorials, we will be introducing you to PyTorch, and how to make the best use of the libraries as well the ecosystem of tools built around it. We’ll first cover the basic building blocks, and then move onto how you can quickly prototype custom architectures. We will finally conclude with a couple of posts on how to scale your code, and how to debug your code if things go awry.&lt;/p&gt;
&lt;p&gt;This is Part 1 of our PyTorch 101 series.&lt;/p&gt;</summary>
    
    
    
    
    <category term="pytorch" scheme="https://jpccc.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Jensen_inequality</title>
    <link href="https://jpccc.github.io/2021/10/28/Jensen-inequality/"/>
    <id>https://jpccc.github.io/2021/10/28/Jensen-inequality/</id>
    <published>2021-10-28T08:10:44.000Z</published>
    <updated>2022-04-05T16:03:21.412Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Jensen不等式"><a href="#Jensen不等式" class="headerlink" title="Jensen不等式"></a>Jensen不等式</h1><p>Jensen不等式（Jensen’s inequality）是以丹麦数学家Johan Jensen命名的，它在概率论、机器学习、测度论、统计物理等领域都有相关应用。 在机器学习领域，我目前接触到的是用Jensen不等式用来证明KL散度大于等于0。</p><blockquote><p>Jensen不等式是和凸函数的定义是息息相关的，首先介绍什么是凸函数(convec function)。</p></blockquote><span id="more"></span><h2 id="凸函数"><a href="#凸函数" class="headerlink" title="凸函数"></a>凸函数</h2><p>凸函数是一个定义在某个向量空间的凸子集 C（区间）上的实值函数 f，如果在其定义域 C 上的任意两点 $x_1$,$x_2$ 有:</p><p>$$tf(x_1)+(1-t)f(x_2)\geq f(tx_1+(1-t)x_2)   \tag{1}$$</p><p>也就是说凸函数任意两点的割线位于函数图形上方， 这也是Jensen不等式的两点形式。</p><h2 id="Jensen不等式2134123"><a href="#Jensen不等式2134123" class="headerlink" title="Jensen不等式2134123"></a>Jensen不等式2134123</h2><p>若对于任意点集${x_i}$，若 $\lambda_i\geq 0$ 且 $\underset {i}\sum\lambda_i=1$ ，使用数学归纳法，可以证明凸函数 f (x) 满足：<br>$$f(\sum_{i=1}^M\lambda_ix_i)\leq \sum_{i=1}^M\lambda_if(x_i) \tag{2} $$</p><p>公式(2)被称为 Jensen 不等式，它是公式(1)的泛化形式。</p><blockquote><p><strong>证明如下：</strong></p><blockquote><p>当i=1或2时，由凸函数的定义成立<br>    假设当i=M时，公式(2)成立<br>    现在证明则i=M+1时，Jensen不等式也成立：<br><a href="https://blog.csdn.net/AndrewHYang/article/details/86477162">证明</a></p></blockquote></blockquote><p>在概率论中，如果把$\lambda_i$看成取值为$x_i$的离散变量x的概率分布，那么公式(2)就可以写成：</p><p>$$f(E(X))\leq E[f(x)]$$</p><p>其中, E[·] 表示期望。</p><p>对于连续变量，Jensen不等式给出了积分的凸函数值和凸函数的积分值间的关系。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Jensen不等式&quot;&gt;&lt;a href=&quot;#Jensen不等式&quot; class=&quot;headerlink&quot; title=&quot;Jensen不等式&quot;&gt;&lt;/a&gt;Jensen不等式&lt;/h1&gt;&lt;p&gt;Jensen不等式（Jensen’s inequality）是以丹麦数学家Johan Jensen命名的，它在概率论、机器学习、测度论、统计物理等领域都有相关应用。 在机器学习领域，我目前接触到的是用Jensen不等式用来证明KL散度大于等于0。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Jensen不等式是和凸函数的定义是息息相关的，首先介绍什么是凸函数(convec function)。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    
    <category term="math" scheme="https://jpccc.github.io/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>统计学习数学基础-1</title>
    <link href="https://jpccc.github.io/2021/10/28/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0part1/"/>
    <id>https://jpccc.github.io/2021/10/28/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0part1/</id>
    <published>2021-10-28T04:44:13.000Z</published>
    <updated>2022-04-06T01:40:36.657Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>对概率的诠释有两大学派，一种是频率派另一种是贝叶斯派。后面我们对观测集采用下面记号：<br>$$<br>X_{N\times p}=(x_{1},x_{2},\cdots,x_{N})^{T},x_{i}=(x_{i1},x_{i2},\cdots,x_{ip})^{T}<br>$$<br>这个记号表示有 $N$ 个样本，每个样本都是 $p$ 维向量。其中每个观测都是由 $p(x|\theta)$ 生成的。</p><span id="more"></span><h2 id="频率派的观点"><a href="#频率派的观点" class="headerlink" title="频率派的观点"></a>频率派的观点</h2><p>$p(x|\theta)$中的 $\theta$ 是一个未知常量，数据是随机变量。对于 $N$ 个观测来说观测集的概率为$p(X|\theta)\mathop{=}\limits _{iid}\prod\limits <em>{i=1}^{N}p(x</em>{i}|\theta))$.为了求 $\theta$ 的大小，我们采用最大对数似然MLE的方法：</p><p>$$<br>\theta_{MLE}=\mathop{argmax}\limits_{\theta}\log p(X|\theta)\mathop{=}\limits_{iid}\mathop{argmax}\limits_{\theta}\sum\limits <em>{i=1}^{N}\log p(x</em>{i}|\theta)<br>$$</p><p>$x_i$服从独立同分布的条件，所以$P(X|\theta)=\prod_{i=0}^n p(x_i|\theta)$,为了<b>方便计算</b>，在前面加上log,将连乘变成连加。</p><h2 id="贝叶斯派的观点"><a href="#贝叶斯派的观点" class="headerlink" title="贝叶斯派的观点"></a>贝叶斯派的观点</h2><p>贝叶斯派认为 $p(x|\theta)$ 中的 $\theta$ 不是一个常量。这个 $\theta$ 满足一个预设的<strong>先验</strong>的分布 $\theta\sim p(\theta)$ （比喻可以假设为高斯分布）,并借助贝叶斯定理，用似然将参数的先验和后验连接起来，于是根据贝叶斯定理依赖观测集参数的后验可以写成：</p><p>$$<br>p(\theta|X)=\frac{p(X|\theta)\cdot p(\theta)}{p(X)}=\frac{p(X|\theta)\cdot p(\theta)}{\int\limits _{\theta}p(X|\theta)\cdot p(\theta)d\theta}<br>$$<br>为了求 $\theta$ 的值，我们要最大化这个参数后验MAP：</p><blockquote><p>最大化后验的解释：在给定观测X的情况下，找出$\theta$的概率最大时所对应的值，即这时候的$\theta$更可能为我们要找的参数。</p></blockquote><p>$$<br>\theta_{MAP}=\mathop{argmax}\limits _{\theta}p(\theta|X)=\mathop{argmax}\limits _{\theta}p(X|\theta)\cdot p(\theta)<br>$$<br>其中第二个等号是由于分母和 $\theta$ 没有关系。求解这个 $\theta$ 值后计算$\frac{p(X|\theta)\cdot p(\theta)}{\int\limits _{\theta}p(X|\theta)\cdot p(\theta)d\theta}$ ，就得到了参数的后验概率。其中 $p(X|\theta)$ 叫似然，是我们的模型分布。得到了参数的后验分布后，我们可以将这个分布用于<strong>贝叶斯预测</strong>：</p><p>$$<br>p(x_{new}|X)=\int\limits <em>{\theta}p(x</em>{new},\theta|X)=\int\limits <em>{\theta}p(x</em>{new}|\theta)\cdot p(\theta|X)d\theta<br>$$</p><p> 其中积分中的被乘数是模型，乘数是后验分布。</p><blockquote><p>$p(x|\theta)$是似然，$p(\theta|x)$是后验。注意第一个等式中，$\theta$不论放分子还是分母都可以这样积分掉。</p></blockquote><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>频率派和贝叶斯派分别给出了一系列的机器学习算法。频率派的观点导出了一系列的统计机器学习算法而贝叶斯派导出了概率图理论。在应用频率派的 MLE 方法时最<strong>优化</strong>理论占有重要地位。而贝叶斯派的算法无论是后验概率的建模还是应用这个后验进行推断时<strong>积分</strong>占有重要地位。因此采样积分方法如 MCMC 有很多应用。(即频率派需要设计损失函数并进行优化，而贝叶斯派需要积分后验中分母。)</p><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><blockquote><p>频率派认为参数是客观存在不会改变的，虽然未知，但却是固定值（故可用最优化方法去找那一个唯一的值）；贝叶斯派则认为参数是随机值，因为不可能做完整的实验去确定，因此参数也可以有分布。往小处说，频率派最常关心的是似然函数，他们认为直接用样本去计算出的概率就是真实的，而贝叶斯派最常关心的是后验分布，他们认为样本只是用来修正经验观点。</p></blockquote><blockquote><p>贝叶斯派因为所有的参数都是随机变量，都有分布，因此可以使用一些基于采样的方法 （如MCMC）使得我们更容易构建复杂模型。频率派的优点则是没有假设一个先验分布，因此更加客观，也更加无偏，在一些保守的领域（比如制药业、法律）比贝叶斯方法更受到信任。</p></blockquote><h1 id="MathBasics"><a href="#MathBasics" class="headerlink" title="MathBasics"></a>MathBasics</h1><h2 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a>高斯分布</h2><h3 id="一维情况-MLE"><a href="#一维情况-MLE" class="headerlink" title="一维情况 MLE"></a>一维情况 MLE</h3><p>高斯分布在机器学习中占有举足轻重的作用。在 MLE 方法中：</p><p>$$<br>\theta=(\mu,\Sigma)=(\mu,\sigma^{2}),\theta_{MLE}=\mathop{argmax}\limits _{\theta}\log p(X|\theta)\mathop{=}\limits _{iid}\mathop{argmax}\limits _{\theta}\sum\limits <em>{i=1}^{N}\log p(x</em>{i}|\theta)<br>$$</p><p>一般地，高斯分布的概率密度函数PDF写为：</p><p>$$<br>p(x|\mu,\Sigma)=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)}<br>$$</p><p>带入 MLE 中我们考虑一维的情况</p><p>$$<br>\log p(X|\theta)=\sum\limits <em>{i=1}^{N}\log p(x</em>{i}|\theta)=\sum\limits <em>{i=1}^{N}\log\frac{1}{\sqrt{2\pi}\sigma}\exp(-(x</em>{i}-\mu)^{2}/2\sigma^{2})<br>$$</p><p>首先对 $\mu$ 的极值可以得到 ：<br>$$<br>\mu_{MLE}=\mathop{argmax}\limits _{\mu}\log p(X|\theta)=\mathop{argmax}\limits _{\mu}\sum\limits <em>{i=1}^{N}(x</em>{i}-\mu)^{2}<br>$$</p><p> 于是：<br>$$<br>\frac{\partial}{\partial\mu}\sum\limits <em>{i=1}^{N}(x</em>{i}-\mu)^{2}=0\longrightarrow\mu_{MLE}=\frac{1}{N}\sum\limits <em>{i=1}^{N}x</em>{i}<br>$$</p><p>其次对 $\theta$ 中的另一个参数 $\sigma$ ，有：</p><p>$$<br>\begin{align}<br>\sigma_{MLE}=\mathop{argmax}\limits _{\sigma}\log p(X|\theta)&amp;=\mathop{argmax}\limits _{\sigma}\sum\limits <em>{i=1}^{N}[-\log\sigma-\frac{1}{2\sigma^{2}}(x</em>{i}-\mu)^{2}]\<br>&amp;=\mathop{argmin}\limits _{\sigma}\sum\limits <em>{i=1}^{N}[\log\sigma+\frac{1}{2\sigma^{2}}(x</em>{i}-\mu)^{2}]<br>\end{align}<br>$$</p><p>于是：</p><p>$$<br>\frac{\partial}{\partial\sigma}\sum\limits <em>{i=1}^{N}[\log\sigma+\frac{1}{2\sigma^{2}}(x</em>{i}-\mu)^{2}]=0\longrightarrow\sigma_{MLE}^{2}=\frac{1}{N}\sum\limits <em>{i=1}^{N}(x</em>{i}-\mu)^{2}<br>$$</p><p>值得注意的是，上面的推导中，首先对 $\mu$ 求 MLE， 然后利用这个结果求 $\sigma_{MLE}$ ，因此可以预期的是对数据集求期望时 $\mathbb{E}<em>{\mathcal{D}}[\mu</em>{MLE}]$ 是无偏差的：</p><p>$$<br>\mathbb{E}<em>{\mathcal{D}}[\mu</em>{MLE}]=\mathbb{E}<em>{\mathcal{D}}[\frac{1}{N}\sum\limits <em>{i=1}^{N}x</em>{i}]=\frac{1}{N}\sum\limits <em>{i=1}^{N}\mathbb{E}</em>{\mathcal{D}}[x</em>{i}]=\mu<br>$$</p><p>但是当对 $\sigma_{MLE}$ 求 期望的时候由于使用了单个数据集的 $\mu_{MLE}$，因此对所有数据集求期望的时候我们会发现 $\sigma_{MLE}$ 是 有偏的：</p><p>$$<br>\begin{align}<br>\mathbb{E}<em>{\mathcal{D}}[\sigma</em>{MLE}^{2}]&amp;=\mathbb{E}<em>{\mathcal{D}}[\frac{1}{N}\sum\limits <em>{i=1}^{N}(x</em>{i}-\mu</em>{MLE})^{2}]=\mathbb{E}<em>{\mathcal{D}}[\frac{1}{N}\sum\limits <em>{i=1}^{N}(x</em>{i}^{2}-2x</em>{i}\mu_{MLE}+\mu_{MLE}^{2})<br>\&amp;=\mathbb{E}<em>{\mathcal{D}}[\frac{1}{N}\sum\limits <em>{i=1}^{N}x</em>{i}^{2}-\mu</em>{MLE}^{2}]=\mathbb{E}<em>{\mathcal{D}}[\frac{1}{N}\sum\limits <em>{i=1}^{N}x</em>{i}^{2}-\mu^{2}+\mu^{2}-\mu</em>{MLE}^{2}]\<br>&amp;= \mathbb{E}<em>{\mathcal{D}}[\frac{1}{N}\sum\limits <em>{i=1}^{N}x</em>{i}^{2}-\mu^{2}]-\mathbb{E}</em>{\mathcal{D}}[\mu_{MLE}^{2}-\mu^{2}]=\sigma^{2}-(\mathbb{E}<em>{\mathcal{D}}[\mu</em>{MLE}^{2}]-\mu^{2})\&amp;=\sigma^{2}-(\mathbb{E}<em>{\mathcal{D}}[\mu</em>{MLE}^{2}]-\mathbb{E}<em>{\mathcal{D}}^{2}[\mu</em>{MLE}])=\sigma^{2}-Var[\mu_{MLE}]\&amp;=\sigma^{2}-Var[\frac{1}{N}\sum\limits <em>{i=1}^{N}x</em>{i}]=\sigma^{2}-\frac{1}{N^{2}}\sum\limits <em>{i=1}^{N}Var[x</em>{i}]=\frac{N-1}{N}\sigma^{2}<br>\end{align}<br>$$</p><p>所以：${\sigma}^{2}$的无偏估计应该为：</p><p>$$<br>\hat{\sigma}^{2}=\frac{1}{N-1}\sum\limits <em>{i=1}^{N}(x</em>{i}-\mu)^{2}<br>$$</p><h3 id="多维情况"><a href="#多维情况" class="headerlink" title="多维情况"></a>多维情况</h3><p>多维高斯分布表达式为：</p><p>$$<br>p(x|\mu,\Sigma)=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)}<br>$$</p><p>其中 $x,\mu\in\mathbb{R}^{p},\Sigma\in\mathbb{R}^{p\times p}$ ，$\Sigma$ 为协方差矩阵，一般而言也是半正定矩阵。这里我们只考虑正定矩阵。首先我们处理指数上的数字，指数上的数字可以记为 $x$ 和 $\mu$ 之间的马氏距离。对于对称的协方差矩阵可进行特征值分解，$\Sigma=U\Lambda U^{T}=(u_{1},u_{2},\cdots,u_{p})diag(\lambda_{i})(u_{1},u_{2},\cdots,u_{p})^{T}=\sum\limits <em>{i=1}^{p}u</em>{i}\lambda_{i}u_{i}^{T}$ ，于是：</p><p>$$<br>\Sigma^{-1}=\sum\limits <em>{i=1}^{p}u</em>{i}\frac{1}{\lambda_{i}}u_{i}^{T}<br>$$</p><p>$$<br>\Delta=(x-\mu)^{T}\Sigma^{-1}(x-\mu)=\sum\limits <em>{i=1}^{p}(x-\mu)^{T}u</em>{i}\frac{1}{\lambda_{i}}u_{i}^{T}(x-\mu)=\sum\limits <em>{i=1}^{p}\frac{y</em>{i}^{2}}{\lambda_{i}}<br>$$</p><p>我们注意到 $y_{i}$ 是 $x-\mu$ 在特征向量 $u_{i}$ 上的投影长度，因此上式子就是 $\Delta$ 取不同值时的同心椭圆。</p><p>下面我们看多维高斯模型在实际应用时的两个问题</p><ol><li><p> 参数 $\Sigma,\mu$ 的自由度为 $O(p^{2})$ 对于维度很高的数据其自由度太高。解决方案：高自由度的来源是 $\Sigma$ 有 $\frac{p(p+1)}{2}$ 个自由参数，可以假设其是对角矩阵，甚至在各向同性假设中假设其对角线上的元素都相同。前一种的算法有 Factor Analysis，后一种有概率 PCA(p-PCA) 。</p></li><li><p> 第二个问题是单个高斯分布是单峰的，对有多个峰的数据分布不能得到好的结果。解决方案：高斯混合GMM 模型。</p></li></ol><p>下面对多维高斯分布的常用定理进行介绍。</p><p>我们记 $x=(x_1, x_2,\cdots,x_p)^T=(x_{a,m\times 1}, x_{b,n\times1})^T,\mu=(\mu_{a,m\times1}, \mu_{b,n\times1}),\Sigma=\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}$，已知 $x\sim\mathcal{N}(\mu,\Sigma)$。</p><p>首先是一个高斯分布的定理：</p><blockquote><p>  定理：已知 $x\sim\mathcal{N}(\mu,\Sigma), y\sim Ax+b$，那么 $y\sim\mathcal{N}(A\mu+b, A\Sigma A^T)$。</p><p>  证明：$\mathbb{E}[y]=\mathbb{E}[Ax+b]=A\mathbb{E}[x]+b=A\mu+b$，$Var[y]=Var[Ax+b]=Var[Ax]=A\cdot Var[x]\cdot A^T$。</p></blockquote><p>下面利用这个定理得到 $p(x_a),p(x_b),p(x_a|x_b),p(x_b|x_a)$ 这四个量。</p><ol><li><p>$x_a=\begin{pmatrix}\mathbb{I}<em>{m\times m}&amp;\mathbb{O}</em>{m\times n})\end{pmatrix}\begin{pmatrix}x_a\x_b\end{pmatrix}$，代入定理中得到：</p><p> $$<br> \mathbb{E}[x_a]=\begin{pmatrix}\mathbb{I}&amp;\mathbb{O}\end{pmatrix}\begin{pmatrix}\mu_a\\mu_b\end{pmatrix}=\mu_a\<br> Var[x_a]=\begin{pmatrix}\mathbb{I}&amp;\mathbb{O}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\begin{pmatrix}\mathbb{I}\\mathbb{O}\end{pmatrix}=\Sigma_{aa}<br> $$</p><p> 所以 $x_a\sim\mathcal{N}(\mu_a,\Sigma_{aa})$。</p></li><li><p> 同样的，$x_b\sim\mathcal{N}(\mu_b,\Sigma_{bb})$。</p></li><li><p>对于两个条件概率，我们引入三个量：</p><p> $$<br> x_{b\cdot a}=x_b-\Sigma_{ba}\Sigma_{aa}^{-1}x_a\<br> \mu_{b\cdot a}=\mu_b-\Sigma_{ba}\Sigma_{aa}^{-1}\mu_a\<br> \Sigma_{bb\cdot a}=\Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}<br> $$</p><p> 特别的，最后一个式子叫做 $\Sigma_{bb}$ 的 Schur Complementary。可以看到：</p><p> $$<br> x_{b\cdot a}=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbb{I}_{n\times n}\end{pmatrix}\begin{pmatrix}x_a\x_b\end{pmatrix}<br> $$</p><p> 所以：</p><p> $$<br> \mathbb{E}[x_{b\cdot a}]=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbb{I}<em>{n\times n}\end{pmatrix}\begin{pmatrix}\mu_a\\mu_b\end{pmatrix}=\mu</em>{b\cdot a}\<br> Var[x_{b\cdot a}]=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbb{I}<em>{n\times n}\end{pmatrix}\begin{pmatrix}\Sigma</em>{aa}&amp;\Sigma_{ab}\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\begin{pmatrix}-\Sigma_{aa}^{-1}\Sigma_{ba}^T\\mathbb{I}<em>{n\times n}\end{pmatrix}=\Sigma</em>{bb\cdot a}<br> $$</p><p> 利用这三个量可以得到 $x_b=x_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a$。因此：</p><p> $$<br> \mathbb{E}[x_b|x_a]=\mu_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a<br> $$</p><p> $$<br> Var[x_b|x_a]=\Sigma_{bb\cdot a}<br> $$</p><p> 这里同样用到了定理。</p></li><li><p>同样：</p><p> $$<br> x_{a\cdot b}=x_a-\Sigma_{ab}\Sigma_{bb}^{-1}x_b\<br> \mu_{a\cdot b}=\mu_a-\Sigma_{ab}\Sigma_{bb}^{-1}\mu_b\<br> \Sigma_{aa\cdot b}=\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba}<br> $$</p><p> 所以：</p><p> $$<br> \mathbb{E}[x_a|x_b]=\mu_{a\cdot b}+\Sigma_{ab}\Sigma_{bb}^{-1}x_b<br> $$</p><p> $$<br> Var[x_a|x_b]=\Sigma_{aa\cdot b}<br> $$</p></li></ol><p>下面利用上边四个量，求解线性模型：</p><blockquote><p>  已知：$p(x)=\mathcal{N}(\mu,\Lambda^{-1}),p(y|x)=\mathcal{N}(Ax+b,L^{-1})$，求解：$p(y),p(x|y)$。</p><p>  解：令 $y=Ax+b+\epsilon,\epsilon\sim\mathcal{N}(0,L^{-1})$，所以 $\mathbb{E}[y]=\mathbb{E}[Ax+b+\epsilon]=A\mu+b$，$Var[y]=A \Lambda^{-1}A^T+L^{-1}$，因此：<br>  $$<br>  p(y)=\mathcal{N}(A\mu+b,L^{-1}+A\Lambda^{-1}A^T)<br>  $$<br>  引入 $z=\begin{pmatrix}x\y\end{pmatrix}$，我们可以得到 $Cov[x,y]=\mathbb{E}[(x-\mathbb{E}[x])(y-\mathbb{E}[y])^T]$。对于这个协方差可以直接计算：<br>  $$<br>  \begin{align}<br>  Cov(x,y)&amp;=\mathbb{E}[(x-\mu)(Ax-A\mu+\epsilon)^T]=\mathbb{E}[(x-\mu)(x-\mu)^TA^T]=Var[x]A^T=\Lambda^{-1}A^T<br>  \end{align}<br>  $$<br>  注意到协方差矩阵的对称性，所以 $p(z)=\mathcal{N}\begin{pmatrix}\mu\A\mu+b\end{pmatrix},\begin{pmatrix}\Lambda^{-1}&amp;\Lambda^{-1}A^T\A\Lambda^{-1}&amp;L^{-1}+A\Lambda^{-1}A^T\end{pmatrix})$。根据之前的公式，我们可以得到：<br>  $$<br>  \mathbb{E}[x|y]=\mu+\Lambda^{-1}A^T(L^{-1}+A\Lambda^{-1}A^T)^{-1}(y-A\mu-b)<br>  $$</p><p>  $$<br>  Var[x|y]=\Lambda^{-1}-\Lambda^{-1}A^T(L^{-1}+A\Lambda^{-1}A^T)^{-1}A\Lambda^{-1}<br>  $$</p></blockquote>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;p&gt;对概率的诠释有两大学派，一种是频率派另一种是贝叶斯派。后面我们对观测集采用下面记号：&lt;br&gt;$$&lt;br&gt;X_{N\times p}=(x_{1},x_{2},\cdots,x_{N})^{T},x_{i}=(x_{i1},x_{i2},\cdots,x_{ip})^{T}&lt;br&gt;$$&lt;br&gt;这个记号表示有 $N$ 个样本，每个样本都是 $p$ 维向量。其中每个观测都是由 $p(x|\theta)$ 生成的。&lt;/p&gt;</summary>
    
    
    
    
    <category term="统计学习" scheme="https://jpccc.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>pytorch</title>
    <link href="https://jpccc.github.io/2021/10/25/pytorch/"/>
    <id>https://jpccc.github.io/2021/10/25/pytorch/</id>
    <published>2021-10-25T14:33:05.000Z</published>
    <updated>2021-10-25T14:33:05.364Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>DroupOut</title>
    <link href="https://jpccc.github.io/2021/10/25/DroupOut/"/>
    <id>https://jpccc.github.io/2021/10/25/DroupOut/</id>
    <published>2021-10-25T12:45:13.000Z</published>
    <updated>2021-10-25T12:45:13.205Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>BN算法</title>
    <link href="https://jpccc.github.io/2021/10/25/BN%E7%AE%97%E6%B3%95/"/>
    <id>https://jpccc.github.io/2021/10/25/BN%E7%AE%97%E6%B3%95/</id>
    <published>2021-10-25T12:20:14.000Z</published>
    <updated>2021-10-25T14:32:34.264Z</updated>
    
    <content type="html"><![CDATA[<h1 id="BN算法概要"><a href="#BN算法概要" class="headerlink" title="BN算法概要"></a>BN算法概要</h1><p>Batch Normalization是2015年一篇论文中提出的数据归一化方法，往往用在深度神经网络中激活层之前。其作用可以加快模型训练时的收敛速度，使得模型训练过程更加稳定，避免梯度爆炸或者梯度消失。并且起到一定的正则化作用，几乎代替了Dropout。</p><span id="more"></span><h1 id="BN算法产生的背景"><a href="#BN算法产生的背景" class="headerlink" title="BN算法产生的背景"></a>BN算法产生的背景</h1><img src="https://jpccc.github.io/resource/deepLearning/001.png">&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;<img src="https://jpccc.github.io/resource/deepLearning/001.png"><blockquote><p>首先对第一张图进行分析。</p><blockquote><p>由于我们通常使用采用零均值化对网络进行参数初始化，我们初始的拟合直线也就是红色部分。另外的一条绿色直线，是我们的目标直线。从图能够直观看出，我们应该需要多次迭代才能得到我们的需要的目标直线。</p></blockquote></blockquote><blockquote><p>我们再看第二张图</p><blockquote><p>假设我们还是和第一张图有相同的分布，只是我们做了减均值，让数据均值为零。能够直观的发现可能只进行简单的微调就能够实现拟合（理想）。大大提高了我们的训练速度。因此，在训练开始前，对数据进行零均值是一个必要的操作。</p></blockquote></blockquote><blockquote><p>但是，随着网络层次加深参数对分布的影响不定(什么意思?)，导致网络每层间以及不同迭代轮次的相同层的输入分布发生改变，导致网络需要重新适应新的分布，迫使我们降低学习率降低影响。在这个背景下BN算法开始出现。       有些人首先提出在每层增加PCA白化(先对数据进行去相关然后再进行归一化)，这样基本满足了数据的0均值、单位方差、弱相关性。但是这样是不可取的，因为在白化过程中会计算协方差矩阵、求逆等操作，计算量会很大，另外，在反向传播时，白化的操作不一定可微。因此，在此背景下BN算法开始出现。</p></blockquote><h1 id="BN算法的实现和优点"><a href="#BN算法的实现和优点" class="headerlink" title="BN算法的实现和优点"></a>BN算法的实现和优点</h1><p>上面提到了PCA白化优点，能够去相关和数据均值，标准值归一化等优点。但是当数据量比较大的情况下去相关的话需要大量的计算，因此有些人提出了只对数据进行均值和标准差归一化。叫做近似白化预处理。</p><p>$$\hat{x}^k=\frac{X^k-E(X^k)}{\sqrt{Var[(x^k)}]}$$</p><p>由于训练过程采用了batch随机梯度下降，因此$E(X^k)$指的是一批训练数据时，各神经元输入值的平均值；$\sqrt{Var[(x^k)}]$指的是一批训练数据时各神经元输入值的标准差。</p><p>但是，这些应用到深度学习网络还远远不够，因为可能由于这种的强制转化导致数据的分布发生破坏。因此需要对公式的鲁棒性进行优化，就有人提出了变换重构的概念。就是在基础公式的基础之上加上了两个参数γ、β。这样在训练过程中就可以学习这两个参数，采用适合自己网络的BN公式。公式如下：<br>$$y^k=\gamma^k\hat x^k+\beta^k$$<br>每一个神经元都会有一对这样的参数γ、β。这样其实当<br>$$\beta^k=E[x^k],\gamma^k=\sqrt{var[x^k]}$$<br>时，是可以恢复出原始的某一层所学到的特征的。引入可学习重构参数γ、β，让网络可以学习恢复出原始网络所要学习的特征分布。</p><p>总结上面我们会得到BN的向前传导公式：</p><center><p>$\mu_\beta\leftarrow\frac{1}{m}\sum_{i=1}^nx_i$ //mnni batch mean<br>$\delta_\beta^2\leftarrow\frac{1}{m}\sum_{i=1}^n(x_i-\mu_\beta)^2$//mnni batch variance<br>$\hat{x}\leftarrow\frac{x_i-\mu_\beta}{\sqrt{\delta_\beta^2+\epsilon}}$//normalize<br>$y_i\leftarrow\gamma\hat{x_i}+\beta\equiv BN_{\gamma,\beta}(x_i)$ //scale and shift</p></center>2. BN算法在网络中的作用<p>   BN算法像卷积层，池化层、激活层一样也输入一层。BN层添加在激活函数前，对输入激活函数的输入进行归一化。这样解决了输入数据发生偏移和增大的影响。</p><p>优点：</p><p>1、加快训练速度，能够增大学习率，即使小的学习率也能够有快速的学习速率;<br>2、不用理会拟合中的droupout、L2 正则化项的参数选择，采用BN算法可以省去这两项或者只需要小的L2正则化约束。原因，BN算法后，参数进行了归一化，原本经过激活函数没有太大影响的神经元，分布变得明显，经过一个激活函数以后，神经元会自动削弱或者去除一些神经元，就不用再对其进行dropout。另外就是L2正则化，由于每次训练都进行了归一化，就很少发生由于数据分布不同导致的参数变动过大，带来的参数不断增大。<br>3、 可以吧训练数据集打乱，防止训练发生偏移。</p><p>使用： 在卷积中，会出现每层卷积层中有（L）多个特征图。AxAxL特征矩阵。我们只需要以每个特征图为单元求取一对γ、β。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><div id="refer-anchor-1"></div><ul><li>[1] <a href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;BN算法概要&quot;&gt;&lt;a href=&quot;#BN算法概要&quot; class=&quot;headerlink&quot; title=&quot;BN算法概要&quot;&gt;&lt;/a&gt;BN算法概要&lt;/h1&gt;&lt;p&gt;Batch Normalization是2015年一篇论文中提出的数据归一化方法，往往用在深度神经网络中激活层之前。其作用可以加快模型训练时的收敛速度，使得模型训练过程更加稳定，避免梯度爆炸或者梯度消失。并且起到一定的正则化作用，几乎代替了Dropout。&lt;/p&gt;</summary>
    
    
    
    
    <category term="deepLearning" scheme="https://jpccc.github.io/tags/deepLearning/"/>
    
  </entry>
  
  <entry>
    <title>互信息</title>
    <link href="https://jpccc.github.io/2021/10/25/%E4%BA%92%E4%BF%A1%E6%81%AF/"/>
    <id>https://jpccc.github.io/2021/10/25/%E4%BA%92%E4%BF%A1%E6%81%AF/</id>
    <published>2021-10-25T04:44:13.000Z</published>
    <updated>2021-10-26T07:24:49.438Z</updated>
    
    <content type="html"><![CDATA[<h1 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h1><p>机器学习中很多地方都要根据目前的信息做出决策，信息熵主要是反应信息的不确定性；它的一个很重要的作用，就是做决策时提供一定的判断依据，比如决策树根据熵来往下设置分枝(branch)。</p><ol><li>计算方法 $$H(X)=-\sum_{i=1}^nP(x_i)logP(x_i)$$<br> 其中$P(x_i)$代表随机事件X为$x_i$的概率。</li></ol><span id="more"></span><ol start="2"><li>信息量</li></ol><blockquote><p>信息量是对信息的度量，就跟时间的度量是秒一样；我们考虑一个离散的随机变量x，当我们观察到的这个变量的一个具体值的时候，我们接收到了多少信息呢？</p><blockquote><p>多少信息用信息量来衡量，我们接受到的信息量信息的大小跟随机事件的概率分布有关，越小概率的事情发生了产生的信息量越大，如湖南产生的地震了；越大概率的事情发生了产生的信息量越小，如太阳从东边升起来了（肯定发生嘛，没什么信息量）。这很好理解！ 所以描述信息量的函数应该是一个与随机变量的发生概率成负相关的函数，且不能为负数。</p></blockquote></blockquote><blockquote><p>例子:</p><blockquote><p>如果我们有俩个不相关的事件x和y，那么我们观察到的俩个事件同时发生时获得的信息应该等于观察到的事件各自发生时获得的信息之和，即：h(x,y) = h(x) + h(y)。 由于x，y是俩个不相关的事件，那么满足p(x,y) = p(x)*p(y)。<br>根据上面推导，我们很容易看出h(x)一定与p(x)的对数有关（因为只有对数形式的真数相乘之后，能够对应对数的相加形式，可以试试）。因此我们有信息量公式如下：<br>$$h(x)=-log_2p(x)$$ </p></blockquote></blockquote><blockquote><p>下面解决两个疑问:</p><blockquote><p>(1). 为什么有一个负号?<br>&nbsp;&nbsp;&nbsp;&nbsp; 信息量取概率的负对数，其实是因为信息量的定义是概率的倒数的对数。而用概率的倒数，是为了使概率越大，信息量越小，同时因为概率的倒数大于1，其对数自然大于0了。<br>(2). 为什么底数为2?<br>&nbsp;&nbsp;&nbsp;&nbsp;这是因为，我们只需要信息量满足低概率事件x对应于高的信息量，那么对数的选择是任意的，我们只是遵循信息论的普遍传统，使用2作为对数的底！</p></blockquote></blockquote><ol start="3"><li>信息熵</li></ol><blockquote><p>信息量度量的是一个具体事件发生了所带来的信息，而熵则是在结果出来之前对可能产生的信息量的期望——考虑该随机变量的所有可能取值，即所有可能发生事件所带来的信息量的期望。即<br>                     $$H(X)=-\sum_{i=1}^np(x_i)logp(x_i)$$</p><blockquote><p>信息熵还可以作为一个系统复杂程度的度量，如果系统越复杂，出现不同情况的种类越多，那么他的信息熵是比较大的。如果一个系统越简单，出现情况种类很少（极端情况为1种情况，那么对应概率为1，那么对应的信息熵为0），此时的信息熵较小。</p></blockquote></blockquote><h1 id="互信息"><a href="#互信息" class="headerlink" title="互信息"></a>互信息</h1><ol><li><p>定义</p><blockquote><p>互信息(Mutual Information)是衡量随机变量之间相互依赖程度的度量。</p><blockquote><p>它的形象化解释是，假如明天下雨是个随机事件，假如今晚有晚霞同样是个随机事件，那么这两个随机事件互相依赖的程度是：<br>&nbsp;&nbsp;&nbsp;&nbsp;当<b>不知道</b>”今晚有晚霞“情况下，”明天下雨“带来的不确定性<br>&nbsp;&nbsp;&nbsp;&nbsp;<b>与我们已知</b>“今晚有晚霞“情况下，”明天下雨”带来的不确定性之差。</p></blockquote></blockquote></li><li><p>解释<br>假设存在一个随机变量$X$ ，和另外一个随机变量$Y$ ，那么它们的互信息是<br>$$I(X;Y)=H(X)-H(X|Y)$$</p><p>$H(X)$是$X$的信息熵,$H(X|Y)$是已知$Y$情况下，X带来的信息熵（条件熵）。</p><blockquote><p>直观理解是，我们知道存在两个随机事件X,Y，其中一个随机事件X 给我们带来了一些不确定性H(X)，我们想衡量Y,X 之间的关系。那么，如果X,Y 存在关联，当Y已知时，X给我们的不确定性会变化，这个变化值就是X的信息熵减去当已知 Y时，X的条件熵，就是互信息。</p></blockquote><p> 从概率角度，互信息是由随机变量 $X,Y$ 的联合概率分布 p(x,y) 和边缘概率分布 $p(x),p(y)$ 得出。</p><p> $$I(X;Y)=\sum_{y \in \cal Y}\sum_{x \in \cal X}p(x,y)log(\frac{p(x,y)}{p(x)p(y)})$$<br>  互信息和信息熵的关系是：</p> <p align="center">             <img src="https://jpccc.github.io/resource/Mutual_%20Information/001.jpg"> </p><p> 通常我们使用的最大化互信息条件，就是最大化两个随机事件的相关性。在数据集里，就是最大化两个数据集所拟合出的概率分布的相关性。当两个随机变量相同时,互信息最大，如下:<br>$$I(X;Y)=H(X)-H(X|X)=H(X)$$</p><blockquote><p>在机器学习中，理想情况下，当互信息最大，可以认为从数据集中拟合出来的随机变量的概率分布与真实分布相同。</p></blockquote><p>到这里，应该足够大家日常理解使用了，以下是性质，应用和变形，几乎都是数学。</p></li><li><p>The most common lower bound is InfoNCE [35] whose formula is given by:</p> <p align="center">             <img src="https://jpccc.github.io/resource/Mutual_%20Information/002.jpg"> </p></li></ol>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;信息熵&quot;&gt;&lt;a href=&quot;#信息熵&quot; class=&quot;headerlink&quot; title=&quot;信息熵&quot;&gt;&lt;/a&gt;信息熵&lt;/h1&gt;&lt;p&gt;机器学习中很多地方都要根据目前的信息做出决策，信息熵主要是反应信息的不确定性；它的一个很重要的作用，就是做决策时提供一定的判断依据，比如决策树根据熵来往下设置分枝(branch)。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;计算方法 $$H(X)=-\sum_{i=1}^nP(x_i)logP(x_i)$$&lt;br&gt; 其中$P(x_i)$代表随机事件X为$x_i$的概率。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    
    <category term="math" scheme="https://jpccc.github.io/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>algebra</title>
    <link href="https://jpccc.github.io/2021/10/22/algebra/"/>
    <id>https://jpccc.github.io/2021/10/22/algebra/</id>
    <published>2021-10-22T08:26:34.000Z</published>
    <updated>2021-11-18T05:23:04.062Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一-向量"><a href="#一-向量" class="headerlink" title="一. 向量"></a>一. 向量</h1><ol><li>向量对于不同的学科有不一样的定义，我们将从三个角度对向量进行解释。<ul><li>物理学<br> 物理中的向量由长度和方向决定，长度和方向不变的情况下随意移动，表示的都是同一个向量。</li><li>计算机<br> 计算机中的向量更多的是对数据的抽象，可以是根据面积和价格定义的一个房子特征$\begin{bmatrix}100m^2\\700000￥\end{bmatrix}$或是通过神经网络得到的图象的的一个向量。</li><li>数学<br> 数学中的向量可以是任意东西，只要保证两个向量的相加$\vec v + \vec w$以及数字和向量相乘$2\vec v$是有意义的即可。<span id="more"></span></li></ul></li><li>线性代数中的向量可以理解为一个空间中的箭头，这个箭头起点落在原点。如果空间中有许多的向量，可以用点表示一个向量，即向量头的坐标。</li></ol><h1 id="二-向量的基本运算"><a href="#二-向量的基本运算" class="headerlink" title="二. 向量的基本运算"></a>二. 向量的基本运算</h1><ol><li><p>向量的加法<br> 可以理解为在坐标系中两个向量的移动。</p> <p align="center">             <img src="https://jpccc.github.io/resource/algebra/001.png"> </p></li><li><p>向量的乘法</p> <p align="center">             <img src="https://jpccc.github.io/resource/algebra/002.png"> </p></li></ol><h1 id="三-线性组合、张成空间、基"><a href="#三-线性组合、张成空间、基" class="headerlink" title="三. 线性组合、张成空间、基"></a>三. 线性组合、张成空间、基</h1><blockquote><p> 线性组合</p></blockquote><p>两个数乘向量相加称为两个向量的线性组合$a\vec v+ b\vec w$。<br>两个不共线的向量通过不同的线性组合可以得到二维平面中的所有向量。<br>两个共线的向量通过线程组合只能得到一个直线的所有向量。<br>如果两个向量都是零向量那么它只能在原点。</p><blockquote><p>张成向量</p></blockquote><p>所有可以表示给定向量线性组合的向量的集合称为给定向量的张成空间（span）。<br>一般来说两个向量张成空间可以是直线、平面。<br>三个向量张成空间可以是平面、空间。<br>如果多个向量，并且可以移除其中一个而不减小张成空间，那么它们是线性相关的，也可以说一个向量可以表示为其他向量的线性组合$\vec u = a \vec v + b\vec w$。<br>如果所有的向量都给张成的空间增加了新的维度，它们就成为线性无关的$\vec u \neq a \vec v + b\vec w$。</p><blockquote><p>基</p></blockquote><p>向量空间的一组基是张成该空间的一个线性无关向量集。</p><h1 id="四-矩阵与线性变换"><a href="#四-矩阵与线性变换" class="headerlink" title="四. 矩阵与线性变换"></a>四. 矩阵与线性变换</h1><p>(向量的基默认为(0,1)(1,0)正交基。左乘向量可以看作是向量对基向量进行操作。同一向量乘以不同的基表示对不同的基做相同的运算。)</p><p>严格意义上来说，线性变换是将向量作为输入和输出的一类函数。<br>变化可以多种多样，线性变化将变化限制在一个特殊类型的变换上，可以简单的理解为网格线保持平行且等距分布。<br>线性变化满足一下两个性质：</p><ul><li>线性变化前后直线依旧是直线不能弯曲。</li><li>原点必须保持固定</li></ul><p align="center">    <img src="https://jpccc.github.io/resource/algebra/003.png"></p><p>可以使用<strong>基向量来描述线性变化：</strong><br>通过记录两个基向量$\hat{i}$,$\hat{j}$的变换，就可以得到其他变化后的向量。<br>已知向量$\vec v = \begin{bmatrix}-1\\2\end{bmatrix}$<br>变换之前的$\hat i$和$\hat j$：</p><p>$$<br>\hat{i} = \begin{bmatrix}<br>1 \<br>   0<br>  \end{bmatrix} \<br>  \hat{j} = \begin{bmatrix}<br>0 \<br>   1<br>  \end{bmatrix} \<br>\vec{v} = -1\hat{i} + 2 \hat{j} = \begin{bmatrix}<br>-1 \<br>   2<br>  \end{bmatrix}<br>$$</p><p>变换之后的$\hat i$和$\hat j$：</p><p>$$<br>\begin{aligned}<br>\hat{i} = \begin{bmatrix}<br>1 \<br>   -2<br>  \end{bmatrix} \<br>  \hat{j} = \begin{bmatrix}<br>3 \<br>   0<br>  \end{bmatrix} \<br>\vec{v} = -1\hat{i} + 2 \hat{j} &amp;= \begin{bmatrix}<br>5 \<br>   2<br>  \end{bmatrix} \<br>  &amp;= -1\begin{bmatrix}<br>1 \<br>   -2<br>  \end{bmatrix}  + 2 \begin{bmatrix}<br>3 \<br>  0<br>  \end{bmatrix} \<br>  &amp;= \begin{bmatrix}<br>1 &amp; 3 \<br>   -2 &amp; 0<br>  \end{bmatrix}  \begin{bmatrix}<br>-1 \<br>   2<br>  \end{bmatrix} \<br>  &amp;= \begin{bmatrix}<br>5 \<br>   2<br>  \end{bmatrix} \<br>\end{aligned}<br>$$<br><strong>将对基向量的变换记录下来，对其作用于其它向量，就可以得到其它向量在变换后的空间中的值</strong></p><p>我们可以将变换后的$\hat i$和$\hat j$写成矩阵的形式：$\begin{bmatrix}<br>1 &amp; 3 \<br>   -2 &amp; 0<br>  \end{bmatrix}  \begin{bmatrix}<br>-1 \<br>   2<br>  \end{bmatrix}$，通过矩阵的乘法得到变化后的向量。(左侧矩阵是基向量的线性变换矩阵)</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/004.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/005.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/006.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/007.png"></p><p>如果变化后的$\hat{i}$和$\hat{j}$是线性相关的，变化后向量的张量就是一维空间：</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/008.png"></p><h1 id="五-矩阵乘法与线性变换复合的联系"><a href="#五-矩阵乘法与线性变换复合的联系" class="headerlink" title="五. 矩阵乘法与线性变换复合的联系"></a>五. 矩阵乘法与线性变换复合的联系</h1><blockquote><p>线性变化的复合</p></blockquote><p>如何描述先旋转再剪切的操作呢？</p><p>一个通俗的方法是首先左乘旋转矩阵然后左乘剪切矩阵。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/009.png"></p>   <p>两个矩阵的乘积需要从右向左读，类似函数的复合。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/010.png"></p><p>这样两个矩阵的乘积就对应了一个复合的线性变换，最终得到对应变换后的$\hat{i}$和$\hat{j}$</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/011.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/012.png"></p>这一过程具有普适性：<p align="center">            <img src="https://jpccc.github.io/resource/algebra/013.png"></p><blockquote><p>矩阵乘法的顺序</p></blockquote><p align="center">            <img src="https://jpccc.github.io/resource/algebra/014.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/015.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/016.png"></p><blockquote><p>如何证明矩阵乘法的结合性？</p></blockquote><p>$(AB)C = A(BC)$<br>根据线性变化我们可以得出，矩阵的乘法都是以CBA的顺序变换得到，所以他们本质上相同，通过变化的形式解释比代数计算更加容易理解。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/017.png"></p><h1 id="六-三维空间的线性变化"><a href="#六-三维空间的线性变化" class="headerlink" title="六. 三维空间的线性变化"></a>六. 三维空间的线性变化</h1><p>三维的空间变化和二维的类似。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/018.png"></p><p>同样跟踪基向量的变换，能很好的解释变换后的向量，同样两个矩阵相乘的复合变换也是。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/019.png"></p><h1 id="七-行列式"><a href="#七-行列式" class="headerlink" title="七. 行列式"></a>七. 行列式</h1><blockquote><p>行列式的本质</p></blockquote><p>行列式的本质是计算线性变化对空间的缩放比例，具体一点就是，测量一个给定区域面积增大或减小的比例。<strong>注意，面积的变化比例是对原空间中的网格来说的。也可以说成是原基向量组成的区域面积的变化。</strong><br>单位面积的变换代表任意区域的面积变换比例。<br>值得注意的是：</p><ul><li>如果一个二维线性变换的行列式为0，说明其将说明起其将整个平面压缩成一条线甚至一个点上。</li><li>所以只要检测一个矩阵的行列式是否为0，我们就可以知道矩阵所代表的变换是否将空间压缩到更小的维度上。</li></ul><p align="center">            <img src="https://jpccc.github.io/resource/algebra/020.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/021.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/022.png"></p>行列式的值表示缩放比例。<p align="center">            <img src="https://jpccc.github.io/resource/algebra/023.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/024.png"></p><p>行列式为什么有负值呢？可以从两个角度考虑：</p><ul><li>一是变换将平面进行了反转。就好像将一张纸的正面通过变换将这张纸进行了翻面。</li><li>二是考虑的$\hat i$和$\hat j$的相对位置。如$\hat i$在$\hat j$的左边，通过变换将$\hat i$变换到了$\hat j$的右边，那么这个变换所对应矩阵的行列式值为负。<p align="center">          <img src="https://jpccc.github.io/resource/algebra/001.gif"></p></li></ul><p>三维空间的行列式类似，它的单位是一个单位1的立方体。</p><p>三位空间的线性变换，可以使用右手定则判断三维空间的定向。如果变换前后都可以通过右手定则得到，那么他的行列式就是正值，否则为负值.</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/025.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/026.png"></p><h1 id="八-逆矩阵、列空间、秩、零空间"><a href="#八-逆矩阵、列空间、秩、零空间" class="headerlink" title="八. 逆矩阵、列空间、秩、零空间"></a>八. 逆矩阵、列空间、秩、零空间</h1><blockquote><p>线性方程组</p></blockquote><p align="center">            <img src="https://jpccc.github.io/resource/algebra/027.png"></p><p>从几何的角度来思考，矩阵A表示一个线性变换，我们需要找到一个$\vec x$使得它在变换后和$\vec v$重合。</p><blockquote><p>逆矩阵</p></blockquote><p>矩阵的逆运算，记为$\vec A = \begin{bmatrix}3&amp;1 \0&amp;2\end{bmatrix}^{-1}$，对于线程方程$A \vec x = \vec v $来说，找到$A^{-1}$就得到解$\vec x = A^{-1} \vec v$。<br>$A^{-1}A=\begin{bmatrix}1&amp;0 \0&amp;1\end{bmatrix}$，什么都不做称为恒等变换。</p><blockquote><p>线性方程组的解</p></blockquote><p>对于方程组$A\vec x = \vec v$，线性变换A存在两种情况：</p><ul><li>$det(A) \neq0$：这时空间的维数并没有改变，有且只有一个向量经过线性变换后和$\vec v$重合。</li><li>$det(A) =0$：空间被压缩到更低的维度，这时不存在逆变换。因为不能将一个直线解压缩为一个平面(这会要求将一个单独的向量变换为一整条线的向量，函数多对一可以，一对多不行，即存在一个矩阵A将多个向量映射到一个点，但不可能存在一个矩阵A将一个点映射成一条线的向量)。但是即使不存在逆变换，解可能仍然存在，这时候目标$\vec v$必须刚好落在压缩后的空间上。(例如一个变换将空间压缩成了一条直线，而向量$\vec v$恰好在这条直线上。共线的情况下，则有无穷解，因为有无穷都被压缩到直线上的每一点)。</li></ul><blockquote><p>秩</p></blockquote><p>秩代表变换后空间的维度。<br>如果线性变化后将空间压缩成一条直线，那么称这个变化的秩为1；<br>如果线性变化后向量落在二维平面，那么称这个变化的秩为2。</p><blockquote><p>列空间</p></blockquote><p>所有可能的输出向量$A\vec v$构成的集合(A为基向量的集合，对其进行任意的组合(组合即乘以$\vec{v}$)，所得到的所有向量)，称为列空间，即所有列向量张成的空间。</p><ul><li>更精确的秩的定义就是列空间的维数。</li></ul><blockquote><p>零空间（Null space）</p></blockquote><p>所有的线性变化中，零向量一定包含在列空间中，因为线性变换原点保持不动。对于非满秩的情况来说，会有一系列的向量在变换后仍为零向量（二维空间压缩为一条直线，一条线上的向量都会落到原点。）</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/002.gif"></p><p>三维空间压缩为二维平面，一条线上的向量都会落到原点。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/003.gif"></p><p>三维空间压缩为一条直线，整个平面上的向量都会落到原点。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/004.gif"></p><p>当$A\vec x = \vec v$中的$\vec v$是一个零向量，即$A\vec x = \begin{bmatrix}0 \\0\end{bmatrix}$时，零空间就是它所有可能的解。</p><h1 id="非方阵、不同维度空间之间的线性变换"><a href="#非方阵、不同维度空间之间的线性变换" class="headerlink" title="非方阵、不同维度空间之间的线性变换"></a>非方阵、不同维度空间之间的线性变换</h1><p>不同维度的变换也是存在的。</p><p>一个$3\times2$的矩阵：$\begin{bmatrix}2&amp;0\\-1&amp;1\\-2&amp;1 \end{bmatrix}$它的几何意义是将一个二维空间映射到三维空间上，矩阵有两列表明输入空间有两个基向量，有三行表示每个向量在变换后用三个独立的坐标描述。(A是满秩的，因为其与输入空间x的维度相等)</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/028.png"></p><p>一个$2\times 3$的矩阵：$\begin{bmatrix}3&amp;1&amp;4\\1&amp;5&amp;9 \end{bmatrix}$则表示将一个三维空间映射到二维空间上。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/029.png"></p><p>一个$1\times 2$的矩阵：$\begin{bmatrix}1&amp;2 \end{bmatrix}$表示一个二维空间映射到一维空间。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/030.png"></p>(还可以从解方程的角度，通解+特解，来理解一维变二维。)# 点积与对偶性> 点积<p>对于两个维度相同的向量，他们的点积计算为：$\begin{bmatrix}1\\2\end{bmatrix}\cdot\begin{bmatrix} 3\\4\end{bmatrix}=1\cdot3+2\cdot4=11$。<br>点积的几何解释是将一个向量向一个向量投影，然后两个长度相乘，如果为负数则表示反向。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/031.png"></p><p>为什么点积和坐标相乘联系起来了？这和对偶性有关。</p><blockquote><p>对偶性</p></blockquote><p>对偶性的思想是：<strong>每当看到一个多维空间到数轴上的线性变换时</strong>，他都与空间中的唯一一个向量对应，也就是说使用线性变换和与这个向量点乘等价。这个向量也叫做线性变换的对偶向量。<br>当二维空间向一维空间映射时，如果在二维空间中等距分布的点在变换后还是等距分布的，那么这种变换就是线性的。</p><p>假设有一个线性变换A<br>$\begin{bmatrix}1&amp;-2\end{bmatrix}$<br>和一个向量<br>$\vec v=\begin{bmatrix} 4\\3 \end{bmatrix}$。</p><p>变换后的位置为$\begin{bmatrix}1&amp;-2\end{bmatrix}\begin{bmatrix}4\3\end{bmatrix}=4\cdot1+3\cdot-2=-2$，这个变换是一个二维空间向一维空间的变化，所以变换后的结果为一个坐标值。<br>我们可以看到线性变换的计算过程和向量的点积相同$\begin{bmatrix}1\-2\end{bmatrix}\cdot\begin{bmatrix}4\3\end{bmatrix}=4\cdot1+3\cdot-2=-2$，所以向量和一个线性变化有着微妙的联系。<br>假设有一个倾斜的数轴，上面有一个单位向量$\vec v$，对于任意一个向量它在数轴上的投影都是一个数字，这表示了一个二维向量到一位空间的一种线性变换，那么如何得到这个线性变化呢？</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/032.png">        </p><p>由之前的内容来说，我们可以观察基向量$\vec i$和$\vec j$的变化，从而得到对应的线性变化。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/033.png">        </p><p>因为$\vec i$、$\vec j$、$\vec u$都是单位向量，根据对称性可以得到$\vec i$和$\vec j$在$\vec u$上的投影长度刚好是$\vec u$的坐标。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/034.png">        </p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/035.png">        </p><p>这样空间中的所有向量都可以通过线性变化<br>$\begin{bmatrix} u_x&amp;u_y \end{bmatrix}$<br>得到，而这个计算过程刚好和单位向量的点积相同。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/036.png"></p><p>也就是为什么向量投影到直线的长度，刚好等于它与直线上单位向量的点积，对于非单位向量也是类似，只是将其扩大到对应倍数。</p><h1 id="叉积"><a href="#叉积" class="headerlink" title="叉积"></a>叉积</h1><p>对于两个向量所围成的面积来说，可以使用行列式计算，将两个向量看作是变换后的基向量，这样通过行列式就可以得到变换后面积缩放的比例，因为基向量的单位为1，所以就得到了对应的面积。<br>考虑到正向，这个面积的值存在负值，这是参照基向量$\vec i$和$\vec j$的相对位置来说的。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/037.png">        </p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/038.png">        </p><p>真正的叉积是通过两个三维向量$\vec v$和$\vec w$，生成一个新的三维向量$\vec u$，这个向量垂直于向量$\vec v$和$\vec w$所在的平面，长度等于它们围成的面积。<br>叉积的反向可以通过右手定则判断：</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/039.png">        </p>叉积的计算方法：<p align="center">            <img src="https://jpccc.github.io/resource/algebra/040.png">        </p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/041.png">        </p><h1 id="线性代数看叉积"><a href="#线性代数看叉积" class="headerlink" title="线性代数看叉积"></a>线性代数看叉积</h1><p>参考二维向量的叉积计算：</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/042.png"></p><p>三维的可以写成类似的形式，但是他并是真正的叉积，不过和真正的叉积已经很接近了。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/043.png"></p><p>我可以构造一个函数，它可以把一个三维空间映射到一维空间上。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/044.png"></p><p>右侧行列式是线性的，所以我们可以找到一个线性变换代替这个函数。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/045.png"></p><p>根据对偶性的思想，从多维空间到一维空间的线性变换，等于与对应向量的点积，这个特殊的向量$\vec p$就是我们要找的向量。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/046.png"></p><blockquote><p>从数值计算上:</p></blockquote><p align="center">            <img src="https://jpccc.github.io/resource/algebra/047.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/048.png"></p><p>向量$\vec p$的计算结果刚好和叉积计算的结果相同。</p><blockquote><p>从几何意义：</p></blockquote><p align="center">            <img src="https://jpccc.github.io/resource/algebra/049.png"></p><p>当向量$\vec p$和向量$\begin{bmatrix}x\\y\\z \end{bmatrix}$点乘时，得到一个$\begin{bmatrix}x\\y\\z \end{bmatrix}$与$\vec v$与$\vec w$确定的平行六面体的有向体积，什么样的向量满足这个性质呢？<br>点积的几何解释是，其他向量在$\vec p$上的投影的长度乘以$\vec p$的长度。<br>对于平行六面体的体积来说，它等于$\vec v$和$\vec w$所确定的面积乘以$\begin{bmatrix}x\\y\\z \end{bmatrix}$在垂线上的投影。<br>那么$\vec p$要想满足这一要求，那么它就刚好符合，长度等于$\vec v,\vec w$所围成的面积，且刚好垂直这个平面。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/050.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/051.png"></p><h1 id="基变换"><a href="#基变换" class="headerlink" title="基变换"></a>基变换</h1><p>标准坐标系的基向量为$\vec {i}: \begin{bmatrix}1\0 \end{bmatrix}$和$\vec {j}: \begin{bmatrix}0\1 \end{bmatrix}$，假如詹妮弗有另一个坐标系：她的基向量为$\vec i \begin{bmatrix}2\1 \end{bmatrix}$和$\vec j \begin{bmatrix}-1\1 \end{bmatrix}$。<br>对于同一个点$\begin{bmatrix}3\2 \end{bmatrix}$来说他们所表示的形式不同，在詹妮弗的坐标系中表示为$\begin{bmatrix}\frac{5}{3}\\frac{1}{3} \end{bmatrix}$。（在不同基向量下，坐标同基相乘的结果是一样的。）<br>{詹尼弗的坐标乘以其基向量的结果(向量)是在我们坐标系中的表示。，即$\begin{bmatrix}3\2 \end{bmatrix}$}<br>从标准坐标到詹尼佛的坐标系，我能可以得到一个线性变换$A:\begin{bmatrix}2&amp;-1\1&amp;1 \end{bmatrix}$。（这个变换将詹尼佛的0，1变成我们语言表示的詹尼佛的0，1）</p><p>如果想知道詹妮弗的坐标系中点$\begin{bmatrix}3\2 \end{bmatrix}$在标准坐标系的位置，可以通过$\begin{bmatrix}2&amp;-1\1&amp;1 \end{bmatrix}\begin{bmatrix}3\2 \end{bmatrix}$得到。（基是我们的语言表示，而坐标是詹妮弗中的坐标，那么在我们的空间网格中，詹妮弗的坐标系中点$\begin{bmatrix}3\2 \end{bmatrix}$所代表的向量在我们的坐标系中的坐标为$\begin{bmatrix}4\5 \end{bmatrix}$）</p><p>如果想知道标准坐标系中点$\begin{bmatrix}3\2 \end{bmatrix}$在詹妮弗坐标系的位置，可以通过$\begin{bmatrix}2&amp;-1\1&amp;1 \end{bmatrix}^{-1}\begin{bmatrix}3\2 \end{bmatrix}$得到。<br>具体的例子，90°旋转。<br>在标准坐标系可以跟踪基向量的变化来体现：</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;一-向量&quot;&gt;&lt;a href=&quot;#一-向量&quot; class=&quot;headerlink&quot; title=&quot;一. 向量&quot;&gt;&lt;/a&gt;一. 向量&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;向量对于不同的学科有不一样的定义，我们将从三个角度对向量进行解释。&lt;ul&gt;
&lt;li&gt;物理学&lt;br&gt; 物理中的向量由长度和方向决定，长度和方向不变的情况下随意移动，表示的都是同一个向量。&lt;/li&gt;
&lt;li&gt;计算机&lt;br&gt; 计算机中的向量更多的是对数据的抽象，可以是根据面积和价格定义的一个房子特征$\begin{bmatrix}100m^2\\700000￥\end{bmatrix}$或是通过神经网络得到的图象的的一个向量。&lt;/li&gt;
&lt;li&gt;数学&lt;br&gt; 数学中的向量可以是任意东西，只要保证两个向量的相加$\vec v + \vec w$以及数字和向量相乘$2\vec v$是有意义的即可。</summary>
    
    
    
    
    <category term="math" scheme="https://jpccc.github.io/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://jpccc.github.io/2021/10/17/hello-world/"/>
    <id>https://jpccc.github.io/2021/10/17/hello-world/</id>
    <published>2021-10-17T07:02:13.391Z</published>
    <updated>2021-10-23T05:02:31.970Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
