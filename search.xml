<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>DroupOut</title>
      <link href="/2021/10/25/DroupOut/"/>
      <url>/2021/10/25/DroupOut/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>BN算法</title>
      <link href="/2021/10/25/BN%E7%AE%97%E6%B3%95/"/>
      <url>/2021/10/25/BN%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>基础知识掌握情况决定研究的高度，我们刚开始接触深度学习时，一般都是看到别人的概括，这个方法很好能让我们快速上手，但是也有一个很大的缺点， 知识理解的不透彻，导致我们对算法优化时一头雾水。我也是抱着知识总结的思想开始自己的深度学习知识精髓的探索，也希望能从中帮助到更多人。文章中间存在表述不清的地方希望各位研友（研究深度学习的朋友）提出，我会努力完善自己的文章。</p><h1 id="BN算法产生的背景"><a href="#BN算法产生的背景" class="headerlink" title="BN算法产生的背景"></a>BN算法产生的背景</h1><p align="center">            <img src="https://jpccc.github.io/resource/deepLearning/001.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/deepLearning/001.png"></p><blockquote><p>首先对第一张图进行分析。</p><blockquote><p>由于我们通常使用采用零均值化对网络进行参数初始化，我们初始的拟合直线也就是红色部分。另外的一条绿色直线，是我们的目标直线。从图能够直观看出，我们应该需要多次迭代才能得到我们的需要的目标直线。</p></blockquote></blockquote><blockquote><p>我们再看第二张图</p><blockquote><p>假设我们还是和第一张图有相同的分布，只是我们做了减均值，让数据均值为零。能够直观的发现可能只进行简单的微调就能够实现拟合（理想）。大大提高了我们的训练速度。因此，在训练开始前，对数据进行零均值是一个必要的操作。</p></blockquote></blockquote><blockquote><p>但是，随着网络层次加深参数对分布的影响不定(什么意思?)，导致网络每层间以及不同迭代轮次的相同层的输入分布发生改变，导致网络需要重新适应新的分布，迫使我们降低学习率降低影响。在这个背景下BN算法开始出现。       有些人首先提出在每层增加PCA白化(先对数据进行去相关然后再进行归一化)，这样基本满足了数据的0均值、单位方差、弱相关性。但是这样是不可取的，因为在白化过程中会计算协方差矩阵、求逆等操作，计算量会很大，另外，在反向传播时，白化的操作不一定可微。因此，在此背景下BN算法开始出现。</p></blockquote><h1 id="BN算法的实现和优点"><a href="#BN算法的实现和优点" class="headerlink" title="BN算法的实现和优点"></a>BN算法的实现和优点</h1><p>上面提到了PCA白化优点，能够去相关和数据均值，标准值归一化等优点。但是当数据量比较大的情况下去相关的话需要大量的计算，因此有些人提出了只对数据进行均值和标准差归一化。叫做近似白化预处理。</p><p>$$\hat{x}^k=\frac{X^k-E(X^k)}{\sqrt{Var[(x^k)}]}$$</p><p>由于训练过程采用了batch随机梯度下降，因此$E(X^k)$指的是一批训练数据时，各神经元输入值的平均值；$\sqrt{Var[(x^k)}]$指的是一批训练数据时各神经元输入值的标准差。</p><p>但是，这些应用到深度学习网络还远远不够，因为可能由于这种的强制转化导致数据的分布发生破坏。因此需要对公式的鲁棒性进行优化，就有人提出了变换重构的概念。就是在基础公式的基础之上加上了两个参数γ、β。这样在训练过程中就可以学习这两个参数，采用适合自己网络的BN公式。公式如下：<br>$$y^k=\gamma^k\hat x^k+\beta^k$$<br>每一个神经元都会有一对这样的参数γ、β。这样其实当<br>$$\beta^k=E[x^k],\gamma^k=\sqrt{var[x^k]}$$<br>时，是可以恢复出原始的某一层所学到的特征的。引入可学习重构参数γ、β，让网络可以学习恢复出原始网络所要学习的特征分布。</p><p>总结上面我们会得到BN的向前传导公式：</p><p>$\mu_\beta\leftarrow\frac{1}{m}\sum_{i=1}^nx_i$ //mnni batch mean<br>$\delta_\beta^2\leftarrow\frac{1}{m}\sum_{i=1}^n(x_i-\mu_\beta)^2$//mnni batch variance<br>$\hat{x}\leftarrow\frac{x_i-\mu_\beta}{\sqrt{\delta_\beta^2+\epsilon}}$//normalize<br>$y_i\leftarrow\gamma\hat{x_i}+\beta\equiv BN_{\gamma,\beta}(x_i)$ //scale and shift<br>2. BN算法在网络中的作用</p><p>   BN算法像卷积层，池化层、激活层一样也输入一层。BN层添加在激活函数前，对输入激活函数的输入进行归一化。这样解决了输入数据发生偏移和增大的影响。</p><p>优点：</p><p>1、加快训练速度，能够增大学习率，即使小的学习率也能够有快速的学习速率;<br>2、不用理会拟合中的droupout、L2 正则化项的参数选择，采用BN算法可以省去这两项或者只需要小的L2正则化约束。原因，BN算法后，参数进行了归一化，原本经过激活函数没有太大影响的神经元，分布变得明显，经过一个激活函数以后，神经元会自动削弱或者去除一些神经元，就不用再对其进行dropout。另外就是L2正则化，由于每次训练都进行了归一化，就很少发生由于数据分布不同导致的参数变动过大，带来的参数不断增大。<br>3、 可以吧训练数据集打乱，防止训练发生偏移。</p><p>使用： 在卷积中，会出现每层卷积层中有（L）多个特征图。AxAxL特征矩阵。我们只需要以每个特征图为单元求取一对γ、β。</p>]]></content>
      
      
      
        <tags>
            
            <tag> deepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>互信息</title>
      <link href="/2021/10/25/%E4%BA%92%E4%BF%A1%E6%81%AF/"/>
      <url>/2021/10/25/%E4%BA%92%E4%BF%A1%E6%81%AF/</url>
      
        <content type="html"><![CDATA[<h1 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h1><p>机器学习中很多地方都要根据目前的信息做出决策，信息熵主要是反应信息的不确定性；它的一个很重要的作用，就是做决策时提供一定的判断依据，比如决策树根据熵来往下设置分枝(branch)。</p><ol><li>计算方法 $$H(X)=-\sum_{i=1}^nP(x_i)logP(x_i)$$<br> 其中$P(x_i)$代表随机事件X为$x_i$的概率。</li></ol><span id="more"></span><ol start="2"><li>信息量</li></ol><blockquote><p>信息量是对信息的度量，就跟时间的度量是秒一样；我们考虑一个离散的随机变量x，当我们观察到的这个变量的一个具体值的时候，我们接收到了多少信息呢？</p><blockquote><p>多少信息用信息量来衡量，我们接受到的信息量信息的大小跟随机事件的概率分布有关，越小概率的事情发生了产生的信息量越大，如湖南产生的地震了；越大概率的事情发生了产生的信息量越小，如太阳从东边升起来了（肯定发生嘛，没什么信息量）。这很好理解！ 所以描述信息量的函数应该是一个与随机变量的发生概率成负相关的函数，且不能为负数。</p></blockquote></blockquote><blockquote><p>例子:</p><blockquote><p>如果我们有俩个不相关的事件x和y，那么我们观察到的俩个事件同时发生时获得的信息应该等于观察到的事件各自发生时获得的信息之和，即：h(x,y) = h(x) + h(y)。 由于x，y是俩个不相关的事件，那么满足p(x,y) = p(x)*p(y)。<br>根据上面推导，我们很容易看出h(x)一定与p(x)的对数有关（因为只有对数形式的真数相乘之后，能够对应对数的相加形式，可以试试）。因此我们有信息量公式如下：<br>$$h(x)=-log_2p(x)$$ </p></blockquote></blockquote><blockquote><p>下面解决两个疑问:</p><blockquote><p>(1). 为什么有一个负号?<br>&nbsp;&nbsp;&nbsp;&nbsp; 信息量取概率的负对数，其实是因为信息量的定义是概率的倒数的对数。而用概率的倒数，是为了使概率越大，信息量越小，同时因为概率的倒数大于1，其对数自然大于0了。<br>(2). 为什么底数为2?<br>&nbsp;&nbsp;&nbsp;&nbsp;这是因为，我们只需要信息量满足低概率事件x对应于高的信息量，那么对数的选择是任意的，我们只是遵循信息论的普遍传统，使用2作为对数的底！</p></blockquote></blockquote><ol start="3"><li>信息熵</li></ol><blockquote><p>信息量度量的是一个具体事件发生了所带来的信息，而熵则是在结果出来之前对可能产生的信息量的期望——考虑该随机变量的所有可能取值，即所有可能发生事件所带来的信息量的期望。即<br>                     $$H(X)=-\sum_{i=1}^np(x_i)logp(x_i)$$</p><blockquote><p>信息熵还可以作为一个系统复杂程度的度量，如果系统越复杂，出现不同情况的种类越多，那么他的信息熵是比较大的。如果一个系统越简单，出现情况种类很少（极端情况为1种情况，那么对应概率为1，那么对应的信息熵为0），此时的信息熵较小。</p></blockquote></blockquote><h1 id="互信息"><a href="#互信息" class="headerlink" title="互信息"></a>互信息</h1><ol><li><p>定义</p><blockquote><p>互信息(Mutual Information)是衡量随机变量之间相互依赖程度的度量。</p><blockquote><p>它的形象化解释是，假如明天下雨是个随机事件，假如今晚有晚霞同样是个随机事件，那么这两个随机事件互相依赖的程度是：<br>&nbsp;&nbsp;&nbsp;&nbsp;当<b>不知道</b>”今晚有晚霞“情况下，”明天下雨“带来的不确定性<br>&nbsp;&nbsp;&nbsp;&nbsp;<b>与我们已知</b>“今晚有晚霞“情况下，”明天下雨”带来的不确定性之差。</p></blockquote></blockquote></li><li><p>解释<br>假设存在一个随机变量$X$ ，和另外一个随机变量$Y$ ，那么它们的互信息是<br>$$I(X;Y)=H(X)-H(X|Y)$$</p><p>$H(X)$是$X$的信息熵,$H(X|Y)$是已知$Y$情况下，X带来的信息熵（条件熵）。</p><blockquote><p>直观理解是，我们知道存在两个随机事件X,Y，其中一个随机事件X 给我们带来了一些不确定性H(X)，我们想衡量Y,X 之间的关系。那么，如果X,Y 存在关联，当Y已知时，X给我们的不确定性会变化，这个变化值就是X的信息熵减去当已知 Y时，X的条件熵，就是互信息。</p></blockquote><p> 从概率角度，互信息是由随机变量 $X,Y$ 的联合概率分布 p(x,y) 和边缘概率分布 $p(x),p(y)$ 得出。</p><p> $$I(X;Y)=\sum_{y \in \cal Y}\sum_{x \in \cal X}p(x,y)log(\frac{p(x,y)}{p(x)p(y)})$$<br>  互信息和信息熵的关系是：</p> <p align="center">             <img src="https://jpccc.github.io/resource/Mutual_%20Information/001.jpg"> </p><p> 通常我们使用的最大化互信息条件，就是最大化两个随机事件的相关性。在数据集里，就是最大化两个数据集所拟合出的概率分布的相关性。当两个随机变量相同时,互信息最大，如下:<br>$$I(X;Y)=H(X)-H(X|X)=H(X)$$</p><blockquote><p>在机器学习中，理想情况下，当互信息最大，可以认为从数据集中拟合出来的随机变量的概率分布与真实分布相同。</p></blockquote><p>到这里，应该足够大家日常理解使用了，以下是性质，应用和变形，几乎都是数学。</p></li><li></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> math </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>algebra</title>
      <link href="/2021/10/22/algebra/"/>
      <url>/2021/10/22/algebra/</url>
      
        <content type="html"><![CDATA[<h1 id="一-向量"><a href="#一-向量" class="headerlink" title="一. 向量"></a>一. 向量</h1><ol><li>向量对于不同的学科有不一样的定义，我们将从三个角度对向量进行解释。<ul><li>物理学<br> 物理中的向量由长度和方向决定，长度和方向不变的情况下随意移动，表示的都是同一个向量。</li><li>计算机<br> 计算机中的向量更多的是对数据的抽象，可以是根据面积和价格定义的一个房子特征$\begin{bmatrix}100m^2\\700000￥\end{bmatrix}$或是通过神经网络得到的图象的的一个向量。</li><li>数学<br> 数学中的向量可以是任意东西，只要保证两个向量的相加$\vec v + \vec w$以及数字和向量相乘$2\vec v$是有意义的即可。<span id="more"></span></li></ul></li><li>线性代数中的向量可以理解为一个空间中的箭头，这个箭头起点落在原点。如果空间中有许多的向量，可以点表示一个向量，即向量头的坐标。</li></ol><h1 id="二-向量的基本运算"><a href="#二-向量的基本运算" class="headerlink" title="二. 向量的基本运算"></a>二. 向量的基本运算</h1><ol><li><p>向量的加法<br> 可以理解为在坐标中两个向量的移动。</p> <p align="center">             <img src="https://jpccc.github.io/resource/algebra/001.png"> </p></li><li><p>向量的乘法</p> <p align="center">             <img src="https://jpccc.github.io/resource/algebra/002.png"> </p></li></ol><h1 id="线性组合、张成空间、基"><a href="#线性组合、张成空间、基" class="headerlink" title="线性组合、张成空间、基"></a>线性组合、张成空间、基</h1><blockquote><p> 线性组合</p></blockquote><p>两个数乘向量称为两个向量的线性组合$a\vec v+ b\vec w$。<br>两个不共线的向量通过不同的线性组合可以得到二维平面中的所有向量。<br>两个共线的向量通过线程组合只能得到一个直线的所有向量。<br>如果两个向量都是零向量那么它只能在原点。</p><blockquote><p>张成向量</p></blockquote><p>所有可以表示给定向量线性组合的向量的集合称为给定向量的张成空间（span）。<br>一般来说两个向量张成空间可以是直线、平面。<br>三个向量张成空间可以是平面、空间。<br>如果多个向量，并且可以移除其中一个而不减小张成空间，那么它们是线性相关的，也可以说一个向量可以表示为其他向量的线性组合$\vec u = a \vec v + b\vec w$。<br>如果所有的向量都给张成的空间增加了新的维度，它们就成为线性无关的$\vec u \neq a \vec v + b\vec w$。</p><blockquote><p>基</p></blockquote><p>向量空间的一组基是张成该空间的一个线性无关向量集。</p><h1 id="矩阵与线性变换"><a href="#矩阵与线性变换" class="headerlink" title="矩阵与线性变换"></a>矩阵与线性变换</h1><p>(向量的基默认为(0,1)(1,0)正交基。左乘向量可以看作是向量对基向量进行操作。同一向量乘以不同的基表示对不同的基做相同的运算。)<br>严格意义上来说，线性变换是将向量作为输入和输出的一类函数。<br>变化可以多种多样，线性变化将变化限制在一个特殊类型的变换上，可以简单的理解为网格线保持平行且等距分布。<br>线性变化满足一下两个性质：</p><ul><li>线性变化前后直线依旧是直线不能弯曲。</li><li>原点必须保持固定  <p align="center">              <img src="https://jpccc.github.io/resource/algebra/003.png">  </p></li></ul><p>可以使用<strong>基向量来描述线性变化：</strong><br>通过记录两个基向量$\hat{i}$,$\hat{j}$的变换，就可以得到其他变化后的向量。<br>已知向量$\vec v = \begin{bmatrix}-1\\2\end{bmatrix}$<br>变换之前的$\hat i$和$\hat j$：</p><p>$$<br>\hat{i} = \begin{bmatrix}<br>1 \<br>   0<br>  \end{bmatrix} \<br>  \hat{j} = \begin{bmatrix}<br>0 \<br>   1<br>  \end{bmatrix} \<br>\vec{v} = -1\hat{i} + 2 \hat{j} = \begin{bmatrix}<br>-1 \<br>   2<br>  \end{bmatrix}<br>$$</p><p>变换之后的$\hat i$和$\hat j$：</p><p>$$<br>\begin{aligned}<br>\hat{i} = \begin{bmatrix}<br>1 \<br>   -2<br>  \end{bmatrix} \<br>  \hat{j} = \begin{bmatrix}<br>3 \<br>   0<br>  \end{bmatrix} \<br>\vec{v} = -1\hat{i} + 2 \hat{j} &amp;= \begin{bmatrix}<br>5 \<br>   2<br>  \end{bmatrix} \<br>  &amp;= -1\begin{bmatrix}<br>1 \<br>   -2<br>  \end{bmatrix}  + 2 \begin{bmatrix}<br>3 \<br>  0<br>  \end{bmatrix} \<br>  &amp;= \begin{bmatrix}<br>1 &amp; 3 \<br>   -2 &amp; 0<br>  \end{bmatrix}  \begin{bmatrix}<br>-1 \<br>   2<br>  \end{bmatrix} \<br>  &amp;= \begin{bmatrix}<br>5 \<br>   2<br>  \end{bmatrix} \<br>\end{aligned}<br>$$<br><strong>将对基向量的变换记录下来，对其作用于其它向量，就可以得到其它向量在变换后的空间中的值</strong></p><p>我们可以将变换后的$\hat i$和$\hat j$写成矩阵的形式：$\begin{bmatrix}<br>1 &amp; 3 \<br>   -2 &amp; 0<br>  \end{bmatrix}  \begin{bmatrix}<br>-1 \<br>   2<br>  \end{bmatrix}$，通过矩阵的乘法得到变化后的向量。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/004.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/005.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/006.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/007.png"></p>如果变化后的$\hat{i}$和$\hat{j}$是线性相关的，变化后向量的张量就是一维空间：<p align="center">            <img src="https://jpccc.github.io/resource/algebra/008.png"></p><h1 id="矩阵乘法与线性变换复合的联系"><a href="#矩阵乘法与线性变换复合的联系" class="headerlink" title="矩阵乘法与线性变换复合的联系"></a>矩阵乘法与线性变换复合的联系</h1><blockquote><p>线性变化的复合</p></blockquote><p>如何描述先旋转再剪切的操作呢？</p><p>一个通俗的方法是首先左乘旋转矩阵然后左乘剪切矩阵。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/009.png"></p>   <p>两个矩阵的乘积需要从右向左读，类似函数的复合。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/010.png"></p><p>这样两个矩阵的乘积就对应了一个复合的线性变换，最终得到对应变换后的$\hat{i}$和$\hat{j}$</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/011.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/012.png"></p>这一过程具有普适性：<p align="center">            <img src="https://jpccc.github.io/resource/algebra/013.png"></p><blockquote><p>矩阵乘法的顺序</p></blockquote><p align="center">            <img src="https://jpccc.github.io/resource/algebra/014.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/015.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/016.png"></p><blockquote><p>如何证明矩阵乘法的结合性？</p></blockquote><p>$(AB)C = A(BC)$<br>根据线性变化我们可以得出，矩阵的乘法都是以CBA的顺序变换得到，所以他们本质上相同，通过变化的形式解释比代数计算更加容易理解。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/017.png"></p><h1 id="三维空间的线性变化"><a href="#三维空间的线性变化" class="headerlink" title="三维空间的线性变化"></a>三维空间的线性变化</h1><p>三维的空间变化和二维的类似。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/018.png"></p><p>同样跟踪基向量的变换，能很好的解释变换后的向量，同样两个矩阵相乘的复合变换也是。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/019.png"></p><h1 id="行列式"><a href="#行列式" class="headerlink" title="行列式"></a>行列式</h1><blockquote><p>行列式的本质</p></blockquote><p>行列式的本质是计算线性变化对空间的缩放比例，具体一点就是，测量一个给定区域面积增大或减小的比例。<strong>注意，面积的变化比例是对原空间中的网格来说的。也可以说成是原基向量组成的区域面积的变化。</strong><br>单位面积的变换代表任意区域的面积变换比例。<br>值得注意的是：</p><ul><li>如果一个二维线性变换的行列式为0，说明其将说明起其将整个平面压缩成一条线甚至一个点上。</li><li>所以只要检测一个矩阵的行列式是否为0，我们就可以知道矩阵所代表的变换是否将空间压缩到更小的维度上。</li></ul><p align="center">            <img src="https://jpccc.github.io/resource/algebra/020.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/021.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/022.png"></p>行列式的值表示缩放比例。<p align="center">            <img src="https://jpccc.github.io/resource/algebra/023.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/024.png"></p><p>行列式为什么有负值呢？可以从两个角度考虑：</p><ul><li>一是变换将平面进行了反转。就好像将一张纸的正面通过变换将这张纸进行了翻面。</li><li>二是考虑的$\hat i$和$\hat j$的相对位置。如$\hat i$在$\hat j$的左边，通过变换将$\hat i$变换到了$\hat j$的右边，那么这个变换所对应矩阵的行列式值为负。<p align="center">          <img src="https://jpccc.github.io/resource/algebra/001.gif"></p></li></ul><p>三维空间的行列式类似，它的单位是一个单位1的立方体。</p><p>三位空间的线性变换，可以使用右手定则判断三维空间的定向。如果变换前后都可以通过右手定则得到，那么他的行列式就是正值，否则为负值.</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/025.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/026.png"></p><h1 id="逆矩阵、列空间、秩、零空间"><a href="#逆矩阵、列空间、秩、零空间" class="headerlink" title="逆矩阵、列空间、秩、零空间"></a>逆矩阵、列空间、秩、零空间</h1><blockquote><p>线性方程组</p></blockquote><p align="center">            <img src="https://jpccc.github.io/resource/algebra/027.png"></p><p>从几何的角度来思考，矩阵A表示一个线性变换，我们需要找到一个$\vec x$使得它在变换后和$\vec v$重合。</p><blockquote><p>逆矩阵</p></blockquote><p>矩阵的逆运算，记为$\vec A = \begin{bmatrix}3&amp;1 \0&amp;2\end{bmatrix}^{-1}$，对于线程方程$A \vec x = \vec v $来说，找到$A^{-1}$就得到解$\vec x = A^{-1} \vec v$。<br>$A^{-1}A=\begin{bmatrix}1&amp;0 \0&amp;1\end{bmatrix}$，什么都不做称为恒等变换。</p><blockquote><p>线性方程组的解</p></blockquote><p>对于方程组$A\vec x = \vec v$，线性变换A存在两种情况：<br>$det(A) \neq0$：这时空间的维数并没有改变，有且只有一个向量经过线性变换后和$\vec v$重合。<br>$det(A) =0$：空间被压缩到更低的维度，这时不存在逆变换，因为不能将一个直线解压缩为一个平面(这会要求将一个单独的向量变换为一整条线的向量，函数多对一可以，一对多不行，即存在一个矩阵A将多个向量映射到一个点，但不可能存在一个矩阵A将一个点映射成一条线的向量)。但是即使不存在逆变换，解可能仍然存在，这时候目标$\vec v$必须刚好落在压缩后的空间上。(例如一个变换将空间压缩成了一条直线，而向量$\vec v$恰好在这条直线上。共线的情况下，则有无穷解，因为有一个直线上的向量都被压缩到直线上的每一点)。（但是能将直线上的一个非0向量映射成平面上的某个唯一向量）</p><blockquote><p>秩</p></blockquote><p>秩代表变换后空间的维度。<br>如果线性变化后将空间压缩成一条直线，那么称这个变化的秩为1；<br>如果线性变化后向量落在二维平面，那么称这个变化的秩为2。</p><blockquote><p>列空间</p></blockquote><p>所有可能的输出向量$A\vec v$构成的集合(A为基向量的集合，对其进行任意的组合(组合即乘以$\vec{v}$)，所得到的所有向量)，称为列空间，即所有列向量张成的空间。</p><ul><li>更精确的秩的定义就是列空间的维数。</li></ul><blockquote><p>零空间（Null space）</p></blockquote><p>所有的线性变化中，零向量一定包含在列空间中，因为线性变换原点保持不动。对于非满秩的情况来说，会有一系列的向量在变换后仍为零向量（二维空间压缩为一条直线，一条线上的向量都会落到原点。）</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/002.gif"></p><p>三维空间压缩为二维平面，一条线上的向量都会落到原点。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/003.gif"></p><p>三维空间压缩为一条直线，整个平面上的向量都会落到原点。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/004.gif"></p><p>当$A\vec x = \vec v$中的$\vec v$是一个零向量，即$A\vec x = \begin{bmatrix}0 \\0\end{bmatrix}$时，零空间就是它所有可能的解。</p><h1 id="非方阵、不同维度空间之间的线性变换"><a href="#非方阵、不同维度空间之间的线性变换" class="headerlink" title="非方阵、不同维度空间之间的线性变换"></a>非方阵、不同维度空间之间的线性变换</h1><p>不同维度的变换也是存在的。</p><p>一个$3\times2$的矩阵：$\begin{bmatrix}2&amp;0\\-1&amp;1\\-2&amp;1 \end{bmatrix}$它的几何意义是将一个二维空间映射到三维空间上，矩阵有两列表明输入空间有两个基向量，有三行表示每个向量在变换后用三个独立的坐标描述。(A是满秩的，因为其与输入空间x的维度相等)</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/028.png"></p><p>一个$2\times 3$的矩阵：$\begin{bmatrix}3&amp;1&amp;4\\1&amp;5&amp;9 \end{bmatrix}$则表示将一个三维空间映射到二维空间上。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/029.png"></p><p>一个$1\times 2$的矩阵：$\begin{bmatrix}1&amp;2 \end{bmatrix}$表示一个二维空间映射到一维空间。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/030.png"></p>(还可以从解方程的角度，通解+特解，来理解一维变二维。)# 点积与对偶性> 点积<p>对于两个维度相同的向量，他们的点积计算为：$\begin{bmatrix}1\\2\end{bmatrix}\cdot\begin{bmatrix} 3\\4\end{bmatrix}=1\cdot3+2\cdot4=11$。<br>点积的几何解释是将一个向量向一个向量投影，然后两个长度相乘，如果为负数则表示反向。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/031.png"></p><p>为什么点积和坐标相乘联系起来了？这和对偶性有关。</p><blockquote><p>对偶性</p></blockquote><p>对偶性的思想是：每当看到一个多维空间到数轴上的线性变换时，他都与空间中的唯一一个向量对应，也就是说使用线性变换和与这个向量点乘等价。这个向量也叫做线性变换的对偶向量。<br>当二维空间向一维空间映射时，如果在二维空间中等距分布的点在变换后还是等距分布的，那么这种变换就是线性的。<br>假设有一个线性变换A$\begin{bmatrix}1&amp;-2\end{bmatrix}$和一个向量$\vec v=\begin{bmatrix}4\3\end{bmatrix}$。<br>变换后的位置为$\begin{bmatrix}1&amp;-2\end{bmatrix}\begin{bmatrix}4\3\end{bmatrix}=4\cdot1+3\cdot-2=-2$，这个变换是一个二维空间向一维空间的变化，所以变换后的结果为一个坐标值。<br>我们可以看到线性变换的计算过程和向量的点积相同$\begin{bmatrix}1\-2\end{bmatrix}\cdot\begin{bmatrix}4\3\end{bmatrix}=4\cdot1+3\cdot-2=-2$，所以向量和一个线性变化有着微妙的联系。<br>假设有一个倾斜的数轴，上面有一个单位向量$\vec v$，对于任意一个向量它在数轴上的投影都是一个数字，这表示了一个二维向量到一位空间的一种线性变换，那么如何得到这个线性变化呢？</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/032.png">        </p><p>由之前的内容来说，我们可以观察基向量$\vec i$和$\vec j$的变化，从而得到对应的线性变化。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/033.png">        </p><p>因为$\vec i$、$\vec j$、$\vec u$都是单位向量，根据对称性可以得到$\vec i$和$\vec j$在$\vec u$上的投影长度刚好是$\vec u$的坐标。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/034.png">        </p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/035.png">        </p>这样空间中的所有向量都可以通过线性变化$\begin{bmatrix}u_x&u_y \end{bmatrix}$得到，而这个计算过程刚好和单位向量的点积相同。<p align="center">            <img src="https://jpccc.github.io/resource/algebra/036.png">        </p><p>也就是为什么向量投影到直线的长度，刚好等于它与直线上单位向量的点积，对于非单位向量也是类似，只是将其扩大到对应倍数。</p><h1 id="叉积"><a href="#叉积" class="headerlink" title="叉积"></a>叉积</h1><p>对于两个向量所围成的面积来说，可以使用行列式计算，将两个向量看作是变换后的基向量，这样通过行列式就可以得到变换后面积缩放的比例，因为基向量的单位为1，所以就得到了对应的面积。<br>考虑到正向，这个面积的值存在负值，这是参照基向量$\vec i$和$\vec j$的相对位置来说的。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/037.png">        </p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/038.png">        </p><p>真正的叉积是通过两个三维向量$\vec v$和$\vec w$，生成一个新的三维向量$\vec u$，这个向量垂直于向量$\vec v$和$\vec w$所在的平面，长度等于它们围成的面积。<br>叉积的反向可以通过右手定则判断：</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/039.png">        </p>叉积的计算方法：<p align="center">            <img src="https://jpccc.github.io/resource/algebra/040.png">        </p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/041.png">        </p><h1 id="线性代数看叉积"><a href="#线性代数看叉积" class="headerlink" title="线性代数看叉积"></a>线性代数看叉积</h1><p>参考二维向量的叉积计算：</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/042.png"></p><p>三维的可以写成类似的形式，但是他并是真正的叉积，不过和真正的叉积已经很接近了。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/043.png"></p><p>我可以构造一个函数，它可以把一个三维空间映射到一维空间上。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/044.png"></p><p>右侧行列式是线性的，所以我们可以找到一个线性变换代替这个函数。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/045.png"></p><p>根据对偶性的思想，从多维空间到一维空间的线性变换，等于与对应向量的点积，这个特殊的向量$\vec p$就是我们要找的向量。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/046.png"></p><blockquote><p>从数值计算上:</p></blockquote><p align="center">            <img src="https://jpccc.github.io/resource/algebra/047.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/048.png"></p><p>向量$\vec p$的计算结果刚好和叉积计算的结果相同。</p><blockquote><p>从几何意义：</p></blockquote><p align="center">            <img src="https://jpccc.github.io/resource/algebra/049.png"></p><p>当向量$\vec p$和向量$\begin{bmatrix}x\\y\\z \end{bmatrix}$点乘时，得到一个$\begin{bmatrix}x\\y\\z \end{bmatrix}$与$\vec v$与$\vec w$确定的平行六面体的有向体积，什么样的向量满足这个性质呢？<br>点积的几何解释是，其他向量在$\vec p$上的投影的长度乘以$\vec p$的长度。<br>对于平行六面体的体积来说，它等于$\vec v$和$\vec w$所确定的面积乘以$\begin{bmatrix}x\\y\\z \end{bmatrix}$在垂线上的投影。<br>那么$\vec p$要想满足这一要求，那么它就刚好符合，长度等于$\vec v,\vec w$所围成的面积，且刚好垂直这个平面。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/050.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/051.png"></p><h1 id="基变换"><a href="#基变换" class="headerlink" title="基变换"></a>基变换</h1><p>标准坐标系的基向量为$\vec {i}: \begin{bmatrix}1\0 \end{bmatrix}$和$\vec {j}: \begin{bmatrix}0\1 \end{bmatrix}$，假如詹妮弗有另一个坐标系：她的基向量为$\vec i \begin{bmatrix}2\1 \end{bmatrix}$和$\vec j \begin{bmatrix}-1\1 \end{bmatrix}$。<br>对于同一个点$\begin{bmatrix}3\2 \end{bmatrix}$来说他们所表示的形式不同，在詹妮弗的坐标系中表示为$\begin{bmatrix}\frac{5}{3}\\frac{1}{3} \end{bmatrix}$。（在不同基向量下，坐标同基相乘的结果是一样的。）<br>{詹尼弗的坐标乘以其基向量的结果(向量)是在我们坐标系中的表示。，即$\begin{bmatrix}3\2 \end{bmatrix}$}<br>从标准坐标到詹尼佛的坐标系，我能可以得到一个线性变换$A:\begin{bmatrix}2&amp;-1\1&amp;1 \end{bmatrix}$。（这个变换将詹尼佛的0，1变成我们语言表示的詹尼佛的0，1）</p><p>如果想知道詹妮弗的坐标系中点$\begin{bmatrix}3\2 \end{bmatrix}$在标准坐标系的位置，可以通过$\begin{bmatrix}2&amp;-1\1&amp;1 \end{bmatrix}\begin{bmatrix}3\2 \end{bmatrix}$得到。（基是我们的语言表示，而坐标是詹妮弗中的坐标，那么在我们的空间网格中，詹妮弗的坐标系中点$\begin{bmatrix}3\2 \end{bmatrix}$所代表的向量在我们的坐标系中的坐标为$\begin{bmatrix}4\5 \end{bmatrix}$）</p><p>如果想知道标准坐标系中点$\begin{bmatrix}3\2 \end{bmatrix}$在詹妮弗坐标系的位置，可以通过$\begin{bmatrix}2&amp;-1\1&amp;1 \end{bmatrix}^{-1}\begin{bmatrix}3\2 \end{bmatrix}$得到。<br>具体的例子，90°旋转。<br>在标准坐标系可以跟踪基向量的变化来体现：</p>]]></content>
      
      
      
        <tags>
            
            <tag> math </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2021/10/17/hello-world/"/>
      <url>/2021/10/17/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
