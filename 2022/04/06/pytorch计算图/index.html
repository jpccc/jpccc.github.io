<!DOCTYPE html>
<html lang="zh-Hans">
<head>

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no" />
<meta name="author" content=" liusha" />



<meta name="description" content="IntroductionPyTorch is one of the foremost python deep learning libraries out there. It’s the go to choice for deep learning research, and as each days passes by, more and more companies and rese">
<meta property="og:type" content="article">
<meta property="og:title" content="Understanding Graphs, Automatic Differentiation and Autograd">
<meta property="og:url" content="https://jpccc.github.io/2022/04/06/pytorch%E8%AE%A1%E7%AE%97%E5%9B%BE/index.html">
<meta property="og:site_name" content="流沙">
<meta property="og:description" content="IntroductionPyTorch is one of the foremost python deep learning libraries out there. It’s the go to choice for deep learning research, and as each days passes by, more and more companies and rese">
<meta property="og:locale">
<meta property="og:image" content="https://jpccc.github.io/resource/pytorch/full_graph.png">
<meta property="og:image" content="https://jpccc.github.io//resource/pytorch/full_graph-16491739555532.png">
<meta property="og:image" content="https://jpccc.github.io/resource/pytorch/computation_graph.png">
<meta property="og:image" content="https://jpccc.github.io/resource/pytorch/d_mini.png">
<meta property="og:image" content="https://jpccc.github.io/resource/pytorch/d_mini_grad.png">
<meta property="og:image" content="https://jpccc.github.io/resource/pytorch/full_graph-16491739555532.png">
<meta property="og:image" content="https://jpccc.github.io/resource/pytorch/image-4.png">
<meta property="article:published_time" content="2022-04-05T16:08:00.000Z">
<meta property="article:modified_time" content="2022-04-06T01:32:53.828Z">
<meta property="article:author" content=" liusha">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jpccc.github.io/resource/pytorch/full_graph.png">

<link rel="apple-touch-icon" href= "/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="流沙" type="application/atom+xml">



    <link rel="shortcut icon" href="/favicon.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">



<link rel="stylesheet" href="/css/style.css">




<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>Understanding Graphs, Automatic Differentiation and Autograd | 流沙</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: false,
        isPost: true,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: true
    }
</script>


    <script> yiliaConfig.jquery_ui = [false]; </script>



    <script> yiliaConfig.rootUrl = "\/";</script>






<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/avatar.png" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/"></a></h1>
        </hgroup>

        

        
            <form id="search-form">
            <input type="text" id="local-search-input" name="q" placeholder="search..." class="search form-control" autocomplete="off" autocorrect="off" searchonload="" />
            <i class="fa fa-times" onclick="resetSearch()"></i>
            </form>
            <div id="local-search-result"></div>
            <p class='no-result'>No results found <i class='fa fa-spinner fa-pulse'></i></p>
        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                        <li>关于我</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="https://jpccc.github.io/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/%E9%9A%8F%E7%AC%94">随笔</a></li>
                        
                            <li><a href="/tags/">标签云</a></li>
                        
                            <li><a href="/about/">关于我</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="mailto:jiangpc21@mails.glu.edu.cn" title="Email"></a>
                            
                                <a class="fa GitHub" href="#" title="GitHub"></a>
                            
                                <a class="fa RSS" href="/atom.xml" title="RSS"></a>
                            
                                <a class="fa QQ" href="/1778013127" title="QQ"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/deepLearning/" rel="tag">deepLearning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/math/" rel="tag">math</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pytorch/" rel="tag">pytorch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/" rel="tag">统计学习</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://hexo.io">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="https://pages.github.com/">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" target="_blank" rel="noopener" href="http://moxfive.xyz/">MOxFIVE</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">机器学习</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页"></a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/avatar.png" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页"></a></h1>
            </hgroup>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="https://jpccc.github.io/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/%E9%9A%8F%E7%AC%94">随笔</a></li>
                
                    <li><a href="/tags/">标签云</a></li>
                
                    <li><a href="/about/">关于我</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="mailto:jiangpc21@mails.glu.edu.cn" title="Email"></a>
                            
                                <a class="fa GitHub" target="_blank" href="#" title="GitHub"></a>
                            
                                <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a>
                            
                                <a class="fa QQ" target="_blank" href="/1778013127" title="QQ"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="标签" friends="友情链接" about="关于我"/>
</nav>
      <div class="body-wrap"><article id="post-pytorch计算图" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2022/04/06/pytorch%E8%AE%A1%E7%AE%97%E5%9B%BE/" class="article-date">
      <time datetime="2022-04-05T16:08:00.000Z" itemprop="datePublished">2022-04-06</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Understanding Graphs, Automatic Differentiation and Autograd
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        

        
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/pytorch/" rel="tag">pytorch</a></li></ul>
    </div>

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <div align=center>

<p><img src="https://jpccc.github.io/resource/pytorch/full_graph.png" alt="img"></p>
</div>

<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>PyTorch is one of the foremost python deep learning libraries out there. It’s the go to choice for deep learning research, and as each days passes by, more and more companies and research labs are adopting this library.</p>
<p>In this series of tutorials, we will be introducing you to PyTorch, and how to make the best use of the libraries as well the ecosystem of tools built around it. We’ll first cover the basic building blocks, and then move onto how you can quickly prototype custom architectures. We will finally conclude with a couple of posts on how to scale your code, and how to debug your code if things go awry.</p>
<p>This is Part 1 of our PyTorch 101 series.</p>
<span id="more"></span>

<ol>
<li><a target="_blank" rel="noopener" href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/">Understanding Graphs, Automatic Differentiation and Autograd</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.paperspace.com/pytorch-101-building-neural-networks/">Building Your First Neural Network</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/blog.paperspace.com/pytorch-101-advanced/">Going Deep with PyTorch</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/blog.paperspace.com/pytorch-memory-multi-gpu-debugging/">Memory Management and Using Multiple GPUs</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/">Understanding Hooks</a></li>
</ol>
<p>You can get all the code in this post, (and other posts as well) in the Github repo <a target="_blank" rel="noopener" href="https://github.com/Paperspace/PyTorch-101-Tutorial-Series">here</a>.</p>
<hr>
<h2 id="Prerequisites"><a href="#Prerequisites" class="headerlink" title="Prerequisites"></a>Prerequisites</h2><ol>
<li>Chain rule</li>
<li>Basic Understanding of Deep Learning</li>
<li>PyTorch 1.0</li>
</ol>
<hr>
<p>You can get all the code in this post, (and other posts as well) in the Github repo <a target="_blank" rel="noopener" href="https://github.com/Paperspace/PyTorch-101-Tutorial-Series">here</a>.</p>
<h2 id="Automatic-Differentiation"><a href="#Automatic-Differentiation" class="headerlink" title="Automatic Differentiation"></a><strong>Automatic</strong> Differentiation</h2><p>A lot of tutorial series on PyTorch would start begin with a rudimentary discussion of what the basic structures are. However, I’d like to instead start by discussing automatic differentiation first.</p>
<p>Automatic Differentiation is a building block of not only PyTorch, but every DL library out there. In my opinion, PyTorch’s automatic differentiation engine, called <em>Autograd</em> is a brilliant tool to understand how automatic differentiation works. This will not only help you understand PyTorch better, but also other DL libraries.</p>
<p>Modern neural network architectures can have millions of learnable parameters. From a computational point of view, training a neural network consists of two phases:</p>
<ol>
<li>A forward pass to compute the value of the loss function.</li>
<li>A backward pass to compute the gradients of the learnable parameters.</li>
</ol>
<p>The forward pass is pretty straight forward. The output of one layer is the input to the next and so forth.</p>
<p>Backward pass is a bit more complicated since it requires us to use the chain rule to compute the gradients of weights w.r.t to the loss function.</p>
<h2 id="A-toy-example"><a href="#A-toy-example" class="headerlink" title="A toy example"></a>A toy example</h2><p>Let us take an very simple neural network consisting of just 5 neurons. Our neural network looks like the following.</p>
<div align=center>

<p><img src="https://jpccc.github.io//resource/pytorch/full_graph-16491739555532.png" alt="img"></p>
</div>

<p>The following equations describe our neural network.</p>
<p>$$<br>b=w1∗a\\<br>c=w2∗a\\<br>d=w3∗b+w4∗c\\<br>L=10−d\\<br>$$</p>
<p>Let us compute the gradients for each of the learnable parameters ww.</p>
<p>$$<br>\frac{\partial{L}}{\partial{w_4}} = \frac{\partial{L}}{\partial{d}}\frac{\partial{d}}{\partial{w_4}} \\<br>\frac{\partial{L}}{\partial{w_3}} = \frac{\partial{L}}{\partial{d}}\frac{\partial{d}}{\partial{w_3}}\\<br>\frac{\partial{L}}{\partial{w_2}} = \frac{\partial{L}}{\partial{d}} * \frac{\partial{d}}{\partial{c}} * \frac{\partial{c}}{\partial{w_2}}\\<br>\frac{\partial{L}}{\partial{w_1}} = \frac{\partial{L}}{\partial{d}}* \frac{\partial{d}}{\partial{b}} * \frac{\partial{b}}{\partial{w_1}}\\<br>$$</p>
<p>All these gradients have been computed by applying the chain rule. Note that all the individual gradients on the right hand side of the equations mentioned above can be computed directly since the <em>numerators</em> of the gradients are explicit functions of the <em>denominators.</em></p>
<hr>
<h2 id="Computation-Graphs"><a href="#Computation-Graphs" class="headerlink" title="Computation Graphs"></a>Computation Graphs</h2><p>We could manually compute the gradients of our network as it was very simple. Imagine, what if you had a network with 152 layers. Or, if the network had multiple branches.</p>
<p>When we design software to implement neural networks, we want to come up with a way that can allow us to seamlessly compute the gradients, regardless of the architecture type so that the programmer doesn’t have to manually compute gradients when changes are made to the network.</p>
<p>We galvanize(激励,启发) this idea in form of a data structure called a <strong>Computation graph</strong>. A computation graph looks very similar to the diagram of the graph that we made in the image above. However, the nodes in a computation graph are basically <strong>operators</strong>. These operators are basically the mathematical operators except for one case, where we need to represent creation of a user-defined variable.</p>
<p>Notice that we have also denoted(表示…) the leaf variables <strong>a,w1,w2,w3,w4</strong> in the graph for sake of clarity. However, it should noted that they are not a part of the computation graph. What they represent in our graph is the special case for user-defined variables which we just covered as an exception.</p>
<div align=center>

<p><img src="https://jpccc.github.io/resource/pytorch/computation_graph.png" alt="img"></p>
</div>

<p>The variables, <em>b,c</em> and <em>d</em> are created as a result of mathematical operations, whereas variables <em>a, w1, w2, w3</em> and <em>w4</em> are initialised by the user itself. Since, they are not created by any mathematical operator, nodes corresponding to their creation is represented by their name itself. This is true for all the <em>leaf</em> nodes in the graph.</p>
<hr>
<h2 id="Computing-the-gradients"><a href="#Computing-the-gradients" class="headerlink" title="Computing the gradients"></a>Computing the gradients</h2><p>Now, we are ready to describe how we will compute gradients using a computation graph.</p>
<p>Each node of the computation graph, with the exception of(除了…外) leaf nodes, can be considered as a function which takes some inputs and produces an output. Consider the node of the graph which produces variable <em>d</em> from w4cand w3b. Therefore we can write,</p>
<p>$$<br>d=f(w_3b,w_4c)<br>$$</p>
<div align=center>

<p><img src="https://jpccc.github.io/resource/pytorch/d_mini.png" alt="img"></p>
</div>

<p>​                                                                                            <center>    d is output of function f(x,y) = x + y  </center></p>
<p>Now, we can easily compute the gradient of the ff with respect to it’s inputs, $\frac{\partial{f}}{\partial{w_3b}}$ and $\frac{\partial{f}}{\partial{w_4b}}$ (which are both 1). Now, label the edges coming into the nodes with their respective gradients like the following image.</p>
<div align=center>

<p><img src="https://jpccc.github.io/resource/pytorch/d_mini_grad.png" alt="img"></p>
</div>

<p>​                                                                                                            <center>    Local Gradients </center></p>
<p>We do it for the entire graph. The graph looks like this.</p>
<div align=center>


<p><img src="https://jpccc.github.io/resource/pytorch/full_graph-16491739555532.png" alt="img"></p>
</div>

<center>Back propagation in an Computational Graph</center>

<p>Following we describe the algorithm for computing derivative(微分) of any node in this graph with respect to the loss, LL. Let’s say we want to compute the derivative, $\frac{\partial{f}}{\partial{w_4}}$</p>
<ol>
<li>We first trace down all possible paths from <em>d</em> to $w_4$.</li>
<li>There is only one such path.</li>
<li>We multiply all the edges long this path.</li>
</ol>
<p>If you see, the product is precisely the same expression we derived using chain rule. If there is more than one path to a variable from <em>L</em> then, we multiply the edges along each path and then add them together. For example,$\frac{\partial{L}}{\partial{a}}$  is computed as</p>
<p>$$<br>\frac{\partial{L}}{\partial{w_4}} = \frac{\partial{L}}{\partial{d}}*\frac{\partial{d}}{\partial{b}}*\frac{\partial{b}}{\partial{a}} + \frac{\partial{L}}{\partial{d}}*\frac{\partial{d}}{\partial{c}}*\frac{\partial{c}}{\partial{a}}<br>$$</p>
<h2 id="PyTorch-Autograd"><a href="#PyTorch-Autograd" class="headerlink" title="PyTorch Autograd"></a>PyTorch Autograd</h2><p>Now we get what a computational graph is, let’s get back to PyTorch and understand how the above is implemented in PyTorch.</p>
<p><em><strong>(注意：对谁求导，对应的导数就保存在对应变量中)</strong></em></p>
<h3 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h3><p><code>Tensor</code> is a data structure which is a fundamental building block of PyTorch. <code>Tensor</code>s are pretty much like numpy arrays, except that unlike numpy, tensors are designed to take advantage of parallel computation capabilities of a GPU. A lot of Tensor syntax(语法) is similar to that of numpy arrays.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]:  <span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: tsr = torch.Tensor(<span class="number">3</span>,<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: tsr</span><br><span class="line">Out[<span class="number">3</span>]: </span><br><span class="line">tensor([[ <span class="number">0.0000e+00</span>,  <span class="number">0.0000e+00</span>,  <span class="number">8.4452e-29</span>, -<span class="number">1.0842e-19</span>,  <span class="number">1.2413e-35</span>],</span><br><span class="line">        [ <span class="number">1.4013e-45</span>,  <span class="number">1.2416e-35</span>,  <span class="number">1.4013e-45</span>,  <span class="number">2.3331e-35</span>,  <span class="number">1.4013e-45</span>],</span><br><span class="line">        [ <span class="number">1.0108e-36</span>,  <span class="number">1.4013e-45</span>,  <span class="number">8.3641e-37</span>,  <span class="number">1.4013e-45</span>,  <span class="number">1.0040e-36</span>]])</span><br></pre></td></tr></table></figure>

<p>One it’s own, <code>Tensor</code> is just like a numpy <code>ndarray</code>. A data structure that can let you do fast linear algebra options. If you want PyTorch to create a graph corresponding to these operations, you will have to set the <code>requires_grad</code> attribute of the <code>Tensor</code> to True.</p>
<p>The API can be a bit confusing here. There are multiple ways to initialise tensors in PyTorch. While some ways can let you explicitly define that the <code>requires_grad</code> in the constructor itself, others require you to set it manually after creation of the Tensor.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; t1 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>) </span><br><span class="line"></span><br><span class="line">&gt;&gt; t2 = torch.FloatTensor(<span class="number">3</span>,<span class="number">3</span>) <span class="comment"># No way to specify requires_grad while initiating </span></span><br><span class="line">&gt;&gt; t2.requires_grad = <span class="literal">True</span></span><br></pre></td></tr></table></figure>

<p><code>requires_grad</code> is contagious. It means that when a <code>Tensor</code> is created by operating on other <code>Tensor</code>s, the <code>requires_grad</code> of the resultant <code>Tensor</code> would be set <code>True</code> given at least one of the tensors used for creation has it’s <code>requires_grad</code> set to <code>True</code>.</p>
<p>Each <code>Tensor</code> has a something an attribute called <code>grad_fn</code><em>,</em> which refers to the <strong>mathematical operator that create the variable</strong>. If <code>requires_grad</code> is set to False, <code>grad_fn</code> would be None.</p>
<p>In our example where, $d=f(w_3b,w_4c)$, <em>d</em>‘s grad function would be the addition operator, since <em>f</em> adds it’s to input together. Notice, addition operator is also the node in our graph that output’s <em>d</em>. If our <code>Tensor</code> is a leaf node (initialised by the user), then the <code>grad_fn</code> is also None.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line">a = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">w1 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w2 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w3 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w4 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">b = w1*a </span><br><span class="line">c = w2*a</span><br><span class="line"></span><br><span class="line">d = w3*b + w4*c </span><br><span class="line"></span><br><span class="line">L = <span class="number">10</span> - d</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The grad fn for a is&quot;</span>, a.grad_fn)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The grad fn for d is&quot;</span>, d.grad_fn)</span><br></pre></td></tr></table></figure>

<p>If you run the code above, you get the following output.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">The grad fn <span class="keyword">for</span> a <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">The grad fn <span class="keyword">for</span> d <span class="keyword">is</span> &lt;AddBackward0 <span class="built_in">object</span> at <span class="number">0x1033afe48</span>&gt;</span><br></pre></td></tr></table></figure>

<p>One can use the member function <code>is_leaf</code> to determine whether a variable is a leaf <code>Tensor</code> or not.</p>
<h3 id="Function"><a href="#Function" class="headerlink" title="Function"></a>Function</h3><p>All mathematical operations in PyTorch are implemented by the <em>torch.nn.Autograd.Function</em> class. This class has two important member functions we need to look at.</p>
<ul>
<li>The first is it’s <em>forward</em>  function, which simply computes the output using it’s inputs.</li>
</ul>
<ul>
<li>The <code>backward</code> function takes the incoming gradient coming from the the part of the network in front of it. As you can see, the gradient to be backpropagated from a function <em>f</em> is basically the <strong>gradient that is backpropagated to f from the layers in front of it</strong> multiplied by <strong>the local gradient of the output of f with respect to it’s inputs</strong>（链式规则）. This is exactly what the <code>backward</code> function does.</li>
</ul>
<p>Let’s again understand with our example of<br>$$<br>d=f(w_3b,w_4c)<br>$$</p>
<ol>
<li><em>d</em> is our <code>Tensor</code> here. It’s <code>grad_fn</code> is <code>&lt;ThAddBackward&gt;</code><em>.</em> This is basically the addition operation since the function that creates <em>d</em> adds inputs.</li>
<li>The <code>forward</code> function of the it’s <code>grad_fn</code> receives the inputs $w_3b$ <em>and</em> $w_4c$ and adds them. This value is basically stored in the <em>d</em>.</li>
<li>The <code>backward</code> function of the <code>&lt;ThAddBackward&gt;</code> basically takes the the <strong>incoming gradient</strong> from the further layers as the input. This is basically $\frac{\partial{L}}{\partial{d}}$ coming along the edge leading from <em>L</em> to <em>d.</em> This gradient is also the gradient of <em>L</em> w.r.t to <em>d</em> and is stored in <code>grad</code> attribute of the <code>d</code>. It can be accessed by calling <code>d.grad</code><em>.</em></li>
<li>It then takes computes the local gradients $\frac{\partial{d}}{\partial{w_4c}}$and$\frac{\partial{d}}{\partial{w_3b}}$.</li>
<li>Then the backward function multiplies the incoming gradient with the <strong>locally computed gradients</strong> respectively and <em><strong>“<em><strong>sends</strong></em>“</strong></em> the gradients to it’s inputs by invoking the backward method of the <code>grad_fn</code> of their inputs.</li>
<li>For example, the <code>backward</code> function of <code>&lt;ThAddBackward&gt;</code> associated with <em>d</em> invokes(援引，调用) backward function of the <em>grad_fn</em> of the $w_4∗c$∗(Here, $w_4∗c$ is a intermediate Tensor, and it’s <em>grad_fn</em> is <code>&lt;ThMulBackward&gt;</code>. At time of invocation of the <code>backward</code> function, the gradient $\frac{\partial{L}}{\partial{d}}*\frac{\partial{d}}{\partial{w_4c}}$ is passed as the input.</li>
<li>Now, for the variable $w_4∗c$, $\frac{\partial{L}}{\partial{d}}*\frac{\partial{d}}{\partial{w_4c}}$ becomes the incoming gradient, $\frac{\partial{L}}{\partial{d}}$ was for $d$ in step 3 and the process repeats.</li>
</ol>
<p>Algorithmically, here’s how back propagation happens with a computation graph. (Not the actual implementation, only representative)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def backward (incoming_gradients):</span><br><span class="line">	self.Tensor.grad = incoming_gradients</span><br><span class="line"></span><br><span class="line">	for inp in self.inputs:</span><br><span class="line">		if inp.grad_fn is not None:</span><br><span class="line">			new_incoming_gradients = //</span><br><span class="line">			  incoming_gradient * local_grad(self.Tensor, inp)</span><br><span class="line">			</span><br><span class="line">			inp.grad_fn.backward(new_incoming_gradients)</span><br><span class="line">		else:</span><br><span class="line">			pass</span><br></pre></td></tr></table></figure>

<p>Here, <code>self.Tensor</code> is basically the <code>Tensor</code> created by Autograd.Function, which was <em>d</em> in our example.</p>
<p>Incoming gradients and local gradients have been described above.</p>
<hr>
<p>In order to compute derivatives in our neural network, we generally call <code>backward</code> on the <code>Tensor</code> representing our loss. Then, we backtrack through the graph starting from node representing the <code>grad_fn</code> of our loss.</p>
<p>As described above, the <code>backward</code> function is recursively called through out the graph as we backtrack. Once, we reach a leaf node, since the <code>grad_fn</code> is None, but stop backtracking through that path.</p>
<p>One thing to note here is that PyTorch gives an error if you call <code>backward()</code> on vector-valued Tensor. This means you can only call <code>backward</code> on a scalar valued Tensor. In our example, if we assume <code>a</code> to be a vector valued Tensor, and call <code>backward</code> on L, it will throw up an error.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line">a = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">w1 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w2 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w3 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w4 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">b = w1*a </span><br><span class="line">c = w2*a</span><br><span class="line"></span><br><span class="line">d = w3*b + w4*c </span><br><span class="line"></span><br><span class="line">L = (<span class="number">10</span> - d)</span><br><span class="line"></span><br><span class="line">L.backward()</span><br></pre></td></tr></table></figure>

<p>Running the above snippet results in the following error.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RuntimeError: grad can be implicitly created only <span class="keyword">for</span> scalar outputs</span><br></pre></td></tr></table></figure>

<p>This is because gradients can be computed with respect to scalar values by definition. You can’t exactly differentiate a vector with respect to another vector. The mathematical entity used for such cases is called a <strong>Jacobian,</strong> the discussion of which is beyond the scope of this article.</p>
<p>There are two ways to overcome this.</p>
<p>If you just make a small change in the above code setting <code>L</code> to be the sum of all the errors, our problem will be solved.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line">a = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">w1 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w2 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w3 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w4 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">b = w1*a </span><br><span class="line">c = w2*a</span><br><span class="line"></span><br><span class="line">d = w3*b + w4*c </span><br><span class="line"></span><br><span class="line"><span class="comment"># Replace L = (10 - d) by </span></span><br><span class="line">L = (<span class="number">10</span> -d).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">L.backward()</span><br></pre></td></tr></table></figure>

<p>Once that’s done, you can access the gradients by calling the <code>grad</code> attribute of <code>Tensor</code>.</p>
<p><strong>Second way is</strong>, for some reason have to absolutely call <code>backward</code> on a vector function, you can pass a <code>torch.ones</code> of size of shape of the tensor you are trying to call backward with.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Replace L.backward() with </span></span><br><span class="line">L.backward(torch.ones(L.shape))</span><br></pre></td></tr></table></figure>

<p>Notice how <code>backward</code> used to take incoming gradients as it’s input. Doing the above makes the <code>backward</code> think that incoming gradient are just Tensor of ones of same size as L, and it’s able to back propagate.</p>
<p>In this way, we can have gradients for every <code>Tensor</code> , and we can update them using Optimisation algorithm of our choice.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w1 = w1 - learning_rate * w1.grad</span><br></pre></td></tr></table></figure>

<p>And so on.</p>
<h2 id="How-are-PyTorch’s-graphs-different-from-TensorFlow-graphs"><a href="#How-are-PyTorch’s-graphs-different-from-TensorFlow-graphs" class="headerlink" title="How are PyTorch’s graphs different from TensorFlow graphs"></a>How are PyTorch’s graphs different from TensorFlow graphs</h2><p>PyTorch creates something called a <strong>Dynamic Computation Graph,</strong> which means that the graph is generated on the fly.</p>
<p>Until the <code>forward</code> function of a Variable is called, there exists no node for the <code>Tensor</code> <em>(<em>it’s <code>grad_fn</code></em>)</em> in the graph.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)   <span class="comment">#No graph yet, as a is a leaf</span></span><br><span class="line"></span><br><span class="line">w1 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)  <span class="comment">#Same logic as above</span></span><br><span class="line"></span><br><span class="line">b = w1*a   <span class="comment">#Graph with node `mulBackward` is created.</span></span><br></pre></td></tr></table></figure>

<p>The graph is created as a result of <code>forward</code> function of many <em>Tensors</em> being invoked. Only then, the buffers for the non-leaf nodes allocated for the graph and intermediate values (used for computing gradients later.  When you call <code>backward</code>, as the gradients are computed, these buffers (for non-leaf variables) are essentially freed, and the graph is <em>destroyed</em> ( In a sense, you can’t backpropagate through it since the buffers holding values to compute the gradients are gone).</p>
<p>Next time, you will call <code>forward</code> on the same set of tensors, <strong>the leaf node buffers from the previous run will be shared, while the non-leaf nodes buffers will be created again.</strong></p>
<p>If you call <code>backward</code> more than once on a graph with non-leaf nodes, you’ll be met with the following error.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=<span class="literal">True</span> when calling backward the first time.</span><br></pre></td></tr></table></figure>

<p>This is because the non-leaf buffers gets destroyed the first time <code>backward()</code> is called and hence, there’s no path to navigate to the leaves when <code>backward</code> is invoked the second time. You can undo this non-leaf buffer destroying behaviour by adding <code>retain_graph = True</code> argument to the <code>backward</code> function.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss.backward(retain_graph = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>If you do the above, you will be able to backpropagate again through the same graph and the gradients will be accumulated, i.e. the next you backpropagate, the gradients will be added to those already stored in the previous back pass.</p>
<hr>
<p>This is in contrast to the <em><strong>Static Computation Graphs</strong></em>, used by TensorFlow where the graph is declared <strong>before</strong> running the program. Then the graph is “run” by feeding values to the predefined graph.</p>
<p>The dynamic graph paradigm allows you to make changes to your network architecture <em>during</em> runtime, as a graph is created only when a piece of code is run.</p>
<p>This means a graph may be redefined during the lifetime for a program since you don’t have to define it beforehand.</p>
<p>This, however, is not possible with static graphs where graphs are created before running the program, and merely executed later.</p>
<p>Dynamic graphs also make debugging way easier since it’s easier to locate the source of your error.</p>
<h2 id="Some-Tricks-of-Trade"><a href="#Some-Tricks-of-Trade" class="headerlink" title="Some Tricks of Trade"></a>Some Tricks of Trade</h2><h3 id="requires-grad"><a href="#requires-grad" class="headerlink" title="requires_grad"></a>requires_grad</h3><p>This is an attribute of the <code>Tensor</code> class. By default, it’s False. It comes handy when you have to freeze some layers, and stop them from updating parameters while training. You can simply set the <code>requires_grad</code> to False, and these <code>Tensors</code> won’t participate in the computation graph.</p>
<div align=center>

<p><img src="https://jpccc.github.io/resource/pytorch/image-4.png" alt="img"></p>
</div>



<p>Thus, no gradient would be propagated to them, or to those layers which depend upon these layers for gradient flow <code>requires_grad</code>. When set to True, <code>requires_grad</code> is contagious meaning even if one operand of an operation has <code>requires_grad</code> set to True, so will the result.</p>
<h3 id="torch-no-grad"><a href="#torch-no-grad" class="headerlink" title="torch.no_grad()"></a>torch.no_grad()</h3><p>When we are computing gradients, we need to cache input values, and intermediate features as they maybe required to compute the gradient later.</p>
<p>The gradient of $ b=w_1∗a$ w.r.t it’s inputs w1w1 and aa is aa and w1w1 respectively. We need to store these values for gradient computation during the backward pass. This affects the memory footprint of the network.</p>
<p>While, we are performing inference, we don’t compute gradients, and thus, don’t need to store these values. Infact, no graph needs to be create during inference as it will lead to useless consumption of memory.</p>
<p>PyTorch offers a context manager, called <code>torch.no_grad</code> for this purpose.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad:</span><br><span class="line">	inference code goes here </span><br></pre></td></tr></table></figure>

<p>No graph is defined for operations executed under this context manager.</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Understanding how <em>Autograd</em> and computation graphs works can make life with PyTorch a whole lot easier. With our foundations rock solid, the next posts will detail how to create custom complex architectures, how to create custom data pipelines and more interesting stuff.</p>
<h2 id="Further-Reading"><a href="#Further-Reading" class="headerlink" title="Further Reading"></a>Further Reading</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.khanacademy.org/math/differential-calculus/dc-chain">Chain Rule</a></li>
<li><a target="_blank" rel="noopener" href="http://neuralnetworksanddeeplearning.com/chap2.html">Backpropagation</a></li>
</ol>

      
    </div>
    
  </div>
  
    
    <div class="copyright">
        <p><span>本文标题:</span><a href="/2022/04/06/pytorch%E8%AE%A1%E7%AE%97%E5%9B%BE/">Understanding Graphs, Automatic Differentiation and Autograd</a></p>
        <p><span>文章作者:</span><a href="/" title="回到主页">流沙</a></p>
        <p><span>发布时间:</span>2022-04-06, 00:08:00</p>
        <p><span>最后更新:</span>2022-04-06, 09:32:53</p>
        <p>
            <span>原始链接:</span><a class="post-url" href="/2022/04/06/pytorch%E8%AE%A1%E7%AE%97%E5%9B%BE/" title="Understanding Graphs, Automatic Differentiation and Autograd">https://jpccc.github.io/2022/04/06/pytorch%E8%AE%A1%E7%AE%97%E5%9B%BE/</a>
            <span class="copy-path" data-clipboard-text="原文: https://jpccc.github.io/2022/04/06/pytorch%E8%AE%A1%E7%AE%97%E5%9B%BE/　　作者: " title="点击复制文章链接"><i class="fa fa-clipboard"></i></span>
            <script> var clipboard = new Clipboard('.copy-path'); </script>
        </p>
        <p>
            <span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" title="CC BY-NC-SA 4.0 International" target = "_blank">"署名-非商用-相同方式共享 4.0"</a> 转载请保留原文链接及作者。
        </p>
    </div>



    <nav id="article-nav">
        
        
            <div id="article-nav-older" class="article-nav-title">
                <a href="/2021/10/28/Jensen-inequality/">
                    Jensen_inequality
                </a>
            </div>
        
    </nav>

  
</article>

    <div id="toc" class="toc-article">
        <strong class="toc-title">文章目录</strong>
        
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction"><span class="toc-number">1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Prerequisites"><span class="toc-number">2.</span> <span class="toc-text">Prerequisites</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Automatic-Differentiation"><span class="toc-number">3.</span> <span class="toc-text">Automatic Differentiation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#A-toy-example"><span class="toc-number">4.</span> <span class="toc-text">A toy example</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Computation-Graphs"><span class="toc-number">5.</span> <span class="toc-text">Computation Graphs</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Computing-the-gradients"><span class="toc-number">6.</span> <span class="toc-text">Computing the gradients</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#PyTorch-Autograd"><span class="toc-number">7.</span> <span class="toc-text">PyTorch Autograd</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Tensor"><span class="toc-number">7.1.</span> <span class="toc-text">Tensor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Function"><span class="toc-number">7.2.</span> <span class="toc-text">Function</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#How-are-PyTorch%E2%80%99s-graphs-different-from-TensorFlow-graphs"><span class="toc-number">8.</span> <span class="toc-text">How are PyTorch’s graphs different from TensorFlow graphs</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Some-Tricks-of-Trade"><span class="toc-number">9.</span> <span class="toc-text">Some Tricks of Trade</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#requires-grad"><span class="toc-number">9.1.</span> <span class="toc-text">requires_grad</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-no-grad"><span class="toc-number">9.2.</span> <span class="toc-text">torch.no_grad()</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Conclusion"><span class="toc-number">10.</span> <span class="toc-text">Conclusion</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Further-Reading"><span class="toc-number">11.</span> <span class="toc-text">Further Reading</span></a></li></ol>
        
    </div>
    <style>
        .left-col .switch-btn,
        .left-col .switch-area {
            display: none;
        }
        .toc-level-3 i,
        .toc-level-3 ol {
            display: none !important;
        }
    </style>

    <input type="button" id="tocButton" value="隐藏目录"  title="点击按钮隐藏或者显示文章目录">

    <script>
        yiliaConfig.toc = ["隐藏目录", "显示目录", !!"false"];
    </script>



    
<div class="share">
    

    
</div>







    




    <div class="scroll" id="post-nav-button">
        
            <a href="/" title="回到主页"><i class="fa fa-home"></i></a>
        

        <a title="文章列表"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a>

        
            <a href="/2021/10/28/Jensen-inequality/" title="下一篇: Jensen_inequality">
                <i class="fa fa-angle-right"></i>
            </a>
        
    </div>

    <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2022/04/06/pytorch%E8%AE%A1%E7%AE%97%E5%9B%BE/">Understanding Graphs, Automatic Differentiation and Autograd</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/28/Jensen-inequality/">Jensen_inequality</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/28/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0part1/">统计学习数学基础-1</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/25/pytorch/">pytorch</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/25/DroupOut/">DroupOut</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/25/BN%E7%AE%97%E6%B3%95/">BN算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/25/%E4%BA%92%E4%BF%A1%E6%81%AF/">互信息</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/22/algebra/">algebra</a></li><li class="post-list-item"><a class="post-list-link" href="/2021/10/17/hello-world/">Hello World</a></li></ul>




    <script>
        
    </script>
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2021-2022  liusha
            </div>
            <div class="footer-right">
                回首向来萧瑟处，归去，也无风雨也无晴。
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_uv">
  本站访客数<span id="busuanzi_value_site_uv"></span>人次
</span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="本页阅读量"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>





    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>