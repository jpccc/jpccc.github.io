<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>流沙</title>
  
  <subtitle>Artificial Intelligence</subtitle>
  <link href="https://jpccc.github.io/atom.xml" rel="self"/>
  
  <link href="https://jpccc.github.io/"/>
  <updated>2022-04-12T09:56:03.665Z</updated>
  <id>https://jpccc.github.io/</id>
  
  <author>
    <name> liusha</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>心得</title>
    <link href="https://jpccc.github.io/2030/04/07/%E5%BF%83%E5%BE%97/"/>
    <id>https://jpccc.github.io/2030/04/07/%E5%BF%83%E5%BE%97/</id>
    <published>2030-04-07T04:44:13.000Z</published>
    <updated>2022-04-12T09:56:03.665Z</updated>
    
    <content type="html"><![CDATA[<ol><li> 卷积层要求输入的图片格式为CHW的，而PIL中展示图片要求HWC的，所以需要用permute()函数调整一下。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;ol&gt;
&lt;li&gt; 卷积层要求输入的图片格式为CHW的，而PIL中展示图片要求HWC的，所以需要用permute()函数调整一下。&lt;/li&gt;
&lt;/ol&gt;
</summary>
      
    
    
    
    
    <category term="深度学习" scheme="https://jpccc.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://jpccc.github.io/2022/04/20/RCNN/"/>
    <id>https://jpccc.github.io/2022/04/20/RCNN/</id>
    <published>2022-04-20T15:02:07.728Z</published>
    <updated>2022-04-21T01:47:32.027Z</updated>
    
    <content type="html"><![CDATA[<h2 id="R-CNN（区域卷积神经网络）"><a href="#R-CNN（区域卷积神经网络）" class="headerlink" title="R-CNN（区域卷积神经网络）"></a>R-CNN（区域卷积神经网络）</h2><p><img src="E:\笔记\markdown\reference\picture\v2-97c647f8b4c67ca873174a8df5ccd04d_720w.jpg" alt="img"></p><p>​                                                                                                                            模型发展图</p><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a><strong>1. 引言</strong></h2><p>目标检测（Object Detection) 就是一种基于<strong>目标几何</strong>和<strong>统计特征</strong>的图像分割，它将目标的分割和识别合二为一，通俗点说就是给定一张图片要精确的定位到物体所在位置，并完成对物体类别的识别。其准确性和实时性是整个系统的一项重要能力。</p><p>R-CNN的全称是Region-CNN （区域卷积神经网络），是第一个成功将深度学习应用到目标检测上的算法。R-CNN基于卷积神经网络(CNN)，线性回归，和支持向量机(SVM)等算法，实现目标检测技术。</p><p>但是这个问题并没有想象的那么简单，首先物体的尺寸变化很大，物体摆放的角度不同，形态各异，而且可以出现在图片的任何地方，有些物体还具有多个类别。</p><h2 id="2-任务"><a href="#2-任务" class="headerlink" title="2. 任务"></a><strong>2. 任务</strong></h2><p>R-CNN主要就是用了做目标检测任务的。先简单了解下目标检测，我的通俗理解是对于给定图片精确的找到物体所在的位置，并且标注物体的类别(一张图像中含有一个或多个物体)。</p><p>输入：image</p><p>输出：类别标签（Category label）；位置（最小外接矩形 / Bounding Box）</p><p><strong>模型构思</strong></p><p>按分类问题对待可分为两个模块：</p><ul><li>模块一：提取物体区域（Region proposal）</li><li>模块二：对区域进行分类识别（Classification）</li></ul><p>主要难度： 在提取区域上需要面临 不同位置，不同尺寸，提取数量很多的问题。在分类识别方面主要面对CNN分类及计算量大的问题。</p><h2 id="3-传统方法-gt-R-CNN"><a href="#3-传统方法-gt-R-CNN" class="headerlink" title="3. 传统方法 -&gt; R-CNN"></a><strong>3. 传统方法 -&gt; R-CNN</strong></h2><h3 id="3-1-模型概述"><a href="#3-1-模型概述" class="headerlink" title="3.1 模型概述"></a><strong>3.1 模型概述</strong></h3><p>传统的目标检测方法大多以图像识别为基础。 一般可以在图片上使用穷举法选出所所有物体可能出现的区域框，对这些区域框提取特征并使用图像识别方法分类， 得到所有分类成功的区域后,通过**非极大值抑制(Non-maximumsuppression)**输出结果。</p><p>R-CNN遵循传统目标检测的思路，同样采用提取框，对每个框提取特征、图像分类、 非极大值抑制四个步骤进行目标检测。只不过在提取特征这一步，将传统的特征(如 SIFT、HOG 特征等)换成了深度卷积网络提取的特征。</p><h3 id="3-2-R-CNN详细步骤"><a href="#3-2-R-CNN详细步骤" class="headerlink" title="3.2 R-CNN详细步骤"></a><strong>3.2 R-CNN详细步骤</strong></h3><p><img src="E:\笔记\markdown\reference\picture\v2-bb2ab5bac50ba1e6bba523082031af2e_720w.jpg" alt="img"></p><p><strong>R-CNN的步骤如下（对应上图）：</strong></p><ol><li><strong>图像输入</strong> 输入待检测的图像。</li><li><strong>区域建议</strong>（Region proposals） 对第一步输入的图像进行区域框的选取。常用的方法是Selective Search EdgeBox，主要是利用图像的边缘、纹理、色彩、颜色变化等信息在图像中选取2000个可能存在包含物体的区域（这一步骤 选择可能存在物体的区域，跟分类无关 ，包含一个物体）。</li><li><strong>特征提取</strong> 使用CNN网络对选取的2000存在物体的潜在区域进行特征提取。但是可能存在一些问题，由于上一步Region proposals所提取出来的图像的尺寸大小是不一样的，我们需要卷积后输出的特征尺度是一样的，所以要将Region proposals选取的区域进行一定的缩放处理（warped region）成统一的227x227的大小，再送到CNN中特征提取。R-CNN特征提取用的网络是对ImageNet上的AlexNet（AlexNet网络详解）的CNN模型进行pre-train（以下有解释，可先行了解pre-train）得到的基本的网络模型。然后需要对网络进行fine-tune，这时网络结构需要一些修改，因为AlexNet是对1000个物体分类，fc7输出为1000，因此我们需要改为（class + 1）若类别数为20则应改为20+1=21个节点，加一的原因是对图像背景类识别，判断是不是背景。其他的都用AlexNet的网络结构fine-tune（全连接），其中包括五层卷积和两层全连接层。 （在这里使用的是ImageNet竞赛上面训练好的AlexNet模型去除最后两层全连接层的模型（也可以是VGG，GoogLeNet，ResNet等）。特征提取用的是卷积神经网络代替了传统的HOG特征，Haar特征等取特征的方法。）</li><li><strong>SVM分类</strong> 将提取出来的特征送入SVM分类器得到分类模型，在这里每个类别对应一个SVM分类器，如果有20个类别，则会有20个SVM分类器。对于每个类别的分类器只需要判断是不是这个类别的，如果同时多个结果为Positive则选择概率之最高的。</li><li><strong>Bounding Box Regression</strong> 这个回归模型主要是用来修正由第二步Region proposals得到的图像区域。同第四步的分类一样，每个类别对应一个Regression模型。这个Bounding Box Regression主要是为了精准定位。它所做的就是把旧的区域（SS算法生成的区域）<img src="https://www.zhihu.com/equation?tex=P^i=(P^i_x,P^i_y,P^i_w,P^i_h)"/> 重新映射到新的区域 <img src="https://www.zhihu.com/equation?tex=G^i=(G^i_x,G^i_y,G^i_w,G^i_h)" />，其中 - 中心位置 <img src="https://www.zhihu.com/equation?tex=(x,y)"/>宽高尺寸 (<img src="https://www.zhihu.com/equation?tex=(w,h)" />) 。</li></ol><p><img src="E:\笔记\markdown\reference\picture\v2-b272a111379a78135077a7b97225c39e_720w.jpg" alt="img"></p><p>上图中$W_*$ 是我们要学习的参数矩阵，一共对应四个参数各有一个$w_*$矩阵。 左图下面四个式子$t_*^i$分别是在直角坐标系和极坐标系下的比例关系。左图第一个式子中$t_*^i$ 对应下面四个式子，<img src="https://www.zhihu.com/equation?tex=W^T_*\phi_5(P^i)"/> 可以但是变化delta或是一个修正，后面的范式则是正则项。在测试的时候我们首先求得delta（右图最后一个式子)，之后分别求得四个区域参数。</p><ol start="6"><li><strong>使用非极大值抑制输出（针对于测试阶段）</strong> 可能几个区域选择的是同一个区域内的物体，为了获得无冗余的区域子集。通过使用非极大值抑制（loU&gt;=0.5）获取无冗余的区域子集。主要有以下几步：</li></ol><p>① 所有区域分值从大到小排列</p><p>② 剔除冗余，与最大分值区域loU&gt;=0.5的所有区域</p><p>③ 保留最大分值区域，剩余区域作为新的候选集</p><p><img src="E:\笔记\markdown\reference\picture\v2-42a254ec684dcd68d8a7d1af60cb8776_720w.jpg" alt="img"></p><h3 id="3-3-对以上出现的一些概念的解释："><a href="#3-3-对以上出现的一些概念的解释：" class="headerlink" title="3.3 对以上出现的一些概念的解释："></a><strong>3.3 对以上出现的一些概念的解释：</strong></h3><p><strong>3.3.1 pre-train</strong></p><p><strong>预训练</strong>，拿一个在其他训练集上面训练好的模型作为初始模型，共用层的参数是相同的已经在其他训练集上面训练好的。e.g. ImageNet是一个很大的数据集，它的1000个分类基本涵盖主要的物体识别，是比较权威的，在前卷积层特征提取已经比较成熟，我们就可以直接把模型的结构参数拿过来直接使用。</p><p><strong>3.3.2 fine-tune</strong></p><p><strong>再训练</strong>，把模型拿过来针对现在要解决问题的数据集再次训练，以更加适应现在数据集。e.g. 我们要分类的物体不包含在ImageNet分类中，则需要针对现在的数据集再训练，重新训练得到参数。</p><p><strong>3.3.3 IoU（Intersection over Union）</strong></p><p><img src="E:\笔记\markdown\reference\picture\v2-7dbc047d8d031c935e5f27509732af95_720w.jpg" alt="img"></p><img src="https://www.zhihu.com/equation?tex=IoU (Intersection\  over\ Union) = （A\bigcap B）/（A\bigcup B）= SI/(SA+SB-SI)"/><h3 id="3-4-一些问题"><a href="#3-4-一些问题" class="headerlink" title="3.4 一些问题"></a><strong>3.4 一些问题</strong></h3><p>R-CNN虽然不再像传统方法那样穷举，但R-CNN流程的第一步中对原始图片通过Selective Search提取的候选框region proposal多达2000个左右，而这2000个候选框每个框都需要进行CNN提特征+SVM分类，计算量很大，导致R-CNN检测速度很慢，一张图都需要47s。</p><p>那么有没有方法提速呢？答案是有的，这2000个region proposal不都是图像的一部分吗，那么我们完全可以对图像提一次卷积层特征，然后只需要将region proposal在原图的位置映射到卷积层特征图上，这样对于一张图像我们只需要提一次卷积层特征，然后将每个region proposal的卷积层特征输入到全连接层做后续操作。</p><p>但现在的问题是每个region proposal的尺度不一样，而全连接层输入必须是固定的长度，所以直接这样输入全连接层肯定是不行的。SPP Net恰好可以解决这个问题。</p><h2 id="4-SPP-Net网络"><a href="#4-SPP-Net网络" class="headerlink" title="4. SPP-Net网络"></a><strong>4. SPP-Net网络</strong></h2><p><strong>4.1 模型概述</strong></p><p><strong>SPP：Spatial Pyramid Pooling（空间金字塔池化）</strong> SPP-Net是出自2015年发表在IEEE上的论文-《Spatial Pyramid Pooling in Deep ConvolutionalNetworks for Visual Recognition》。</p><p>空间金字塔池化， ROI Pooling详解 点击这里：<a href="https://link.zhihu.com/?target=https://blog.csdn.net/H_hei/article/details/87298459">SPP</a>，<a href="https://link.zhihu.com/?target=https://blog.csdn.net/H_hei/article/details/89791176">ROI Pooling（感兴趣区域池化）</a></p><p>众所周知，CNN一般都含有卷积部分和全连接部分，其中，卷积层不需要固定尺寸的图像，而全连接层是需要固定大小的输入。所以当全连接层面对各种尺寸的输入数据时，就需要对输入数据进行crop（crop就是从一个大图扣出网络输入大小的patch，比如227×227），或warp（把一个边界框bounding box的内容resize成227×227）等一系列操作以统一图片的尺寸大小，比如224<em>224（ImageNet）、32</em>32(LenNet)、96*96等。</p><p><img src="E:\笔记\markdown\reference\picture\v2-0b9ff5f1d4c2e85a7fd288162328ae7d_720w.jpg" alt="img"></p><p>正如你在上图中看到的，在R-CNN中，“因为取出的区域大小各自不同，所以需要将每个Region Proposal缩放（warp）成统一的227x227的大小并输入到CNN”。</p><p>但warp/crop这种预处理，导致的问题要么被拉伸变形、要么物体不全，限制了识别精确度。没太明白？说句人话就是，一张16:9比例的图片你硬是要Resize成1:1的图片，你说图片失真不？</p><p>SPP Net的作者Kaiming He等人逆向思考，既然由于全连接FC层的存在，普通的CNN需要通过固定输入图片的大小来使得全连接层的输入固定。那借鉴卷积层可以适应任何尺寸，为何不能在卷积层的最后加入某种结构，使得后面全连接层得到的输入变成固定的呢？</p><p><img src="E:\笔记\markdown\reference\picture\v2-c165b9c20106ed1e660d50fa694f4428_720w.png" alt="img"></p><p><strong>4.2 两个特点</strong></p><p>它的特点有两个:</p><p>\1. 结合空间金字塔方法实现CNN的多尺度输入。SPP Net的第一个贡献就是在最后一个卷积层后，接入了金字塔池化层，保证传到下一层全连接层的输入固定。</p><p>换句话说，在普通的CNN机构中，输入图像的尺寸往往是固定的（比如224*224像素），输出则是一个固定维数的向量。SPP Net在普通的CNN结构中加入了ROI池化层（ROI Pooling），使得网络的输入图像可以是任意尺寸的，输出则不变，同样是一个固定维数的向量。</p><p>简言之，CNN原本只能固定输入、固定输出，CNN加上SSP之后，便能任意输入、固定输出。</p><p>ROI池化层一般跟在卷积层后面，此时网络的输入可以是任意尺度的，在SPP layer中每一个pooling的filter会根据输入调整大小，而SPP的输出则是固定维数的向量，然后给到全连接FC层。</p><p>\2. 只对原图提取一次卷积特征。在R-CNN中，每个候选框先resize到统一大小，然后分别作为CNN的输入，这样是很低效的。</p><p>而SPP Net根据这个缺点做了优化：只对原图进行一次卷积计算，便得到整张图的卷积特征feature map，然后找到每个候选框在feature map上的映射patch，将此patch作为每个候选框的卷积特征输入到SPP layer和之后的层，完成特征提取工作。</p><p>如此这般，R-CNN要对每个区域计算卷积，而SPPNet只需要计算一次卷积，从而节省了大量的计算时间，比R-CNN有一百倍左右的提速。</p><p><strong>4.3 总结： 2大改进</strong></p><ul><li>直接输入整图，所有区域共享卷积计算（一遍），在CNN输出上提取所有区域的特征</li><li>引入<strong>空间金字塔池化（Spatial Pyramid Pooling）</strong>，为不同的尺寸区域在CNN输出上提取特征，映射到固定尺寸的全连接层上。</li></ul><p>SPP-Net网络使用了SPP技术实现了① <strong>共享计算</strong> ② <strong>适应不同输入的尺寸</strong></p><p><strong>4.4 SPP-Net一些问题</strong></p><p>继承了R-CNN的问题</p><ul><li>① 需要存储大量特征</li><li>② 复杂的多阶段训练</li><li>③ 训练时间长</li></ul><h2 id="5-Fast-R-CNN网络"><a href="#5-Fast-R-CNN网络" class="headerlink" title="5 . Fast R-CNN网络"></a><strong>5 . Fast R-CNN网络</strong></h2><p>空间金字塔池化， ROI Pooling详解 点击这里：<a href="https://link.zhihu.com/?target=https://blog.csdn.net/H_hei/article/details/87298459">SPP</a>，<a href="https://link.zhihu.com/?target=https://blog.csdn.net/H_hei/article/details/89791176">ROI Pooling（感兴趣区域池化）</a></p><p><strong>5.1 改进</strong></p><ul><li>比R-CNN，SPP-Net更快的train/test，更高的准确率，召回率。</li><li>实现end-to-end（端对端）单阶段训练，使用多任务损失函数。</li><li>所有层都可以fine-tune</li><li>不需要离线存储特征文件</li></ul><p><strong>引入2个新技术：</strong></p><ul><li><a href="https://link.zhihu.com/?target=https://blog.csdn.net/H_hei/article/details/89791176">感兴趣区域池化层（Rol pooling layer）</a>其实就是是空间金字塔池化特殊形式，空间金字塔池化单层特例。（详细内容这里不再叙述）</li><li>多任务损失函数（Multi-task loss）</li></ul><p><strong>5.2 结构</strong></p><p><img src="E:\笔记\markdown\reference\picture\v2-cfa67a1e68faef876ad493a4d55c4f42_720w.jpg" alt="img"></p><p><strong>5.3 概述</strong></p><p>R-CNN框架图对比，可以发现主要有两处不同：一是最后一个卷积层后加了一个ROI pooling layer，二是损失函数使用了多任务损失函数(multi-task loss)，将边框回归Bounding Box Regression直接加入到CNN网络中训练。</p><p>(1) ROI pooling layer实际上是SPP-NET的一个精简版，SPP-NET对每个proposal使用了不同大小的金字塔映射，而ROI pooling layer只需要下采样到一个7x7的特征图。对于VGG16网络conv5_3有512个特征图，这样所有region proposal对应了一个7<em>7</em>512维度的特征向量作为全连接层的输入。</p><p>换言之，这个网络层可以把不同大小的输入映射到一个固定尺度的特征向量，而我们知道，conv、pooling、relu等操作都不需要固定size的输入，因此，在原始图片上执行这些操作后，虽然输入图片size不同导致得到的feature map尺寸也不同，不能直接接到一个全连接层进行分类，但是可以加入这个神奇的ROI Pooling层，对每个region都提取一个固定维度的特征表示，再通过正常的softmax进行类型识别。</p><p>(2) R-CNN训练过程分为了三个阶段，而Fast R-CNN直接使用softmax替代SVM分类，同时利用多任务损失函数边框回归也加入到了网络中，这样整个的训练过程是端到端的(除去Region Proposal提取阶段)。</p><p>也就是说，之前R-CNN的处理流程是先提proposal，然后CNN提取特征，之后用SVM分类器，最后再做bbox regression，而在Fast R-CNN中，作者巧妙的把bbox regression放进了神经网络内部，与region分类和并成为了一个multi-task模型，实际实验也证明，这两个任务能够共享卷积特征，并相互促进。</p><p><img src="E:\笔记\markdown\reference\picture\v2-203fc9626f0a0325957276d2efa45ca8_720w.jpg" alt="img"></p><p>所以，Fast-RCNN很重要的一个贡献是成功的让人们看到了Region Proposal + CNN这一框架实时检测的希望，原来多类检测真的可以在保证准确率的同时提升处理速度，也为后来的Faster R-CNN做下了铺垫。</p><p><em>画一画重点：</em></p><p>R-CNN有一些相当大的缺点（把这些缺点都改掉了，就成了Fast R-CNN）。</p><p>大缺点：由于每一个候选框都要独自经过CNN，这使得花费的时间非常多。</p><p>解决：共享卷积层，现在不是每一个候选框都当做输入进入CNN了，而是输入一张完整的图片，在第五个卷积层再得到每个候选框的特征</p><p>原来的方法：许多候选框（比如两千个）–&gt;CNN–&gt;得到每个候选框的特征–&gt;分类+回归</p><p>现在的方法：一张完整图片–&gt;CNN–&gt;得到每张候选框的特征–&gt;分类+回归</p><p>所以容易看见，Fast R-CNN相对于R-CNN的提速原因就在于：不过不像R-CNN把每个候选区域给深度网络提特征，而是整张图提一次特征，再把候选框映射到conv5上，而SPP只需要计算一次特征，剩下的只需要在conv5层上操作就可以了。</p><p>在性能上提升也是相当明显的：</p><p><img src="E:\笔记\markdown\reference\picture\v2-52673d1fc49a78c9b8c2e2acee26cca7_720w.jpg" alt="img"></p><p><strong>5.4 多任务损失函数</strong></p><p><strong>分类器loss：</strong></p><p><strong>bounding box回归L1 loss：</strong></p><p>多任务损失函数：</p><h2 id="6-Faster-R-CNN网络"><a href="#6-Faster-R-CNN网络" class="headerlink" title="6 . Faster R-CNN网络"></a><strong>6 . Faster R-CNN网络</strong></h2><p><img src="E:\笔记\markdown\reference\picture\v2-611e9da13b129bedc0034220947a0a0d_720w.jpg" alt="img"></p><p>同 Fast R-CNN 相比 Faster R-CNN引入了RPN（Region Proposal Network）即 <strong>Faster R-CNN = Fast R-CNN + RPN</strong></p><p><strong>6.1 RPN网络介绍参考：</strong></p><p><strong>重点：</strong></p><p><a href="https://link.zhihu.com/?target=https://blog.csdn.net/qq_36269513/article/details/80421990">RegionProposal Network)RPN网络结构及详解</a></p><p><strong>6.2 集成RPN网络（Region Proposal NetWork）的优点</strong></p><ul><li>取代离线Selective Search模块，解决性能瓶颈（之前提取的区域建议都是离线存储的）</li><li>进一步共享卷积层计算</li><li>基于Attention注意机制，引导Fast R-CNN关注区域</li><li>Region proposals量少质优</li><li>高准确率，召回率</li></ul><p><strong>6.3 RPN网络loss</strong></p><p><strong>6.4 训练过程（分步训练）</strong></p><p><strong>Step1 - 训练RPN网络</strong></p><ul><li>- 卷集层初始化 使用ImageNet上pre-train模型参数</li></ul><p><strong>Step2 - 训练Fast R-CNN网络</strong></p><ul><li>- 卷集层初始化 使用ImageNet上pre-train模型参数</li><li>- Region proposals由Step1的RPN生成</li></ul><p><strong>Step3 - 调优RPN网络</strong></p><ul><li>- 卷集层初始化 Fast R-CNN的卷积层参数</li><li>- 固定卷积层，finetune剩余层</li></ul><p><strong>Step4 - 调优Fast R-CNN网络</strong></p><ul><li>- 固定卷积层，finetune剩余层</li><li>- Region proposals由Step3的RPN生成</li></ul><h2 id="7-小结："><a href="#7-小结：" class="headerlink" title="7. 小结："></a><strong>7. 小结：</strong></h2><p>最后总结一下各大算法的步骤：</p><p><strong>RCNN</strong></p><ol><li>在图像中确定约1000-2000个候选框 (使用选择性搜索Selective Search)</li><li>每个候选框内图像块缩放至相同大小，并输入到CNN内进行特征提取</li><li>对候选框中提取出的特征，使用分类器判别是否属于一个特定类</li><li>对于属于某一类别的候选框，用回归器进一步调整其位置</li></ol><p><strong>Fast R-CNN</strong></p><ol><li>在图像中确定约1000-2000个候选框 (使用选择性搜索Selective Search)</li><li>对整张图片输进CNN，得到feature map</li><li>找到每个候选框在feature map上的映射patch，将此patch作为每个候选框的卷积特征输入到SPP layer和之后的层</li><li>对候选框中提取出的特征，使用分类器判别是否属于一个特定类</li><li>对于属于某一类别的候选框，用回归器进一步调整其位置</li></ol><p><strong>Faster R-CNN</strong></p><ol><li>对整张图片输进CNN，得到feature map</li><li>卷积特征输入到RPN，得到候选框的特征信息</li><li>对候选框中提取出的特征，使用分类器判别是否属于一个特定类</li><li>对于属于某一类别的候选框，用回归器进一步调整其位置</li></ol><blockquote><p><strong>简言之，即如本文开头所列</strong><br><strong>R-CNN（Selective Search + CNN + SVM）</strong><br><strong>SPP-net（ROI Pooling）</strong><br><strong>Fast R-CNN（Selective Search + CNN + ROI）</strong><br><strong>Faster R-CNN（RPN + CNN + ROI）</strong></p></blockquote><p>R-CNN, SPP-NET, Fast R-CNN, Faster R-CNN一路走来，基于深度学习目标检测的流程变得越来越精简，精度越来越高，速度也越来越快。</p><h2 id="8-参考："><a href="#8-参考：" class="headerlink" title="8. 参考："></a><strong>8. 参考：</strong></h2><p>算法之道 结构之法 ： <a href="https://link.zhihu.com/?target=https://blog.csdn.net/v_JULY_v/article/details/80170182">https://blog.csdn.net/v_JULY_v/article/details/80170182</a></p><p>RPN网络结构及详解： <a href="https://link.zhihu.com/?target=https://blog.csdn.net/qq_36269513/article/details/80421990">https://blog.csdn.net/qq_36269513/article/details/80421990</a></p><p>ROI Pooling：<a href="https://link.zhihu.com/?target=https://blog.csdn.net/H_hei/article/details/89791176">ROI Pooling（感兴趣区域池化）</a></p><p>SPP-Net： <a href="https://link.zhihu.com/?target=https://blog.csdn.net/H_hei/article/details/87298459">https://blog.csdn.net/H_hei/art</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;R-CNN（区域卷积神经网络）&quot;&gt;&lt;a href=&quot;#R-CNN（区域卷积神经网络）&quot; class=&quot;headerlink&quot; title=&quot;R-CNN（区域卷积神经网络）&quot;&gt;&lt;/a&gt;R-CNN（区域卷积神经网络）&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;E:\笔记\ma</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://jpccc.github.io/2022/04/20/faster%20rcnn/"/>
    <id>https://jpccc.github.io/2022/04/20/faster%20rcnn/</id>
    <published>2022-04-20T14:54:10.912Z</published>
    <updated>2022-05-05T06:40:44.510Z</updated>
    
    <content type="html"><![CDATA[<h1 id="RCNN-将CNN引入目标检测的开山之作"><a href="#RCNN-将CNN引入目标检测的开山之作" class="headerlink" title="RCNN- 将CNN引入目标检测的开山之作"></a>RCNN- 将CNN引入目标检测的开山之作</h1><p><a href="https://link.zhihu.com/?target=https://github.com/rbgirshick/rcnn">RCNN</a> (论文：Rich feature hierarchies for accurate object detection and semantic segmentation) 是将CNN方法引入目标检测领域， 大大提高了目标检测效果，可以说改变了目标检测领域的主要研究思路， 紧随其后的系列文章：<a href="https://link.zhihu.com/?target=https://github.com/rbgirshick/fast-rcnn">Fast RCNN</a>, <a href="https://link.zhihu.com/?target=https://github.com/ShaoqingRen/faster_rcnn">Faster RCNN</a> 。</p><p>【论文主要特点】（相对传统方法的改进）</p><ul><li>速度： 经典的目标检测算法使用滑动窗法依次判断所有可能的区域。本文则(采用Selective Search方法)预先提取一系列较可能是物体的候选区域，之后仅在这些候选区域上(采用CNN)提取特征，进行判断。</li><li>训练集： 经典的目标检测算法在区域中提取人工设定的特征。本文则采用深度网络进行特征提取。使用两个数据库： 一个较大的识别库（ImageNet ILSVC 2012）：标定每张图片中物体的类别。一千万图像，1000类。 一个较小的检测库（PASCAL VOC 2007）：标定每张图片中，物体的类别和位置，一万图像，20类。 本文使用识别库进行预训练得到CNN（有监督预训练），而后用检测库调优参数，最后在检测库上评测。</li></ul><p>看到这里也许你已经对很多名词很困惑，下面会解释。先来看看它的基本流程：</p><h2 id="【基本流程-】"><a href="#【基本流程-】" class="headerlink" title="【基本流程 ===================================】"></a>【基本流程 ===================================】</h2><p>RCNN算法分为4个步骤</p><ol><li>候选区域生成： 一张图像生成1K~2K个候选区域 （采用Selective Search 方法）</li><li>特征提取： 对每个候选区域，使用深度卷积网络提取特征 （CNN）</li><li>类别判断： 特征送入每一类的SVM 分类器，判别是否属于该类</li><li>位置精修： 使用回归器精细修正候选框位置</li></ol><p><img src="E:\笔记\markdown\reference\picture\v2-32e78b7f2e29c3e4e159a52ed38a6f73_720w.png" alt="img"></p><h2 id="【基础知识-】"><a href="#【基础知识-】" class="headerlink" title="【基础知识 ===================================】"></a>【基础知识 ===================================】</h2><h3 id="Selective-Search-主要思想"><a href="#Selective-Search-主要思想" class="headerlink" title="Selective Search 主要思想:"></a><strong><a href="https://link.zhihu.com/?target=http://koen.me/research/pub/uijlings-ijcv2013-draft.pdf">Selective Search</a> 主要思想:</strong></h3><ol><li>使用一种过分割手段，将图像分割成小区域 (1k~2k 个)</li><li>查看现有小区域，按照合并规则合并可能性最高的相邻两个区域。重复直到整张图像合并成一个区域位置</li><li>输出所有曾经存在过的区域，所谓候选区域</li></ol><p>其中合并规则如下： 优先合并以下四种区域：</p><ul><li>颜色（颜色直方图）相近的</li><li>纹理（梯度直方图）相近的</li><li>合并后总面积小的： 保证合并操作的尺度较为均匀，避免一个大区域陆续“吃掉”其他小区域 （例：设有区域a-b-c-d-e-f-g-h。较好的合并方式是：ab-cd-ef-gh -&gt; abcd-efgh -&gt; abcdefgh。 不好的合并方法是：ab-c-d-e-f-g-h -&gt;abcd-e-f-g-h -&gt;abcdef-gh -&gt; abcdefgh）</li><li>合并后，总面积在其BBOX中所占比例大的： 保证合并后形状规则。<br><img src="E:\笔记\markdown\reference\picture\v2-616af8e96637b9b3280e71e05877db59_720w.png" alt="img"></li></ul><p>上述四条规则只涉及区域的颜色直方图、梯度直方图、面积和位置。合并后的区域特征可以直接由子区域特征计算而来，速度较快。</p><h3 id="有监督预训练与无监督预训练"><a href="#有监督预训练与无监督预训练" class="headerlink" title="有监督预训练与无监督预训练:"></a><strong>有监督预训练与无监督预训练:</strong></h3><p>(1)无监督预训练(Unsupervised pre-training)</p><p>预训练阶段的样本不需要人工标注数据，所以就叫做无监督预训练。</p><p>(2)有监督预训练(Supervised pre-training)</p><p>所谓的有监督预训练也可以把它称之为迁移学习。比如你已经有一大堆标注好的人脸年龄分类的图片数据，训练了一个CNN，用于人脸的年龄识别。然后当你遇到新的项目任务时：人脸性别识别，那么这个时候你可以利用已经训练好的年龄识别CNN模型，去掉最后一层，然后其它的网络层参数就直接复制过来，继续进行训练，让它输出性别。这就是所谓的迁移学习，说的简单一点就是把一个任务训练好的参数，拿到另外一个任务，作为神经网络的初始参数值,这样相比于你直接采用随机初始化的方法，精度可以有很大的提高。</p><p>对于目标检测问题： 图片分类标注好的训练数据非常多，但是物体检测的标注数据却很少，如何用少量的标注数据，训练高质量的模型，这就是文献最大的特点，这篇论文采用了迁移学习的思想： 先用了ILSVRC2012这个训练数据库（这是一个图片分类训练数据库），先进行网络图片<strong>分类</strong>训练。这个数据库有大量的标注数据，共包含了1000种类别物体，因此预训练阶段CNN模型的输出是1000个神经元（当然也直接可以采用Alexnet训练好的模型参数）。</p><h3 id="重叠度（IOU）"><a href="#重叠度（IOU）" class="headerlink" title="重叠度（IOU）:"></a><strong>重叠度（IOU）:</strong></h3><p>物体检测需要定位出物体的bounding box，就像下面的图片一样，我们不仅要定位出车辆的bounding box 我们还要识别出bounding box 里面的物体就是车辆。</p><p><img src="E:\笔记\markdown\reference\picture\v2-0659a27df35fd2f62cd00127ca8d1a21_720w.png" alt="img"></p><p>对于bounding box的定位精度，有一个很重要的概念： 因为我们算法不可能百分百跟人工标注的数据完全匹配，因此就存在一个定位精度评价公式：IOU。 它定义了两个bounding box的重叠度，如下图所示</p><p><img src="E:\笔记\markdown\reference\picture\v2-6fe13f10a9cb286f06aa1e3e2a2b29bc_720w.png" alt="img"></p><p><img src="E:\笔记\markdown\reference\picture\v2-e26ffc0835bc30dede8d82989ef9e178_720w.png" alt="img"></p><p>就是矩形框A、B的重叠面积占A、B并集的面积比例。</p><h3 id="非极大值抑制（NMS）："><a href="#非极大值抑制（NMS）：" class="headerlink" title="非极大值抑制（NMS）："></a><strong>非极大值抑制（</strong>NMS<strong>）：</strong></h3><p>RCNN会从一张图片中找出n个可能是物体的矩形框，然后为每个矩形框做类别分类概率：</p><p><img src="E:\笔记\markdown\reference\picture\v2-19c03377416e437a288e29bd27e97c14_720w.png" alt="img"></p><p>就像上面的图片一样，定位一个车辆，最后算法就找出了一堆的方框，我们需要判别哪些矩形框是没用的。非极大值抑制的方法是：先假设有6个矩形框，根据分类器的类别分类概率做排序，假设从小到大属于车辆的概率 分别为A、B、C、D、E、F。</p><p>(1)从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;</p><p>(2)假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。</p><p>(3)从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。</p><p>就这样一直重复，找到所有被保留下来的矩形框。</p><p>非极大值抑制（NMS）顾名思义就是抑制不是极大值的元素，搜索局部的极大值。这个局部代表的是一个邻域，邻域有两个参数可变，一是邻域的维数，二是邻域的大小。这里不讨论通用的NMS算法，而于在目标检测中用于提取分数最高的窗口的。例如在行人检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到NMS来选取那些邻域里分数最高（是行人的概率最大），并且抑制那些分数低的窗口。</p><h3 id="VOC物体检测任务"><a href="#VOC物体检测任务" class="headerlink" title="VOC物体检测任务:"></a><strong>VOC物体检测任务:</strong></h3><p>相当于一个竞赛，里面包含了20个物体类别：<a href="https://link.zhihu.com/?target=http://host.robots.ox.ac.uk/pascal/VOC/voc2012/examples/index.html">PASCAL VOC2011 Example Images</a> 还有一个背景，总共就相当于21个类别，因此一会设计fine-tuning CNN的时候，我们softmax分类输出层为21个神经元。</p><h2 id="【各个阶段详解-】"><a href="#【各个阶段详解-】" class="headerlink" title="【各个阶段详解 ===================================】"></a>【各个阶段详解 ===================================】</h2><p>总体思路再回顾：</p><p>首先对每一个输入的图片产生近2000个不分种类的候选区域（region proposals），然后使用CNNs从每个候选框中提取一个固定长度的特征向量（4096维度），接着对每个取出的特征向量使用特定种类的线性SVM进行分类。也就是总个过程分为三个程序：<strong>a、找出候选框；b、利用CNN提取特征向量；c、利用SVM进行特征向量分类。</strong></p><p><img src="E:\笔记\markdown\reference\picture\v2-1738e9bdb129fea5d46d73218606aebd_720w.png" alt="img"></p><h3 id="候选框搜索阶段："><a href="#候选框搜索阶段：" class="headerlink" title="候选框搜索阶段："></a><strong>候选框搜索阶段：</strong></h3><p>当我们输入一张图片时，我们要搜索出所有可能是物体的区域，这里采用的就是前面提到的Selective Search方法，通过这个算法我们搜索出2000个候选框。然后从上面的总流程图中可以看到，搜出的候选框是矩形的，而且是大小各不相同。然而CNN对输入图片的大小是有固定的，如果把搜索到的矩形选框不做处理，就扔进CNN中，肯定不行。因此对于每个输入的候选框都需要缩放到固定的大小。下面我们讲解要怎么进行缩放处理，为了简单起见我们假设下一阶段CNN所需要的输入图片大小是个正方形图片227*227。因为我们经过selective search 得到的是矩形框，paper试验了两种不同的处理方法：</p><p>(1)各向异性缩放</p><p>这种方法很简单，就是不管图片的长宽比例，管它是否扭曲，进行缩放就是了，全部缩放到CNN输入的大小227*227，如下图(D)所示；</p><p><img src="E:\笔记\markdown\reference\picture\v2-59449e8409b943f384c4cc3bf789d8b9_720w.png" alt="img"></p><p>(2)各向同性缩放</p><p>因为图片扭曲后，估计会对后续CNN的训练精度有影响，于是作者也测试了“各向同性缩放”方案。有两种办法</p><p>A、先扩充后裁剪： 直接在原始图片中，把bounding box的边界进行扩展延伸成正方形，然后再进行裁剪；如果已经延伸到了原始图片的外边界，那么就用bounding box中的颜色均值填充；如上图(B)所示;</p><p>B、先裁剪后扩充：先把bounding box图片裁剪出来，然后用固定的背景颜色填充成正方形图片(背景颜色也是采用bounding box的像素颜色均值),如上图(C)所示;</p><p>对于上面的异性、同性缩放，文献还有个padding处理，上面的示意图中第1、3行就是结合了padding=0,第2、4行结果图采用padding=16的结果。经过最后的试验，作者发现采用各向异性缩放、padding=16的精度最高。</p><p>（备注：候选框的搜索策略作者也考虑过使用一个滑动窗口的方法，然而由于更深的网络，更大的输入图片和滑动步长，使得使用滑动窗口来定位的方法充满了挑战。）</p><h3 id="CNN特征提取阶段："><a href="#CNN特征提取阶段：" class="headerlink" title="CNN特征提取阶段："></a><strong>CNN特征提取阶段：</strong></h3><p><strong>1、算法实现</strong></p><p>a、网络结构设计阶段</p><p>网络架构两个可选方案：第一选择经典的Alexnet；第二选择VGG16。经过测试Alexnet精度为58.5%，VGG16精度为66%。VGG这个模型的特点是选择比较小的卷积核、选择较小的跨步，这个网络的精度高，不过计算量是Alexnet的7倍。后面为了简单起见，我们就直接选用Alexnet，并进行讲解；Alexnet特征提取部分包含了5个卷积层、2个全连接层，在Alexnet中p5层神经元个数为9216、 f6、f7的神经元个数都是4096，通过这个网络训练完毕后，最后提取特征每个输入候选框图片都能得到一个4096维的特征向量。</p><p><img src="E:\笔记\markdown\reference\picture\v2-03e65630d303565dba3a997911e72881_720w.png" alt="img"></p><p><img src="E:\笔记\markdown\reference\picture\v2-002f73d5bb38dfe66e39ff472aca6c31_720w.png" alt="img"></p><p>b、网络有监督预训练阶段 （图片数据库：ImageNet ILSVC ）</p><p>参数初始化部分：物体检测的一个难点在于，物体标签训练数据少，如果要直接采用随机初始化CNN参数的方法，那么目前的训练数据量是远远不够的。这种情况下，最好的是采用某些方法，把参数初始化了，然后在进行有监督的参数微调，这里文献采用的是有监督的预训练。所以paper在设计网络结构的时候，是直接用Alexnet的网络，然后连参数也是直接采用它的参数，作为初始的参数值，然后再fine-tuning训练。网络优化求解时采用随机梯度下降法，学习率大小为0.001；</p><p><img src="E:\笔记\markdown\reference\picture\v2-4a8097e292784ffaff747417b71c863d_720w.png" alt="img"></p><p>C、fine-tuning阶段 （图片数据库： PASCAL VOC）</p><p>我们接着采用 selective search 搜索出来的候选框 （PASCAL VOC 数据库中的图片） 继续对上面预训练的CNN模型进行fine-tuning训练。假设要检测的物体类别有N类，那么我们就需要把上面预训练阶段的CNN模型的最后一层给替换掉，替换成N+1个输出的神经元(加1，表示还有一个背景) (20 + 1bg = 21)，然后这一层直接采用参数随机初始化的方法，其它网络层的参数不变；接着就可以开始继续SGD训练了。开始的时候，SGD学习率选择0.001，在每次训练的时候，我们batch size大小选择128，其中32个事正样本、96个事负样本。</p><p><img src="E:\笔记\markdown\reference\picture\v2-728cc0822b07a6db24468698463efb89_720w.png" alt="img"></p><p>关于正负样本问题：</p><p>一张照片我们得到了2000个候选框。然而人工标注的数据一张图片中就只标注了正确的bounding box，我们搜索出来的2000个矩形框也不可能会出现一个与人工标注完全匹配的候选框。因此在CNN阶段我们需要用IOU为2000个bounding box打标签。如果用selective search挑选出来的候选框与物体的人工标注矩形框（PASCAL VOC的图片都有人工标注）的重叠区域IoU大于0.5，那么我们就把这个候选框标注成物体类别（正样本），否则我们就把它当做背景类别（负样本）。</p><p><img src="E:\笔记\markdown\reference\picture\v2-f67cd928e318ec00bc6047075c88e0b8_720w.png" alt="img"></p><p>（备注： 如果不针对特定任务进行fine-tuning，而是把CNN当做特征提取器，卷积层所学到的特征其实就是基础的共享特征提取层，就类似于SIFT算法一样，可以用于提取各种图片的特征，而f6、f7所学习到的特征是用于针对特定任务的特征。打个比方：对于人脸性别识别来说，一个CNN模型前面的卷积层所学习到的特征就类似于学习人脸共性特征，然后全连接层所学习的特征就是针对性别分类的特征了）</p><p><strong>2. 疑惑点</strong>： CNN训练的时候，本来就是对bounding box的物体进行识别分类训练，在训练的时候最后一层softmax就是分类层。那么为什么作者闲着没事干要先用CNN做特征提取（提取fc7层数据），然后再把提取的特征用于训练svm分类器？</p><p>这个是因为svm训练和cnn训练过程的正负样本定义方式各有不同，导致最后采用CNN softmax输出比采用svm精度还低。事情是这样的，cnn在训练的时候，对训练数据做了比较宽松的标注，比如一个bounding box可能只包含物体的一部分，那么我也把它标注为正样本，用于训练cnn；采用这个方法的主要原因在于因为CNN容易过拟合，所以需要大量的训练数据，所以在CNN训练阶段我们是对Bounding box的位置限制条件限制的比较松(IOU只要大于0.5都被标注为正样本了)；然而svm训练的时候，因为svm适用于少样本训练，所以对于训练样本数据的IOU要求比较严格，我们只有当bounding box把整个物体都包含进去了，我们才把它标注为物体类别，然后训练svm，具体请看下文。</p><h3 id="SVM训练、测试阶段"><a href="#SVM训练、测试阶段" class="headerlink" title="SVM训练、测试阶段"></a><strong>SVM训练、测试阶段</strong></h3><p>训练阶段：</p><p>这是一个二分类问题，我么假设我们要检测车辆。我们知道只有当bounding box把整量车都包含在内，那才叫正样本；如果bounding box 没有包含到车辆，那么我们就可以把它当做负样本。但问题是当我们的检测窗口只有部分包含物体，那该怎么定义正负样本呢？作者测试了IOU阈值各种方案数值0,0.1,0.2,0.3,0.4,0.5。最后通过训练发现，如果选择IOU阈值为0.3效果最好（选择为0精度下降了4个百分点，选择0.5精度下降了5个百分点）,即当重叠度小于0.3的时候，我们就把它标注为负样本。一旦CNN f7层特征被提取出来，那么我们将为每个物体类训练一个svm分类器。当我们用CNN提取2000个候选框，可以得到2000**4096这样的特征向量矩阵，然后我们只需要把这样的一个矩阵与svm权值矩阵4096*N点乘(N为分类类别数目，因为我们训练的N个svm，每个svm包含了4096个权值w)，就可以得到结果了。</p><p><img src="E:\笔记\markdown\reference\picture\v2-3ef21dd028fd210f92107c1ded528045_720w.png" alt="img"></p><p>得到的特征输入到SVM进行分类看看这个feature vector所对应的region proposal是需要的物体还是无关的实物(background) 。</p><p> 排序，canny边界检测之后就得到了我们需要的bounding-box。</p><blockquote><p>再回顾总结一下：整个系统分为三个部分：1.产生不依赖与特定类别的region proposals，这些region proposals定义了一个整个检测器可以获得的候选目标2.一个大的卷积神经网络，对每个region产生一个固定长度的特征向量3.一系列特定类别的线性SVM分类器。</p></blockquote><p>位置精修： 目标检测问题的衡量标准是重叠面积：许多看似准确的检测结果，往往因为候选框不够准确，重叠面积很小。故需要一个位置精修步骤。 回归器：对每一类目标，使用一个线性脊回归器进行精修。正则项λ=10000。 输入为深度网络pool5层的4096维特征，输出为xy方向的缩放和平移。 训练样本：判定为本类的候选框中和真值重叠面积大于0.6的候选框。</p><p><img src="E:\笔记\markdown\reference\picture\v2-7e2c472157f6a4028db9f8ba3c0eb744_720w.png" alt="img"></p><p>测试阶段：</p><p>使用selective search的方法在测试图片上提取2000个region propasals ，将每个region proposals归一化到227x227，然后在CNN中正向传播，将最后一层得到的特征提取出来。然后对于每一个类别，使用为这一类训练的SVM分类器对提取的特征向量进行打分，得到测试图片中对于所有region proposals的对于这一类的分数，再使用贪心的非极大值抑制（NMS）去除相交的多余的框。再对这些框进行canny边缘检测，就可以得到bounding-box(then B-BoxRegression)。</p><p>（非极大值抑制（NMS）先计算出每一个bounding box的面积，然后根据score进行排序，把score最大的bounding box作为选定的框，计算其余bounding box与当前最大score与box的IoU，去除IoU大于设定的阈值的bounding box。然后重复上面的过程，直至候选bounding box为空，然后再将score小于一定阈值的选定框删除得到这一类的结果（然后继续进行下一个分类）。作者提到花费在region propasals和提取特征的时间是13s/张-GPU和53s/张-CPU，可以看出时间还是很长的，不能够达到及时性。</p><p>本文主要整理自以下文章：</p><ul><li><a href="https://link.zhihu.com/?target=http://blog.csdn.net/u011534057/article/details/51240387">RCNN学习笔记(0):rcnn简介</a></li><li><a href="https://link.zhihu.com/?target=http://blog.csdn.net/u011534057/article/details/51218218">RCNN学习笔记(1):Rich feature hierarchies for accurate object detection and semantic segmentation</a></li><li>《Rich feature hierarchies for Accurate Object Detection and Segmentation》</li><li>《Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition》</li></ul><h1 id="SPPnet"><a href="#SPPnet" class="headerlink" title="SPPnet"></a>SPPnet</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>沿着上一篇RCNN的思路，我们继续探索目标检测的痛点，其中RCNN使用CNN作为特征提取器，首次使得目标检测跨入深度学习的阶段。但是RCNN对于每一个区域候选都需要首先将图片放缩到固定的尺寸（224*224），然后为每个区域候选提取CNN特征。容易看出这里面存在的一些性能瓶颈：</p><ul><li>速度瓶颈：重复为每个region proposal提取特征是极其费时的，Selective Search对于每幅图片产生2K左右个region proposal，也就是意味着一幅图片需要经过2K次的完整的CNN计算得到最终的结果。</li><li>性能瓶颈：对于所有的region proposal防缩到固定的尺寸会导致我们不期望看到的几何形变，而且由于速度瓶颈的存在，不可能采用多尺度或者是大量的数据增强去训练模型。</li></ul><p>但是为什么CNN需要固定的输入呢？CNN网络可以分解为卷积网络部分以及全连接网络部分。我们知道卷积网络的参数主要是卷积核，完全能够适用任意大小的输入，并且能够产生任意大小的输出。但是全连接层部分不同，全连接层部分的参数是神经元对于所有输入的连接权重，也就是说输入尺寸不固定的话，全连接层参数的个数都不能固定。</p><p>何凯明团队的SPPNet给出的解决方案是，既然只有全连接层需要固定的输入，那么我们在全连接层前加入一个网络层，让他对任意的输入产生固定的输出不就好了吗？一种常见的想法是对于最后一层卷积层的输出pooling一下，但是这个pooling窗口的尺寸及步伐设置为相对值，也就是输出尺寸的一个比例值，这样对于任意输入经过这层后都能得到一个固定的输出。SPPnet在这个想法上继续加入SPM的思路，SPM其实在传统的机器学习特征提取中很常用，主要思路就是对于一副图像分成若干尺度的一些块，比如一幅图像分成1份，4份，8份等。然后对于每一块提取特征然后融合在一起，这样就可以兼容多个尺度的特征啦。SPPNet首次将这种思想应用在CNN中，对于卷积层特征我们也先给他分成不同的尺寸，然后每个尺寸提取一个固定维度的特征，最后拼接这些特征不就是一个固定维度的输入了吗？</p><p><img src="E:\笔记\markdown\reference\picture\v2-da59abcbe56803aeeef24ffb5131ce83_720w.png" alt="img"></p><p>上面这个图可以看出SPPnet和RCNN的区别，首先是输入不需要放缩到指定大小。其次是增加了一个空间金字塔池化层，还有最重要的一点是每幅图片只需要提取一次特征。</p><p>通过上述方法虽然解决了CNN输入任意大小图片的问题，但是还是需要重复为每个region proposal提取特征啊，能不能我们直接根据region proposal定位到他在卷积层特征的位置，然后直接对于这部分特征处理呢？答案是肯定的，我们将在下一章节介绍。</p><h2 id="网络细节"><a href="#网络细节" class="headerlink" title="网络细节"></a>网络细节</h2><ul><li><strong>卷积层特征图</strong></li></ul><p><img src="E:\笔记\markdown\reference\picture\v2-523707e94ccb850ca4c23cc94054a144_720w.png" alt="img"></p><p>SPPNet通过可视化Conv5层特征，发现卷积特征其实保存了空间位置信息（数学推理中更容易发现这点），并且每一个卷积核负责提取不同的特征，比如C图175、55卷积核的特征，其中175负责提取窗口特征，55负责提取圆形的类似于车轮的特征。我们可以通过传统的方法聚集这些特征，例如词袋模型或是空间金字塔的方法。</p><ul><li><strong>空间金字塔池化层</strong></li></ul><p><img src="E:\笔记\markdown\reference\picture\v2-62c008799df798656236258c64082340_720w.png" alt="img"></p><p>上图的空间金字塔池化层是SPPNet的核心，其主要目的是对于任意尺寸的输入产生固定大小的输出。思路是对于任意大小的feature map首先分成16、4、1个块，然后在每个块上最大池化，池化后的特征拼接得到一个固定维度的输出。以满足全连接层的需要。不过因为不是针对于目标检测的，所以输入的图像为一整副图像。</p><ul><li><strong>SPPNet应用于图像分类</strong></li></ul><p>SPPNet的能够接受任意尺寸图片的输入，但是训练难点在于所有的深度学习框架都需要固定大小的输入，因此SPPNet做出了多阶段多尺寸训练方法。在每一个epoch的时候，我们先将图像放缩到一个size，然后训练网络。训练完整后保存网络的参数，然后resize 到另外一个尺寸，并在之前权值的基础上再次训练模型。相比于其他的CNN网络，SPPNet的优点是可以方便地进行多尺寸训练，而且对于同一个尺度，其特征也是个空间金字塔的特征，综合了多个特征的空间多尺度信息。</p><ul><li><strong>SPPNet应用于目标检测</strong></li></ul><p><img src="E:\笔记\markdown\reference\picture\v2-d68eaa673b48c3176eb48b3cb16a761f_720w.png" alt="img"></p><p>SPPNet理论上可以改进任何CNN网络，通过空间金字塔池化，使得CNN的特征不再是单一尺度的。但是SPPNet更适用于处理目标检测问题，首先是网络可以介绍任意大小的输入，也就是说能够很方便地多尺寸训练。其次是空间金字塔池化能够对于任意大小的输入产生固定的输出，这样使得一幅图片的多个region proposal提取一次特征成为可能。SPPNet的做法是：</p><ol><li>首先通过selective search产生一系列的region proposal，参见：<a href="https://zhuanlan.zhihu.com/p/27467369">目标检测（1）-Selective Search - 知乎专栏</a></li><li>然后训练多尺寸识别网络用以提取区域特征，其中处理方法是每个尺寸的最短边大小在尺寸集合中：</li></ol><p><img src="E:\笔记\markdown\reference\picture\v2-237603b04a4f5f801924219f4fdfad99_720w.png" alt="img"></p><p>训练的时候通过上面提到的多尺寸训练方法，也就是在每个epoch中首先训练一个尺寸产生一个model，然后加载这个model并训练第二个尺寸，直到训练完所有的尺寸。空间金字塔池化使用的尺度为：1<em>1，2</em>2，3<em>3，6</em>6，一共是50个bins。</p><p>3.在测试时，每个region proposal选择能使其包含的像素个数最接近224*224的尺寸，提取相 应特征。</p><p>由于我们的空间金字塔池化可以接受任意大小的输入，因此对于每个region proposal将其映射到feature map上，然后仅对这一块feature map进行空间金字塔池化就可以得到固定维度的特征用以训练CNN了。关于从region proposal映射到feature map的细节我们待会儿去说。</p><p>4.训练SVM，BoundingBox回归</p><p>这部分和RCNN完全一致，参见：<a href="https://zhuanlan.zhihu.com/p/27473413">目标检测（2）-RCNN - 知乎专栏</a></p><ul><li><strong>实验结果</strong></li></ul><p>其中单一尺寸训练结果低于RCNN1.2%，但是速度是其102倍，5个尺寸的训练结果与RCNN相当，其速度为RCNN的38倍。</p><ul><li><strong>如何从一个region proposal 映射到feature map的位置？</strong></li></ul><p>SPPNet通过角点尽量将图像像素映射到feature map感受野的中央，假设每一层的padding都是p/2，p为卷积核大小。对于feature map的一个像素（x’,y’），其实际感受野为：（Sx‘，Sy’），其中S为之前所有层步伐的乘积。然后对于region proposal的位置，我们获取左上右下两个点对应的feature map的位置，然后取特征就好了。左上角映射为：</p><p><img src="E:\笔记\markdown\reference\picture\v2-8c5eddc9f856822aad5ae8d030ce1779_720w.png" alt="img"></p><p>右下角映射为：</p><p><img src="E:\笔记\markdown\reference\picture\v2-7a4ce0c60b8fcac5eb7ffe365f99572e_720w.png" alt="img"></p><p>当然，如果padding大小不一致，那么就需要计算相应的偏移值啦。</p><h2 id="存在的不足"><a href="#存在的不足" class="headerlink" title="存在的不足"></a>存在的不足</h2><p>和RCNN一样，SPP也需要训练CNN提取特征，然后训练SVM分类这些特征。需要巨大的存储空间，并且分开训练也很复杂。而且selective search的方法提取特征是在CPU上进行的，相对于GPU来说还是比较慢的。针对这些问题的改进，我们将在Fast RCNN以及Faster RCNN中介绍，敬请期待。</p><h1 id="Fast-RCNN"><a href="#Fast-RCNN" class="headerlink" title="Fast RCNN"></a>Fast RCNN</h1><p>Fast-RCNN是基于RCNN和SPPnet的进一步改进。Fast-RCNN针对RCNN的以下三点缺点做了改进：</p><ol><li>RCNN是多阶段的模型，使用selective search算法筛选候选区域-&gt;<a href="https://www.zhihu.com/search?q=%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:%22338211515%22%7D">卷积神经网络</a>提取特征值-&gt;SVM判断类型，非常繁琐。</li><li>RCNN训练非常消耗时间和内存。</li><li>RCNN的识别速度很慢。</li></ol><h2 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h2><p><img src="E:\笔记\markdown\reference\picture\v2-ced13fd5cc13bcccfcaf3afa20dce95e_b.jpg" alt="img">Fast-RCNN整体结构图</p><p><strong>从上图可以看到，相比起RCNN，Fast-RCNN使用全连接层替代了SVM来识别物体，并且Fast-RCNN摒弃了以前每一个候选区域分别放入卷积神经网络进行特征提取的方法，将整个图片直接放入卷积神经网络提取特征，避免了重复计算，提高了检测的速度。</strong></p><p>上面这个图片可能有点抽象，下面这个图片摘自<a href="https://link.zhihu.com/?target=https://blog.csdn.net/shenxiaolu1984/article/details/51036677">博客</a>，更加清晰的展示了网络的细节：</p><p><img src="E:\笔记\markdown\reference\picture\v2-d6b522d5eb9ff43c3bcd66c8448844d1_b.jpg" alt="img"></p><p>​                                                                                                                Fast-RCNN网络结构图</p><ul><li>一张（224,224,3）的图片进入网络（VGG16）进行<strong>提取特征。</strong></li><li>进入ROI Pooling层</li><li>再经过两个output都为4096维的全连接层</li><li>分别经过output各为21和84维的全连接层（并列的，前者是分类输出，有21个种类；后者是回归输出，输出边框的位置[x,y,w,h], 21*4=84）</li></ul><p>下面这张来自<a href="https://link.zhihu.com/?target=https://www.robots.ox.ac.uk/~tvg/publications/talks/fast-rcnn-slides.pdf">Fast-RCNN的PPT</a>图可能更加清晰一些：</p><p><img src="E:\笔记\markdown\reference\picture\v2-be21dff7c814eb9d470d35805f965ebb_b.jpg" alt="img">Fast-RCNN结构图</p><p>从上图可以看到，卷积不再是对每个region proposal（候选区域）进行，而是直接对整张图像，这样减少了很多重复计算。原来RCNN是对每个候选区域分别做卷积，因为一张图像中有2000左右的候选区域，肯定相互之间的重叠率很高，因此产生重复计算。</p><p><strong>每一个候选区域都是输入图片的一张子图，那么每一个候选区域都对应<a href="https://www.zhihu.com/search?q=%E7%89%B9%E5%BE%81%E5%9B%BE&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:%22338211515%22%7D">特征图</a>中的一个部分。</strong></p><h2 id="ROI"><a href="#ROI" class="headerlink" title="ROI"></a>ROI</h2><p>使用Selective Search算法对输入图片进行提取候选区域，候选区域的大小不一，所以对应特征图的区域也不一样，而全连接层的输入是固定的，这样就无法直接输入到全连接层中。如何将其变化成统一的格式呢？</p><p>ROI Pooling的作用是对不同大小的候选区域，从最后卷积层输出的特征图提取大小固定的特征图。</p><p>使用ROI Pooling层将每个候选区域均匀分成M×N块，对每块进行Max Pooling。将所有输出值组合起来便形成固定大小为H×W的特征图。这样就可以将特征图上大小不一的候选区域转变为大小统一的数据，送入下一层。图片来自<a href="https://link.zhihu.com/?target=https://blog.csdn.net/shenxiaolu1984/article/details/51036677">博客</a>。</p><p><img src="E:\笔记\markdown\reference\picture\v2-95e27f746e16ee25da88871c3114ff76_b.jpg" alt="img">ROI Pooling示意图</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><strong>总损失</strong>如下：</p><img src='https://www.zhihu.com/equation?tex=L(p, u, t^u , v) = Lcls(p, u) + λ[u ≥ 1]Lloc(t^u, v)'><p>损失函数分为两个部分：<img src='https://www.zhihu.com/equation?tex=Lcls'> 表示分类损失，<img src='https://www.zhihu.com/equation?tex=Lloc'> 表示定位损失(回归损失).</p><p>λ用于判断背景，如果分类不是背景，λ=1</p><p>分类如果是背景λ=0，则不考虑定位损失,即：</p><p>$\begin{equation} L=\left{ \begin{aligned} Lcls+λLloc&amp; &amp; {u不为背景}\ Lcls &amp; &amp; {u为背景}\ \end{aligned} \right. \end{equation}$</p><p><strong>分类损失：</strong></p><p>对于分类的全连接神经网络，它将产生由Softmax产生的概率p=(p0,p1…,pk)。这里K=20,所以共有21个概率值。</p><p><img src='https://www.zhihu.com/equation?tex= Lcls(p, u)=−log(p_u)'>，其中<img src='https://www.zhihu.com/equation?tex=p_u'> 是真实类型预测的那个概率。也就是说这里只计算一个 ，其他概率值不算。</p><p><strong>定位损失：</strong></p><p>对于bounding-box回归的全连接网络，它产生的是位置信息（x,y,w,h），分类神经网络每一个概率值，它都对应有一个位置信息。所以网络最后输出维度为：21*4=84。 </p><p><img src="E:\笔记\markdown\reference\picture\equation-165157845950121.svg+xml" alt="[公式]"> 表示第k个种类所对应的位置信息。</p><p><img src="E:\笔记\markdown\reference\picture\equation-165157845950122.svg+xml" alt="[公式]"> 表示真实的位置信息</p><p><img src="E:\笔记\markdown\reference\picture\in{x%2Cy%2Cw%2Ch}}{smooth_{L_i}(t_i^u-v_i)}.svg+xml" alt="[公式]"> </p><p>其中smooth函数:</p><p><img src="E:\笔记\markdown\reference\picture\end{equation}-165157845950123.svg+xml" alt="[公式]"> </p><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>看一下训练时候的整体结构图：</p><p><img src="E:\笔记\markdown\reference\picture\v2-5fbec4583ca50103e33d421463d3a3a0_b.jpg" alt="img"></p><p>在调优训练时，每一个mini-batch中首先加入N张完整图片，而后加入从N张图片中选取的R个候选框。这R个候选框可以复用N张图片前5个阶段的网络特征。<strong>实际选择N=2， R=128</strong></p><p><strong>训练数据构成</strong></p><p>N张完整图片以50%概率水平翻转。</p><p>R个候选框的构成方式如下：</p><table><thead><tr><th>类别</th><th>比例</th><th>方式</th></tr></thead><tbody><tr><td>前景</td><td>25%</td><td>与某个真值重叠在[0.5,1]的候选框</td></tr><tr><td>背景</td><td>75%</td><td>与真值重叠的最大值在[0.1,0.5)的候选框</td></tr></tbody></table><h2 id="SVD加速"><a href="#SVD加速" class="headerlink" title="SVD加速"></a>SVD加速</h2><p>分类和位置调整都是通过全连接层实现的，假设全连接层参数为W，尺寸u × v 一次前向传播即为：Y=Wx,计算复次数为u × v</p><p><img src="E:\笔记\markdown\reference\picture\v2-7b188e68f559382e0e1ddce4d4dfb837_b.jpg" alt="img">网络中各个部分计算所占用的时间</p><p>从上图可以看出，两个全连接层（fc6、fc7）占用了44.9%的时间，非常消耗时间。</p><p>为此，论文中对W进行SVD分解，并用前t个特征值近似：</p><p><img src="E:\笔记\markdown\reference\picture\equation-165157845950124.svg+xml" alt="[公式]"> </p><p>原来的前向传播分解成两步： <img src="E:\笔记\markdown\reference\picture\equation-165157845950125.svg+xml" alt="[公式]"> </p><p>计算复杂度变为u × t + v × t=(u + v) × t</p><p>在实现时，相当于把一个全连接层拆分成两个，中间以一个<a href="https://www.zhihu.com/search?q=%E4%BD%8E%E7%BB%B4%E6%95%B0%E6%8D%AE&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:%22338211515%22%7D">低维数据</a>相连。</p><p><img src="E:\笔记\markdown\reference\picture\v2-3d11b57e6904c18bc8bd89efc4a06bda_b.jpg" alt="img"></p><p>使用SVD加速后，各部分使用时间如下所示：</p><p><img src="E:\笔记\markdown\reference\picture\v2-8781d554a612c8a2c6686f4ac0df7a22_b.jpg" alt="img"></p><p>​                                                                使用SVD加速后，网络中各个部分计算所占用的时间</p><p>可以看到加速确实有效果了。</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="E:\笔记\markdown\reference\picture\v2-ff6a2918f64d1977fad99bb840facd7b_b.jpg" alt="img"></p><p><img src="E:\笔记\markdown\reference\picture\v2-f0af16bde4eb2271920696e00ed5d465_b.jpg" alt="img"></p><h2 id="参考连接："><a href="#参考连接：" class="headerlink" title="参考连接："></a>参考连接：</h2><p><a href="https://link.zhihu.com/?target=https://www.robots.ox.ac.uk/~tvg/publications/talks/fast-rcnn-slides.pdf">https://www.robots.ox.ac.uk/~tvg/publications/talks/fast-rcnn-slides.pdf</a></p><p><a href="https://link.zhihu.com/?target=https://blog.csdn.net/shenxiaolu1984/article/details/51036677">https://blog.csdn.net/s</a>henxiaolu1984/article/details/51036677</p><p><a href="https://link.zhihu.com/?target=https://blog.csdn.net/u014380165/article/details/72851319">Fast RCNN算法详解_AI之路-CSDN博客_fast rcnn</a></p><p><a href="https://link.zhihu.com/?target=https://www.cnblogs.com/CZiFan/p/9903518.html">Fast R-CNN（理解） - CZiFan - 博客园</a></p><p><a href="https://link.zhihu.com/?target=https://blog.csdn.net/u014380165/article/details/72851319">Fast RCNN算法详解_AI之路-CSDN博客_fast rcnn</a></p><h1 id="Faster-RCNN"><a href="#Faster-RCNN" class="headerlink" title="Faster RCNN"></a>Faster RCNN</h1><p>经过R-CNN和Fast RCNN的积淀，Ross B. Girshick在2016年提出了新的Faster RCNN，在结构上，Faster RCNN已经将特征抽取(feature extraction)，proposal提取，bounding box regression(rect refine)，classification都整合在了一个网络中，使得综合性能有较大提高，在检测速度方面尤为明显。</p><p><img src="E:\笔记\markdown\reference\picture\v2-c0172be282021a1029f7b72b51079ffe_720w.jpg" alt="img">图1 Faster RCNN基本结构（来自原论文）</p><p>依作者看来，如图1，Faster RCNN其实可以分为4个主要内容：</p><ol><li>Conv layers。作为一种CNN网络目标检测方法，Faster RCNN首先使用一组基础的conv+relu+pooling层提取image的feature maps。该feature maps被共享用于后续RPN层和全连接层。</li><li>Region Proposal Networks。RPN网络用于生成region proposals。该层通过softmax判断anchors属于positive或者negative，再利用bounding box regression修正anchors获得精确的proposals。</li><li>Roi Pooling。该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。</li><li>Classification。利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。</li></ol><p>所以本文以上述4个内容作为切入点介绍Faster R-CNN网络。</p><p>图2展示了python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构，可以清晰的看到该网络对于一副任意大小PxQ的图像：</p><ul><li>首先缩放至固定大小MxN，然后将MxN图像送入网络；</li><li>而Conv layers中包含了13个conv层+13个relu层+4个pooling层；</li><li>RPN网络首先经过3x3卷积，再分别生成positive anchors和对应bounding box regression偏移量，然后计算出proposals；</li><li>而Roi Pooling层则利用proposals从feature maps中提取proposal feature送入后续全连接和softmax网络作classification（即分类proposal到底是什么object）。</li></ul><p><img src="E:\笔记\markdown\reference\picture\v2-e64a99b38f411c337f538eb5f093bdf3_720w.jpg" alt="img">图2 faster_rcnn_test.pt网络结构 （pascal_voc/VGG16/faster_rcnn_alt_opt/faster_rcnn_test.pt）</p><p><em>本文不会讨论任何关于R-CNN家族的历史，分析清楚最新的Faster R-CNN就够了，并不需要追溯到那么久。实话说我也不了解R-CNN，更不关心。有空不如看看新算法。</em></p><p>新出炉的pytorch官方Faster RCNN代码导读：</p><p><a href="https://zhuanlan.zhihu.com/p/145842317">捋一捋pytorch官方FasterRCNN代码1308 赞同 · 21 评论文章<img src="E:\笔记\markdown\reference\picture\v2-dc78d0ad1012ad65040bdb7eb657f381_180x120.jpg" alt="img"></a></p><h2 id="1-Conv-layers"><a href="#1-Conv-layers" class="headerlink" title="1 Conv layers"></a>1 Conv layers</h2><p>Conv layers包含了conv，pooling，relu三种层。以python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构为例，如图2，Conv layers部分共有13个conv层，13个relu层，4个pooling层。这里有一个非常容易被忽略但是又无比重要的信息，在Conv layers中：</p><ol><li>所有的conv层都是：kernel_size=3，pad=1，stride=1</li><li>所有的pooling层都是：kernel_size=2，pad=0，stride=2</li></ol><p>为何重要？在Faster RCNN Conv layers中对所有的卷积都做了扩边处理（ pad=1，即填充一圈0），导致原图变为 (M+2)x(N+2)大小，再做3x3卷积后输出MxN 。正是这种设置，导致Conv layers中的conv层不改变输入和输出矩阵大小。如图3：</p><p><img src="E:\笔记\markdown\reference\picture\v2-3c772e9ed555eb86a97ef9c08bf563c9_720w.jpg" alt="img">图3 卷积示意图</p><p>类似的是，Conv layers中的pooling层kernel_size=2，stride=2。这样每个经过pooling层的MxN矩阵，都会变为(M/2)x(N/2)大小。综上所述，在整个Conv layers中，conv和relu层不改变输入输出大小，只有pooling层使输出长宽都变为输入的1/2。</p><p>那么，一个MxN大小的矩阵经过Conv layers固定变为(M/16)x(N/16)！这样Conv layers生成的feature map中都可以和原图对应起来。</p><h2 id="2-Region-Proposal-Networks-RPN"><a href="#2-Region-Proposal-Networks-RPN" class="headerlink" title="2 Region Proposal Networks(RPN)"></a>2 Region Proposal Networks(RPN)</h2><p>经典的检测方法生成检测框都非常耗时，如OpenCV adaboost使用滑动窗口+图像金字塔生成检测框；或如R-CNN使用SS(Selective Search)方法生成检测框。而Faster RCNN则抛弃了传统的滑动窗口和SS方法，直接使用RPN生成检测框，这也是Faster R-CNN的巨大优势，能极大提升检测框的生成速度。</p><p><img src="E:\笔记\markdown\reference\picture\v2-1908feeaba591d28bee3c4a754cca282_720w.jpg" alt="img">图4 RPN网络结构</p><p>上图4展示了RPN网络的具体结构。可以看到RPN网络实际分为2条线，上面一条通过softmax分类anchors获得positive和negative分类，下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。而最后的Proposal层则负责综合positive anchors和对应bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能。</p><h3 id="2-1-多通道图像卷积基础知识介绍"><a href="#2-1-多通道图像卷积基础知识介绍" class="headerlink" title="2.1 多通道图像卷积基础知识介绍"></a>2.1 多通道图像卷积基础知识介绍</h3><p>在介绍RPN前，还要多解释几句基础知识，已经懂的看官老爷跳过就好。</p><ol><li>对于单通道图像+单卷积核做卷积，第一章中的图3已经展示了；</li><li>对于多通道图像+多卷积核做卷积，计算方式如下：</li></ol><p><img src="E:\笔记\markdown\reference\picture\v2-8d72777321cbf1336b79d839b6c7f9fc_720w.jpg" alt="img">图5 多通道卷积计算方式</p><p>如图5，输入有3个通道，同时有2个卷积核。对于每个卷积核，先在输入3个通道分别作卷积，再将3个通道结果加起来得到卷积输出。所以对于某个卷积层，无论输入图像有多少个通道，输出图像通道数总是等于卷积核数量！</p><p>对多通道图像做1x1卷积，其实就是将输入图像于每个通道乘以卷积系数后加在一起，即相当于把原图像中本来各个独立的通道“联通”在了一起。</p><h3 id="2-2-anchors"><a href="#2-2-anchors" class="headerlink" title="2.2 anchors"></a>2.2 anchors</h3><p>提到RPN网络，就不能不说anchors。所谓anchors，实际上就是一组由rpn/generate_anchors.py生成的矩形。直接运行作者demo中的generate_anchors.py可以得到以下输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[[ -84.  -40.   99.   55.]</span><br><span class="line"> [-176.  -88.  191.  103.]</span><br><span class="line"> [-360. -184.  375.  199.]</span><br><span class="line"> [ -56.  -56.   71.   71.]</span><br><span class="line"> [-120. -120.  135.  135.]</span><br><span class="line"> [-248. -248.  263.  263.]</span><br><span class="line"> [ -36.  -80.   51.   95.]</span><br><span class="line"> [ -80. -168.   95.  183.]</span><br><span class="line"> [-168. -344.  183.  359.]]</span><br></pre></td></tr></table></figure><p>其中每行的4个值 <img src="E:\笔记\markdown\reference\picture\equation.svg+xml" alt="[公式]"> 表矩形左上和右下角点坐标。9个矩形共有3种形状，长宽比为大约为 <img src="E:\笔记\markdown\reference\picture}.svg+xml" alt="[公式]"> 三种，如图6。实际上通过anchors就引入了检测中常用到的多尺度方法。</p><p><img src="E:\笔记\markdown\reference\picture\v2-7abead97efcc46a3ee5b030a2151643f_720w.jpg" alt="img">图6 anchors示意图</p><p>注：关于上面的anchors size，其实是根据检测图像设置的。在python demo中，会把任意大小的输入图像reshape成800x600（即图2中的M=800，N=600）。再回头来看anchors的大小，anchors中长宽1:2中最大为352x704，长宽2:1中最大736x384，基本是cover了800x600的各个尺度和形状。</p><p>那么这9个anchors是做什么的呢？借用Faster RCNN论文中的原图，如图7，遍历Conv layers计算获得的feature maps，为每一个点都配备这9种anchors作为初始的检测框。这样做获得检测框很不准确，不用担心，后面还有2次bounding box regression可以修正检测框位置。</p><p><img src="E:\笔记\markdown\reference\picture\v2-c93db71cc8f4f4fd8cfb4ef2e2cef4f4_720w.jpg" alt="img">图7</p><p>解释一下上面这张图的数字。</p><ol><li>在原文中使用的是ZF model中，其Conv Layers中最后的conv5层num_output=256，对应生成256张特征图，所以相当于feature map每个点都是256-dimensions</li><li>在conv5之后，做了rpn_conv/3x3卷积且num_output=256，相当于每个点又融合了周围3x3的空间信息（猜测这样做也许更鲁棒？反正我没测试），同时256-d不变（如图4和图7中的红框）</li><li>假设在conv5 feature map中每个点上有k个anchor（默认k=9），而每个anhcor要分positive和negative，所以每个点由256d feature转化为cls=2•k scores；而每个anchor都有(x, y, w, h)对应4个偏移量，所以reg=4•k coordinates</li><li>补充一点，全部anchors拿去训练太多了，训练程序会在合适的anchors中<strong>随机</strong>选取128个postive anchors+128个negative anchors进行训练（什么是合适的anchors下文5.1有解释）</li></ol><p>注意，在本文讲解中使用的VGG conv5 num_output=512，所以是512d，其他类似。</p><p><strong>其实RPN最终就是在原图尺度上，设置了密密麻麻的候选Anchor。然后用cnn去判断哪些Anchor是里面有目标的positive anchor，哪些是没目标的negative anchor。所以，仅仅是个二分类而已！</strong></p><p>那么Anchor一共有多少个？原图800x600，VGG下采样16倍，feature map每个点设置9个Anchor，所以：</p><p><img src="E:\笔记\markdown\reference\picture\tag{1}.svg+xml" alt="[公式]"></p><p>其中ceil()表示向上取整，是因为VGG输出的feature map size= 50*38。</p><p><img src="E:\笔记\markdown\reference\picture\v2-4b15828dfee19be726835b671748cc4d_720w.jpg" alt="img">图8 Gernerate Anchors</p><h3 id="2-3-softmax判定positive与negative"><a href="#2-3-softmax判定positive与negative" class="headerlink" title="2.3 softmax判定positive与negative"></a>2.3 softmax判定positive与negative</h3><p>一副MxN大小的矩阵送入Faster RCNN网络后，到RPN网络变为(M/16)x(N/16)，不妨设 W=M/16，H=N/16。在进入reshape与softmax之前，先做了1x1卷积，如图9：</p><p><img src="E:\笔记\markdown\reference\picture\v2-1ab4b6c3dd607a5035b5203c76b078f3_720w.jpg" alt="img">图9 RPN中判定positive/negative网络结构</p><p>该1x1卷积的caffe prototxt定义如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;rpn_cls_score&quot;</span><br><span class="line">  type: &quot;Convolution&quot;</span><br><span class="line">  bottom: &quot;rpn/output&quot;</span><br><span class="line">  top: &quot;rpn_cls_score&quot;</span><br><span class="line">  convolution_param &#123;</span><br><span class="line">    num_output: 18   # 2(positive/negative) * 9(anchors)</span><br><span class="line">    kernel_size: 1 pad: 0 stride: 1</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到其num_output=18，也就是经过该卷积的输出图像为WxHx18大小（注意第二章开头提到的卷积计算方式）。这也就刚好对应了feature maps每一个点都有9个anchors，同时每个anchors又有可能是positive和negative，所有这些信息都保存WxHx(9*2)大小的矩阵。为何这样做？后面接softmax分类获得positive anchors，也就相当于初步提取了检测目标候选区域box（一般认为目标在positive anchors中）。</p><p>那么为何要在softmax前后都接一个reshape layer？其实只是为了便于softmax分类，至于具体原因这就要从caffe的实现形式说起了。在caffe基本数据结构blob中以如下形式保存数据：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">blob=[batch_size, channel，height，width]</span><br></pre></td></tr></table></figure><p>对应至上面的保存positive/negative anchors的矩阵，其在caffe blob中的存储形式为[1, 2x9, H, W]。而在softmax分类时需要进行positive/negative二分类，所以reshape layer会将其变为[1, 2, 9xH, W]大小，即单独“腾空”出来一个维度以便softmax分类，之后再reshape回复原状。贴一段caffe softmax_loss_layer.cpp的reshape函数的解释，非常精辟：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;Number of labels must match number of predictions; &quot;</span></span><br><span class="line"><span class="string">&quot;e.g., if softmax axis == 1 and prediction shape is (N, C, H, W), &quot;</span></span><br><span class="line"><span class="string">&quot;label count (number of labels) must be N*H*W, &quot;</span></span><br><span class="line"><span class="string">&quot;with integer values in &#123;0, 1, ..., C-1&#125;.&quot;</span>;</span><br></pre></td></tr></table></figure><p>综上所述，RPN网络中利用anchors和softmax初步提取出positive anchors作为候选区域（另外也有实现用sigmoid代替softmax，输出[1, 1, 9xH, W]后接sigmoid进行positive/negative二分类，原理一样）。</p><h3 id="2-4-bounding-box-regression原理"><a href="#2-4-bounding-box-regression原理" class="headerlink" title="2.4 bounding box regression原理"></a>2.4 bounding box regression原理</h3><p>如图9所示绿色框为飞机的Ground Truth(GT)，红色为提取的positive anchors，即便红色的框被分类器识别为飞机，但是由于红色的框定位不准，这张图相当于没有正确的检测出飞机。所以我们希望采用一种方法对红色的框进行微调，使得positive anchors和GT更加接近。</p><p><img src="E:\笔记\markdown\reference\picture\v2-93021a3c03d66456150efa1da95416d3_720w.jpg" alt="img">图10</p><p>对于窗口一般使用四维向量 <img src="E:\笔记\markdown\reference\picture\equation-16504664534921.svg+xml" alt="[公式]"> 表示，分别表示窗口的中心点坐标和宽高。对于图 11，红色的框A代表原始的positive Anchors，绿色的框G代表目标的GT，我们的目标是寻找一种关系，使得输入原始的anchor A经过映射得到一个跟真实窗口G更接近的回归窗口G’，即：</p><ul><li>给定anchor <img src="E:\笔记\markdown\reference\picture\equation-16504664534922.svg+xml" alt="[公式]"> 和 <img src="E:\笔记\markdown\reference\picture\equation-16504664534933.svg+xml" alt="[公式]"></li><li>寻找一种变换<strong>F，</strong>使得：<img src="E:\笔记\markdown\reference\picture\equation-16504664534934.svg+xml" alt="[公式]">，其中<img src="E:\笔记\markdown\reference\picture\equation-16504664534935.svg+xml" alt="[公式]"></li></ul><p><img src="E:\笔记\markdown\reference\picture\v2-ea7e6e48662bfa68ec73bdf32f36bb85_720w.jpg" alt="img">图11</p><p>那么经过何种变换<strong>F</strong>才能从图10中的anchor A变为G’呢？ 比较简单的思路就是:</p><ul><li>先做平移</li></ul><p><img src="E:\笔记\markdown\reference\picture\tag{2}.svg+xml" alt="[公式]"></p><p><img src="E:\笔记\markdown\reference\picture\tag{3}.svg+xml" alt="[公式]"></p><ul><li>再做缩放</li></ul><p><img src="E:\笔记\markdown\reference\picture\tag{4}.svg+xml" alt="[公式]"></p><p><img src="E:\笔记\markdown\reference\picture\tag{5}.svg+xml" alt="[公式]"></p><p>观察上面4个公式发现，需要学习的是 <img src="E:\笔记\markdown\reference\picture\equation-16504664534936.svg+xml" alt="[公式]"> 这四个变换。当输入的anchor A与GT相差较小时，可以认为这种变换是一种线性变换， 那么就可以用线性回归来建模对窗口进行微调（注意，只有当anchors A和GT比较接近时，才能使用线性回归模型，否则就是复杂的非线性问题了）。</p><p>接下来的问题就是如何通过线性回归获得 <img src="E:\笔记\markdown\reference\picture\equation-16504664534936.svg+xml" alt="[公式]"> 了。线性回归就是给定输入的特征向量X, 学习一组参数W, 使得经过线性回归后的值跟真实值Y非常接近，即<img src="E:\笔记\markdown\reference\picture\equation-16504664534937.svg+xml" alt="[公式]">。对于该问题，输入X是cnn feature map，定义为Φ；同时还有训练传入A与GT之间的变换量，即<img src="E:\笔记\markdown\reference\picture\equation-16504664534938.svg+xml" alt="[公式]">。输出是<img src="E:\笔记\markdown\reference\picture\equation.svg+xml" alt="[公式]">四个变换。那么目标函数可以表示为：</p><p><img src="E:\笔记\markdown\reference\picture\tag{6}.svg+xml" alt="[公式]"></p><p>其中 <img src="E:\笔记\markdown\reference\picture\phi(A).svg+xml" alt="[公式]"> 是对应anchor的feature map组成的特征向量， <img src="E:\笔记\markdown\reference\picture\equation-16504664534949.svg+xml" alt="[公式]"> 是需要学习的参数， <img src="E:\笔记\markdown\reference\picture\equation-165046645349410.svg+xml" alt="[公式]"> 是得到的预测值（*表示 x，y，w，h，也就是每一个变换对应一个上述目标函数）。为了让预测值 <img src="E:\笔记\markdown\reference\picture\equation-16504669589211.svg+xml" alt="[公式]"> 与真实值 <img src="E:\笔记\markdown\reference\picture\equation-165046645349411.svg+xml" alt="[公式]"> 差距最小，设计L1损失函数：</p><p><img src="E:\笔记\markdown\reference\picture\tag{7}.svg+xml" alt="[公式]"></p><p>函数优化目标为：</p><p><img src="E:\笔记\markdown\reference\picture\tag{8}.svg+xml" alt="[公式]"></p><p>为了方便描述，这里以L1损失为例介绍，而真实情况中一般使用soomth-L1损失。</p><p>需要说明，只有在GT与需要回归框位置比较接近时，才可近似认为上述线性变换成立。<br>说完原理，对应于Faster RCNN原文，positive anchor与ground truth之间的平移量 <img src="E:\笔记\markdown\reference\picture\equation-165046645349412.svg+xml" alt="[公式]"> 与尺度因子 <img src="E:\笔记\markdown\reference\picture\equation-165046645349413.svg+xml" alt="[公式]"> 如下：</p><p><img src="E:\笔记\markdown\reference\picture\tag{9}.svg+xml" alt="[公式]"></p><p><img src="E:\笔记\markdown\reference\picture\tag{10}.svg+xml" alt="[公式]"></p><p>对于训练bouding box regression网络回归分支，输入是cnn feature Φ，监督信号是Anchor与GT的差距 <img src="E:\笔记\markdown\reference\picture\equation-165046645349414.svg+xml" alt="[公式]">，即训练目标是：输入 Φ的情况下使网络输出与监督信号尽可能接近。那么当bouding box regression工作时，再输入Φ时，回归网络分支的输出就是每个Anchor的平移量和变换尺度 <img src="E:\笔记\markdown\reference\picture\equation-16504669589212.svg+xml" alt="[公式]">，显然即可用来修正Anchor位置了。</p><h3 id="2-5-对proposals进行bounding-box-regression"><a href="#2-5-对proposals进行bounding-box-regression" class="headerlink" title="2.5 对proposals进行bounding box regression"></a>2.5 对proposals进行bounding box regression</h3><p>在了解bounding box regression后，再回头来看RPN网络第二条线路，如图12。</p><p><img src="E:\笔记\markdown\reference\picture\v2-8241c8076d60156248916fe2f1a5674a_720w.jpg" alt="img">图12 RPN中的bbox reg</p><p>先来看一看上图11中1x1卷积的caffe prototxt定义：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;rpn_bbox_pred&quot;</span><br><span class="line">  type: &quot;Convolution&quot;</span><br><span class="line">  bottom: &quot;rpn/output&quot;</span><br><span class="line">  top: &quot;rpn_bbox_pred&quot;</span><br><span class="line">  convolution_param &#123;</span><br><span class="line">    num_output: 36   # 4 * 9(anchors)</span><br><span class="line">    kernel_size: 1 pad: 0 stride: 1</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到其 num_output=36，即经过该卷积输出图像为WxHx36，在caffe blob存储为[1, 4x9, H, W]，这里相当于feature maps每个点都有9个anchors，每个anchors又都有4个用于回归的</p><p><img src="E:\笔记\markdown\reference\picture\tag{11}.svg+xml" alt="[公式]"></p><p>变换量。</p><p>回到图8，VGG输出 <img src="E:\笔记\markdown\reference\picture\equation-165046645349415.svg+xml" alt="[公式]"> 的特征，对应设置 <img src="E:\笔记\markdown\reference\picture\equation-165046645349416.svg+xml" alt="[公式]"> 个anchors，而RPN输出：</p><ol><li>大小为<img src="E:\笔记\markdown\reference\picture\equation-165046645349417.svg+xml" alt="[公式]"> 的positive/negative softmax分类特征矩阵</li><li>大小为 <img src="E:\笔记\markdown\reference\picture\equation-165046645349418.svg+xml" alt="[公式]"> 的regression坐标回归特征矩阵</li></ol><p>恰好满足RPN完成positive/negative分类+bounding box regression坐标回归.</p><h3 id="2-6-Proposal-Layer"><a href="#2-6-Proposal-Layer" class="headerlink" title="2.6 Proposal Layer"></a>2.6 Proposal Layer</h3><p>Proposal Layer负责综合所有 <img src="E:\笔记\markdown\reference\picture\equation-165046645349519.svg+xml" alt="[公式]"> 变换量和positive anchors，计算出精准的proposal，送入后续RoI Pooling Layer。还是先来看看Proposal Layer的caffe prototxt定义：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &#x27;proposal&#x27;</span><br><span class="line">  type: &#x27;Python&#x27;</span><br><span class="line">  bottom: &#x27;rpn_cls_prob_reshape&#x27;</span><br><span class="line">  bottom: &#x27;rpn_bbox_pred&#x27;</span><br><span class="line">  bottom: &#x27;im_info&#x27;</span><br><span class="line">  top: &#x27;rois&#x27;</span><br><span class="line">  python_param &#123;</span><br><span class="line">    module: &#x27;rpn.proposal_layer&#x27;</span><br><span class="line">    layer: &#x27;ProposalLayer&#x27;</span><br><span class="line">    param_str: &quot;&#x27;feat_stride&#x27;: 16&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Proposal Layer有3个输入：positive vs negative anchors分类器结果rpn_cls_prob_reshape，对应的bbox reg的 <img src="E:\笔记\markdown\reference\picture\equation-165046645349519.svg+xml" alt="[公式]"> 变换量rpn_bbox_pred，以及im_info；另外还有参数feat_stride=16，这和图4是对应的。</p><p>首先解释im_info。对于一副任意大小PxQ图像，传入Faster RCNN前首先reshape到固定MxN，im_info=[M, N, scale_factor]则保存了此次缩放的所有信息。然后经过Conv Layers，经过4次pooling变为WxH=(M/16)x(N/16)大小，其中feature_stride=16则保存了该信息，用于计算anchor偏移量。</p><p><img src="E:\笔记\markdown\reference\picture\v2-1e43500c7cc9a9de211d737bc347ced9_720w.jpg" alt="img">图13</p><p>Proposal Layer forward（caffe layer的前传函数）按照以下顺序依次处理：</p><ol><li>生成anchors，利用<img src="E:\笔记\markdown\reference\picture\equation-165046645349519.svg+xml" alt="[公式]">对所有的anchors做bbox regression回归（这里的anchors生成和训练时完全一致）</li><li>按照输入的positive softmax scores由大到小排序anchors，提取前pre_nms_topN(e.g. 6000)个anchors，即提取修正位置后的positive anchors</li><li>限定超出图像边界的positive anchors为图像边界，防止后续roi pooling时proposal超出图像边界（见文章底部QA部分图21）</li><li>剔除尺寸非常小的positive anchors</li><li>对剩余的positive anchors进行NMS（nonmaximum suppression）</li><li>Proposal Layer有3个输入：positive和negative anchors分类器结果rpn_cls_prob_reshape，对应的bbox reg的(e.g. 300)结果作为proposal输出</li></ol><p>之后输出proposal=[x1, y1, x2, y2]，注意，由于在第三步中将anchors映射回原图判断是否超出边界，所以这里输出的proposal是对应MxN输入图像尺度的，这点在后续网络中有用。另外我认为，严格意义上的检测应该到此就结束了，后续部分应该属于识别了。</p><p>RPN网络结构就介绍到这里，总结起来就是：<br><strong>生成anchors -&gt; softmax分类器提取positvie anchors -&gt; bbox reg回归positive anchors -&gt; Proposal Layer生成proposals</strong></p><h2 id="3-RoI-pooling"><a href="#3-RoI-pooling" class="headerlink" title="3 RoI pooling"></a>3 RoI pooling</h2><p>而RoI Pooling层则负责收集proposal，并计算出proposal feature maps，送入后续网络。从图2中可以看到Rol pooling层有2个输入：</p><ol><li>原始的feature maps</li><li>RPN输出的proposal boxes（大小各不相同）</li></ol><h3 id="3-1-为何需要RoI-Pooling"><a href="#3-1-为何需要RoI-Pooling" class="headerlink" title="3.1 为何需要RoI Pooling"></a>3.1 为何需要RoI Pooling</h3><p>先来看一个问题：对于传统的CNN（如AlexNet和VGG），当网络训练好后输入的图像尺寸必须是固定值，同时网络输出也是固定大小的vector or matrix。如果输入图像大小不定，这个问题就变得比较麻烦。有2种解决办法：</p><ol><li>从图像中crop一部分传入网络</li><li>将图像warp成需要的大小后传入网络</li></ol><p><img src="E:\笔记\markdown\reference\picture\v2-e525342cbde476a11c48a6be393f226c_720w.jpg" alt="img">图14 crop与warp破坏图像原有结构信息</p><p>两种办法的示意图如图14，可以看到无论采取那种办法都不好，要么crop后破坏了图像的完整结构，要么warp破坏了图像原始形状信息。</p><p>回忆RPN网络生成的proposals的方法：对positive anchors进行bounding box regression，那么这样获得的proposals也是大小形状各不相同，即也存在上述问题。所以Faster R-CNN中提出了RoI Pooling解决这个问题。不过RoI Pooling确实是从<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/1406.4729">Spatial Pyramid Pooling</a>发展而来，但是限于篇幅这里略去不讲，有兴趣的读者可以自行查阅相关论文。</p><h3 id="3-2-RoI-Pooling原理"><a href="#3-2-RoI-Pooling原理" class="headerlink" title="3.2 RoI Pooling原理"></a>3.2 RoI Pooling原理</h3><p>分析之前先来看看RoI Pooling Layer的caffe prototxt的定义：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;roi_pool5&quot;</span><br><span class="line">  type: &quot;ROIPooling&quot;</span><br><span class="line">  bottom: &quot;conv5_3&quot;</span><br><span class="line">  bottom: &quot;rois&quot;</span><br><span class="line">  top: &quot;pool5&quot;</span><br><span class="line">  roi_pooling_param &#123;</span><br><span class="line">    pooled_w: 7</span><br><span class="line">    pooled_h: 7</span><br><span class="line">    spatial_scale: 0.0625 # 1/16</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中有新参数pooled_w和pooled_h，另外一个参数spatial_scale认真阅读的读者肯定已经知道知道用途。RoI Pooling layer forward过程：</p><ul><li>由于proposal是对应MxN尺度的，所以首先使用spatial_scale参数将其映射回(M/16)x(N/16)大小的feature map尺度；</li><li>再将每个proposal对应的feature map区域水平分为 <img src="E:\笔记\markdown\reference\picture\text{pooled_h}.svg+xml" alt="[公式]"> 的网格；</li><li>对网格的每一份都进行max pooling处理。</li></ul><p>这样处理后，即使大小不同的proposal输出结果都是 <img src="E:\笔记\markdown\reference\picture\text{pooled_h}.svg+xml" alt="[公式]"> 固定大小，实现了固定长度输出。</p><p><img src="E:\笔记\markdown\reference\picture\v2-e3108dc5cdd76b871e21a4cb64001b5c_720w.jpg" alt="img">图15 proposal示意图</p><h2 id="4-Classification"><a href="#4-Classification" class="headerlink" title="4 Classification"></a>4 Classification</h2><p>Classification部分利用已经获得的proposal feature maps，通过full connect层与softmax计算每个proposal具体属于那个类别（如人，车，电视等），输出cls_prob概率向量；同时再次利用bounding box regression获得每个proposal的位置偏移量bbox_pred，用于回归更加精确的目标检测框。Classification部分网络结构如图16。</p><p><img src="E:\笔记\markdown\reference\picture\v2-9377a45dc8393d546b7b52a491414ded_720w.jpg" alt="img">图16 Classification部分网络结构图</p><p>从RoI Pooling获取到7x7=49大小的proposal feature maps后，送入后续网络，可以看到做了如下2件事：</p><ol><li>通过全连接和softmax对proposals进行分类，这实际上已经是识别的范畴了</li><li>再次对proposals进行bounding box regression，获取更高精度的rect box</li></ol><p>这里来看看全连接层InnerProduct layers，简单的示意图如图17，</p><p><img src="E:\笔记\markdown\reference\picture\v2-38594a97f33ff56fc72542a20a78116d_720w.jpg" alt="img">图17 全连接层示意图</p><p>其计算公式如下：</p><p><img src="E:\笔记\markdown\reference\picture\v2-f56d3209f9a7d5f27d77ead7489ab70f_720w.jpg" alt="img"></p><p>其中W和bias B都是预先训练好的，即大小是固定的，当然输入X和输出Y也就是固定大小。所以，这也就印证了之前Roi Pooling的必要性。到这里，我想其他内容已经很容易理解，不在赘述了。</p><h2 id="5-Faster-RCNN训练"><a href="#5-Faster-RCNN训练" class="headerlink" title="5 Faster RCNN训练"></a>5 Faster RCNN训练</h2><p>Faster R-CNN的训练，是在已经训练好的model（如VGG_CNN_M_1024，VGG，ZF）的基础上继续进行训练。实际中训练过程分为6个步骤：</p><ol><li>在已经训练好的model上，训练RPN网络，对应stage1_rpn_train.pt</li><li>利用步骤1中训练好的RPN网络，收集proposals，对应rpn_test.pt</li><li>第一次训练Fast RCNN网络，对应stage1_fast_rcnn_train.pt</li><li>第二训练RPN网络，对应stage2_rpn_train.pt</li><li>再次利用步骤4中训练好的RPN网络，收集proposals，对应rpn_test.pt</li><li>第二次训练Fast RCNN网络，对应stage2_fast_rcnn_train.pt</li></ol><p>可以看到训练过程类似于一种“迭代”的过程，不过只循环了2次。至于只循环了2次的原因是应为作者提到：”A similar alternating training can be run for more iterations, but we have observed negligible improvements”，即循环更多次没有提升了。接下来本章以上述6个步骤讲解训练过程。</p><p>下面是一张训练过程流程图，应该更加清晰：</p><p><img src="E:\笔记\markdown\reference\picture\v2-ddfcf3dc29976e384b047418aec9002d_720w.jpg" alt="img"></p><h3 id="5-1-训练RPN网络"><a href="#5-1-训练RPN网络" class="headerlink" title="5.1 训练RPN网络"></a>5.1 训练RPN网络</h3><p>在该步骤中，首先读取RBG提供的预训练好的model（本文使用VGG），开始迭代训练。来看看stage1_rpn_train.pt网络结构，如图19。</p><p><img src="E:\笔记\markdown\reference\picture\v2-c39aef1d06e08e4e0cec96b10f50a779_720w.jpg" alt="img">图19 stage1_rpn_train.pt（考虑图片大小，Conv Layers中所有的层都画在一起了，如红圈所示，后续图都如此处理）</p><p>与检测网络类似的是，依然使用Conv Layers提取feature maps。整个网络使用的Loss如下：</p><p><img src="E:\笔记\markdown\reference\picture\tag{12}.svg+xml" alt="[公式]"></p><p>上述公式中 <img src="E:\笔记\markdown\reference\picture\equation-165046645349520.svg+xml" alt="[公式]"> 表示anchors index， <img src="E:\笔记\markdown\reference\picture\equation-165046645349521.svg+xml" alt="[公式]"> 表示positive softmax probability，<img src="E:\笔记\markdown\reference\picture\equation-165046645349522.svg+xml" alt="[公式]">代表对应的GT predict概率（即当第i个anchor与GT间IoU&gt;0.7，认为是该anchor是positive，<img src="E:\笔记\markdown\reference\picture\equation-165046645349523.svg+xml" alt="[公式]">；反之IoU&lt;0.3时，认为是该anchor是negative，<img src="E:\笔记\markdown\reference\picture\equation-165046645349524.svg+xml" alt="[公式]">；至于那些0.3&lt;IoU&lt;0.7的anchor则不参与训练）；<img src="E:\笔记\markdown\reference\picture\equation-165046645349525.svg+xml" alt="[公式]">代表predict bounding box，<img src="E:\笔记\markdown\reference\picture\equation-165046645349526.svg+xml" alt="[公式]">代表对应positive anchor对应的GT box。可以看到，整个Loss分为2部分：</p><ol><li>cls loss，即rpn_cls_loss层计算的softmax loss，用于分类anchors为positive与negative的网络训练</li><li>reg loss，即rpn_loss_bbox层计算的soomth L1 loss，用于bounding box regression网络训练。注意在该loss中乘了 <img src="E:\笔记\markdown\reference\picture\equation-165046645349522.svg+xml" alt="[公式]"> ，相当于只关心positive anchors的回归（其实在回归中也完全没必要去关心negative）。</li></ol><p>由于在实际过程中，<img src="E:\笔记\markdown\reference\picture\text{cls}.svg+xml" alt="[公式]">和<img src="E:\笔记\markdown\reference\picture\text{reg}.svg+xml" alt="[公式]">差距过大，用参数λ平衡二者（如<img src="E:\笔记\markdown\reference\picture\text{cls}%3D256.svg+xml" alt="[公式]">，<img src="E:\笔记\markdown\reference\picture\text{reg}%3D2400.svg+xml" alt="[公式]">时设置 <img src="E:\笔记\markdown\reference\picture\approx10.svg+xml" alt="[公式]"> ），使总的网络Loss计算过程中能够均匀考虑2种Loss。这里比较重要是 <img src="E:\笔记\markdown\reference\picture\text{reg}-165046645349627.svg+xml" alt="[公式]"> 使用的soomth L1 loss，计算公式如下：</p><p><img src="E:\笔记\markdown\reference\picture\tag{13}.svg+xml" alt="[公式]"></p><p><img src="E:\笔记\markdown\reference\picture\tag{14}.svg+xml" alt="[公式]"></p><p>了解数学原理后，反过来看图18：</p><ol><li>在RPN训练阶段，rpn-data（python AnchorTargetLayer）层会按照和test阶段Proposal层完全一样的方式生成Anchors用于训练</li><li>对于rpn_loss_cls，输入的rpn_cls_scors_reshape和rpn_labels分别对应 <img src="E:\笔记\markdown\reference\picture\equation-165046645349628.svg+xml" alt="[公式]"> 与 <img src="E:\笔记\markdown\reference\picture\equation-165046645349629.svg+xml" alt="[公式]"> ， <img src="E:\笔记\markdown\reference\picture\text{cls}.svg+xml" alt="[公式]"> 参数隐含在<img src="E:\笔记\markdown\reference\picture\equation-16504669589223.svg+xml" alt="[公式]">与<img src="E:\笔记\markdown\reference\picture\equation-16504669589224.svg+xml" alt="[公式]">的caffe blob的大小中</li><li>对于rpn_loss_bbox，输入的rpn_bbox_pred和rpn_bbox_targets分别对应 <img src="E:\笔记\markdown\reference\picture\equation-165046645349525.svg+xml" alt="[公式]"> 与 <img src="E:\笔记\markdown\reference\picture\equation-165046645349526.svg+xml" alt="[公式]"> ，rpn_bbox_inside_weigths对应 <img src="E:\笔记\markdown\reference\picture\equation-16504669589224.svg+xml" alt="[公式]">，rpn_bbox_outside_weigths未用到（从smooth_L1_Loss layer代码中可以看到），而 <img src="E:\笔记\markdown\reference\picture\text{reg}.svg+xml" alt="[公式]"> 同样隐含在caffe blob大小中</li></ol><p>这样，公式与代码就完全对应了。特别需要注意的是，在训练和检测阶段生成和存储anchors的顺序完全一样，这样训练结果才能被用于检测！</p><h3 id="5-2-通过训练好的RPN网络收集proposals"><a href="#5-2-通过训练好的RPN网络收集proposals" class="headerlink" title="5.2 通过训练好的RPN网络收集proposals"></a>5.2 通过训练好的RPN网络收集proposals</h3><p>在该步骤中，利用之前的RPN网络，获取proposal rois，同时获取positive softmax probability，如图20，然后将获取的信息保存在python pickle文件中。该网络本质上和检测中的RPN网络一样，没有什么区别。</p><p><img src="E:\笔记\markdown\reference\picture\v2-1ac5f8a2899ee413464ecf7866f8f840_720w.jpg" alt="img">图20 rpn_test.pt</p><h3 id="5-3-训练Faster-RCNN网络"><a href="#5-3-训练Faster-RCNN网络" class="headerlink" title="5.3 训练Faster RCNN网络"></a>5.3 训练Faster RCNN网络</h3><p>读取之前保存的pickle文件，获取proposals与positive probability。从data层输入网络。然后：</p><ol><li>将提取的proposals作为rois传入网络，如图21蓝框</li><li>计算bbox_inside_weights+bbox_outside_weights，作用与RPN一样，传入soomth_L1_loss layer，如图21绿框</li></ol><p>这样就可以训练最后的识别softmax与最终的bounding box regression了。</p><p><img src="E:\笔记\markdown\reference\picture\v2-fbece817952865689187e68f0af86792_720w.jpg" alt="img">图21 stage1_fast_rcnn_train.pt</p><p>之后的stage2训练都是大同小异，不再赘述了。Faster R-CNN还有一种end-to-end的训练方式，可以一次完成train，有兴趣请自己看作者GitHub吧。</p><p><a href="https://link.zhihu.com/?target=https://github.com/rbgirshick/py-faster-rcnn">rbgirshick py-faster-rcnngithub.com/rbgirshick/py-faster-rcnn<img src="E:\笔记\markdown\reference\picture\v2-9f4e9c49a8e59e08abe70f8ba9b14fef_ipico.jpg" alt="img"></a></p><h2 id="QA"><a href="#QA" class="headerlink" title="QA"></a>QA</h2><p>此篇文章初次成文于2016年内部学习分享，再后来经多次修正和完善成为现在的样子。感谢大家一直以来的支持，现在总结常见疑问回答如下：</p><ul><li>为什么Anchor坐标中有负数</li></ul><p>回顾anchor生成步骤：首先生成9个base anchor，然后通过坐标偏移在 <img src="E:\笔记\markdown\reference\picture\equation-165046645349630.svg+xml" alt="[公式]"> 大小的 <img src="E:\笔记\markdown\reference\picture\frac{1}{16}.svg+xml" alt="[公式]"> 下采样FeatureMap每个点都放上这9个base anchor，就形成了 <img src="E:\笔记\markdown\reference\picture\equation-165046645349416.svg+xml" alt="[公式]"> 个anhcors。至于这9个base anchor坐标是什么其实并不重要，不同代码实现也许不同。</p><p>显然这里面有一部分边缘anchors会超出图像边界，而真实中不会有超出图像的目标，所以会有clip anchor步骤。</p><p><img src="E:\笔记\markdown\reference\picture\v2-9d67146e0cb10397d8c2170794412608_720w.jpg" alt="img">图21 clip anchor</p><ul><li>Anchor到底与网络输出如何对应</li></ul><p>VGG输出 <img src="E:\笔记\markdown\reference\picture\equation-165046645349415.svg+xml" alt="[公式]"> 的特征，对应设置 <img src="E:\笔记\markdown\reference\picture\equation-165046645349416.svg+xml" alt="[公式]"> 个anchors，而RPN输出 <img src="E:\笔记\markdown\reference\picture\equation-165046645349417.svg+xml" alt="[公式]"> 的分类特征矩阵和 <img src="E:\笔记\markdown\reference\picture\equation-165046645349418.svg+xml" alt="[公式]"> 的坐标回归特征矩阵。</p><p><img src="E:\笔记\markdown\reference\picture\v2-82196feb7b528d76411feb90bfec2af4_720w.jpg" alt="img">图22 anchor与网络输出如何对应方式</p><p>其实在实现过程中，每个点的 <img src="E:\笔记\markdown\reference\picture\equation-165046645349731.svg+xml" alt="[公式]"> 个分类特征与 <img src="E:\笔记\markdown\reference\picture\equation-165046645349732.svg+xml" alt="[公式]"> 回归特征，与 <img src="E:\笔记\markdown\reference\picture\equation-165046645349733.svg+xml" alt="[公式]"> 个anchor逐个对应即可，这实际是一种“人为设置的逻辑映射”。当然，也可以不这样设置，但是无论如何都需要保证<strong>在训练和测试过程中映射方式必须一致</strong>。</p><ul><li>为何有ROI Pooling还要把输入图片resize到固定大小的MxN</li></ul><p>由于引入ROI Pooling，从原理上说Faster R-CNN确实能够检测任意大小的图片。但是由于在训练的时候需要使用大batch训练网络，而不同大小输入拼batch在实现的时候代码较为复杂，而且当时以Caffe为代表的第一代深度学习框架也不如Tensorflow和PyTorch灵活，所以作者选择了把输入图片resize到固定大小的800x600。这应该算是历史遗留问题。</p><p>另外很多问题，都是属于具体实现问题，真诚的建议读者阅读代码自行理解。</p><h2 id="拓展"><a href="#拓展" class="headerlink" title="拓展"></a>拓展</h2><p>关于torchvision中的FasterRCNN代码：</p><p><a href="https://zhuanlan.zhihu.com/p/145842317">捋一捋pytorch官方FasterRCNN代码1308 赞同 · 21 评论文章<img src="E:\笔记\markdown\reference\picture\v2-dc78d0ad1012ad65040bdb7eb657f381_180x120.jpg" alt="img"></a></p><p>Faster RCNN在文字检测中的应用：CTPN</p><p><a href="https://zhuanlan.zhihu.com/p/34757009">场景文字检测—CTPN原理与实现678 赞同 · 54 评论文章<img src="E:\笔记\markdown\reference\picture\v2-2ea98126ebc05e35be28efe598a021ed_180x120.jpg" alt="img"></a></p><p>Faster RCNN在高德导航中的应用：</p><p><a href="https://link.zhihu.com/?target=https://mp.weixin.qq.com/s/IJUMCOBhgXHv7VC1YT4q_g">机器学习在交通标志检测与精细分类中的应用mp.weixin.qq.com/s/IJUMCOBhgXHv7VC1YT4q_g<img src="E:\笔记\markdown\reference\picture\v2-b7f1350a3179674d93d60d8ad9b52c0f_180x120.jpg" alt="img"></a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;RCNN-将CNN引入目标检测的开山之作&quot;&gt;&lt;a href=&quot;#RCNN-将CNN引入目标检测的开山之作&quot; class=&quot;headerlink&quot; title=&quot;RCNN- 将CNN引入目标检测的开山之作&quot;&gt;&lt;/a&gt;RCNN- 将CNN引入目标检测的开山之作&lt;/h1</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://jpccc.github.io/2022/04/20/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    <id>https://jpccc.github.io/2022/04/20/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</id>
    <published>2022-04-20T14:38:48.702Z</published>
    <updated>2022-04-20T14:39:50.543Z</updated>
    
    <content type="html"><![CDATA[<h1 id="The-Illustrated-Self-Supervised-Learning"><a href="#The-Illustrated-Self-Supervised-Learning" class="headerlink" title="The Illustrated Self-Supervised Learning"></a><a href="https://amitness.com/2020/02/illustrated-self-supervised-learning/">The Illustrated Self-Supervised Learning</a></h1><p>I first got introduced to self-supervised learning in a <a href="https://www.youtube.com/watch?v=7I0Qt7GALVk&t=2639s">talk</a> by Yann Lecun, where he introduced the “cake analogy” to illustrate the importance of self-supervised learning. In the talk, he said:</p><blockquote><p>“If intelligence is a cake, the bulk of the cake is self-supervised learning, the icing on the cake is supervised learning, and the cherry on the cake is reinforcement learning (RL).”</p></blockquote><p>Though the analogy is <a href="https://www.dropbox.com/s/fdw7q8mx3x4wr0c/2017_12_xx_NIPS-keynote-final.pdf?dl=0">debated</a>, we have seen the impact of self-supervised learning in the Natural Language Processing field where recent developments (Word2Vec, Glove, ELMO, BERT) have embraced self-supervision and achieved state of the art results.</p><p><img src="E:\笔记\markdown\reference\picture\self-supervised-nlp-to-vision-16504267629161.png" alt="img"></p><p>Curious to know the current state of self-supervised learning in the Computer Vision field, I read up on existing literature on self-supervised learning applied to computer vision through a <a href="https://arxiv.org/abs/1902.06162">recent survey paper</a> by Jing et. al.</p><p>In this post, I will explain what is self-supervised learning and summarize the patterns of problem formulation being used in self-supervised learning with visualizations.</p><h2 id="Why-Self-Supervised-Learning"><a href="#Why-Self-Supervised-Learning" class="headerlink" title="Why Self-Supervised Learning?"></a>Why Self-Supervised Learning?</h2><p>To apply supervised learning with deep neural networks, we need enough labeled data. To acquire that, human annotators manually label data which is both a time consuming and expensive process. There are also fields such as the medical field where getting enough data is a challenge itself. Thus, a major bottleneck in current supervised learning paradigm is the label generation part.</p><p><img src="E:\笔记\markdown\reference\picture\supervised-manual-annotation-16504267629163.png" alt="Manual Annotation in Supervised Learning"></p><h2 id="What-is-Self-Supervised-Learning"><a href="#What-is-Self-Supervised-Learning" class="headerlink" title="What is Self-Supervised Learning?"></a>What is Self-Supervised Learning?</h2><p>Self supervised learning is a method that poses the following question to formulate an unsupervised learning problem as a supervised one:</p><blockquote><p>Can we design the task in such a way that we can generate virtually unlimited labels from our existing images and use that to learn the representations?</p></blockquote><p><img src="E:\笔记\markdown\reference\picture\supervised-automated-16504267629165.png" alt="Automating manual labeling with Self Supervised Learning"></p><p>In self-supervised learning, we replace the human annotation block by creatively exploiting some property of data to set up a pseudo-supervised task. For example, here instead of labeling images as cat/dog, we could instead rotate them by 0/90/180/270 degrees and train a model to predict rotation. We can generate virtually unlimited training data from millions of images we have freely available on the internet.</p><p><img src="E:\笔记\markdown\reference\picture\self-supervised-workflow-16504267629167.png" alt="Self-supervised Learning Workflow Diagram"></p><p>Figure: End to End Workflow of Self-Supervised Learning</p><p>Once we learn representations from these millions of images, we can use transfer learning to fine-tune it on some supervised task like image classification of cats vs dogs with very few examples.</p><p><img src="E:\笔记\markdown\reference\picture\self-supervised-finetuning-16504267629169.png" alt="img"></p><h2 id="Survey-of-Self-Supervised-Learning-Methods"><a href="#Survey-of-Self-Supervised-Learning-Methods" class="headerlink" title="Survey of Self-Supervised Learning Methods"></a>Survey of Self-Supervised Learning Methods</h2><p>Let’s now understand the various approaches researchers have proposed to exploit image and video properties and apply self-supervised learning for representation learning.</p><h3 id="A-Self-Supervised-Learning-from-Image"><a href="#A-Self-Supervised-Learning-from-Image" class="headerlink" title="A. Self-Supervised Learning from Image"></a>A. Self-Supervised Learning from Image</h3><h4 id="Pattern-1-Reconstruction"><a href="#Pattern-1-Reconstruction" class="headerlink" title="Pattern 1: Reconstruction"></a>Pattern 1: Reconstruction</h4><h5 id="1-Image-Colorization"><a href="#1-Image-Colorization" class="headerlink" title="1. Image Colorization"></a>1. <a href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#1-image-colorization"><strong>Image Colorization</strong></a></h5><p>Formulation:</p><blockquote><p>What if we prepared pairs of (grayscale, colorized) images by applying grayscale to millions of images we have freely available?</p></blockquote><p><img src="E:\笔记\markdown\reference\picture\ss-colorization-data-gen-165042676291611.png" alt="Data Generation for Image Colorization"></p><p>We could use an encoder-decoder architecture based on a fully convolutional neural network and compute the L2 loss between the predicted and actual color images.</p><p><img src="E:\笔记\markdown\reference\picture\ss-image-colorization-165042676291613.png" alt="Architecture for Image Colorization"></p><p>To solve this task, the model has to learn about different objects present in the image and related parts so that it can paint those parts in the same color. Thus, representations learned are useful for downstream tasks.<img src="E:\笔记\markdown\reference\picture\ss-colorization-learning-165042676291615.png" alt="Learning to colorize images"></p><p><strong>Papers:</strong><br>        <a href="https://arxiv.org/abs/1603.08511">Colorful Image Colorization</a> | <a href="https://arxiv.org/abs/1705.02999">Real-Time User-Guided Image Colorization with Learned Deep Priors</a> | <a href="http://iizuka.cs.tsukuba.ac.jp/projects/colorization/en/">Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification</a></p><h5 id="2-Image-Superresolution"><a href="#2-Image-Superresolution" class="headerlink" title="2. Image Superresolution"></a>2. <a href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#2-image-superresolution"><strong>Image Superresolution</strong></a></h5><p>Formulation:</p><blockquote><p>What if we prepared training pairs of (small, upscaled) images by downsampling millions of images we have freely available?</p></blockquote><p><img src="E:\笔记\markdown\reference\picture\ss-superresolution-training-gen-165042676291617.png" alt="Training Data for Superresolution"></p><p>GAN based models such as <a href="https://arxiv.org/abs/1609.04802">SRGAN</a> are popular for this task. A generator takes a low-resolution image and outputs a high-resolution image using a fully convolutional network. The actual and generated images are compared using both mean-squared-error and content loss to imitate human-like quality comparison. A binary-classification discriminator takes an image and classifies whether it’s an actual high-resolution image(1) or a fake generated superresolution image(0). This interplay between the two models leads to generator learning to produce images with fine details.</p><p><img src="E:\笔记\markdown\reference\picture\ss-srgan-architecture-165042676291619.png" alt="Architecture for SRGAN"></p><p>Both generator and discriminator learn semantic features that can be used for downstream tasks.</p><p><strong>Papers</strong>:<br><a href="https://arxiv.org/abs/1609.04802">Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</a></p><h5 id="3-Image-Inpainting"><a href="#3-Image-Inpainting" class="headerlink" title="3. Image Inpainting"></a>3. <a href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#3-image-inpainting"><strong>Image Inpainting</strong></a></h5><p>Formulation:</p><blockquote><p>What if we prepared training pairs of (corrupted, fixed) images by randomly removing part of images?</p></blockquote><p><img src="E:\笔记\markdown\reference\picture\ss-image-inpainting-data-gen-165042676291621.png" alt="Training Data for Image Inpainting"></p><p>Similar to superresolution, we can leverage a GAN-based architecture where the Generator can learn to reconstruct the image while discriminator separates real and generated images.</p><p><img src="E:\笔记\markdown\reference\picture\ss-inpainting-architecture-165042676291723.png" alt="Architecture for Image Inpainting"></p><p>For downstream tasks, <a href="https://arxiv.org/abs/1604.07379">Pathak et al.</a> have shown that semantic features learned by such a generator give 10.2% improvement over random initialization on the <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html">PASCAL VOC 2012</a> semantic segmentation challenge while giving &lt;4% improvements over classification and object detection.</p><p><strong>Papers</strong>:<br>        <a href="https://arxiv.org/abs/1604.07379">Context encoders: Feature learning by inpainting</a></p><h5 id="4-Cross-Channel-Prediction"><a href="#4-Cross-Channel-Prediction" class="headerlink" title="4. Cross-Channel Prediction"></a>4. <a href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#4-cross-channel-prediction"><strong>Cross-Channel Prediction</strong></a></h5><p>Formulation:</p><blockquote><p>What if we predict one channel of the image from the other channel and combine them to reconstruct the original image?</p></blockquote><p>Zhang et al. used this idea in their paper called “Split-Brain Autoencoder”. To understand the idea of the paper, let’s take an example of a color image of tomato.</p><p><img src="E:\笔记\markdown\reference\picture\split-brain-autoencoder-165042676291725.png" alt="img"></p><p>Example adapted from “Split-Brain Autoencoder” paper</p><p>For this color image, we can split it into grayscale and color channels. Then, for the grayscale channel, we predict the color channel and for the color channel part, we predict the grayscale channel. The two predicted channels $X_1$ and $X_2$ are combined to get back a reconstruction of the original image. We can compare this reconstruction to the original color image to get a loss and improve the model.</p><p>This same setup can be applied for images with depth as well where we use the color channels and the depth channels from a RGB-HHA image to predict each other and compare output image and original image.</p><p><img src="E:\笔记\markdown\reference\picture\split-brain-autoencoder-rgbhha-165042676291727.png" alt="img"></p><p>Example adapted from “Split-Brain Autoencoder” paper</p><p><strong>Papers</strong>:<br><a href="https://arxiv.org/abs/1611.09842">Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction</a></p><h4 id="Pattern-2-Common-Sense-Tasks"><a href="#Pattern-2-Common-Sense-Tasks" class="headerlink" title="Pattern 2: Common Sense Tasks"></a>Pattern 2: Common Sense Tasks</h4><h5 id="1-Image-Jigsaw-Puzzle"><a href="#1-Image-Jigsaw-Puzzle" class="headerlink" title="1. Image Jigsaw Puzzle"></a>1. <a href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#1-image-jigsaw-puzzle"><strong>Image Jigsaw Puzzle</strong></a></h5><p>Formulation:</p><blockquote><p>What if we prepared training pairs of (shuffled, ordered) puzzles by randomly shuffling patches of images?</p></blockquote><p><img src="E:\笔记\markdown\reference\picture\ss-image-jigsaw-data-165042676291729.png" alt="Training Data For Image Jigsaw Puzzle"></p><p>Even with only 9 patches, there can be 362880 possible puzzles. To overcome this, only a subset of possible permutations is used such as 64 permutations with the highest hamming distance.</p><p><img src="E:\笔记\markdown\reference\picture\ss-jigsaw-permutations-165042676291731.png" alt="Number of Permutations in Image Jigsaw"></p><p>Suppose we use a permutation that changes the image as shown below. Let’s use the permutation number 64 from our total available 64 permutations.</p><p><img src="E:\笔记\markdown\reference\picture\ss-jigsaw-permutation-64-165042676291733.png" alt="Example of single permutation in jigsaw"></p><p>Now, to recover back the original patches, <a href="https://arxiv.org/abs/1603.09246">Noroozi et al.</a> proposed a neural network called context-free network (CFN) as shown below. Here, the individual patches are passed through the same siamese convolutional layers that have shared weights. Then, the features are combined in a fully-connected layer. In the output, the model has to predict which permutation was used from the 64 possible classes. If we know the permutation, we can solve the puzzle.</p><p><img src="E:\笔记\markdown\reference\picture\ss-jigsaw-architecture-165042676291735.png" alt="Architecture for Image Jigsaw Task"></p><p>To solve the Jigsaw puzzle, the model needs to learn to identify how parts are assembled in an object, relative positions of different parts of objects and shape of objects. Thus, the representations are useful for downstream tasks in classification and detection.</p><p><strong>Papers</strong>:<br><a href="https://arxiv.org/abs/1603.09246">Unsupervised learning of visual representations by solving jigsaw puzzles</a></p><h5 id="2-Context-Prediction"><a href="#2-Context-Prediction" class="headerlink" title="2. Context Prediction"></a>2. <a href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#2-context-prediction"><strong>Context Prediction</strong></a></h5><p>Formulation:</p><blockquote><p>What if we prepared training pairs of (image-patch, neighbor) by randomly taking an image patch and one of its neighbors around it from large, unlabeled image collection?</p></blockquote><p><img src="E:\笔记\markdown\reference\picture\ss-context-prediction-gen-165042676291737.png" alt="Training Data for Context Prediction"></p><p>To solve this pre-text task, <a href="https://arxiv.org/abs/1505.05192">Doersch et al.</a> used an architecture similar to that of a jigsaw puzzle. We pass the patches through two siamese ConvNets to extract features, concatenate the features and do a classification over 8 classes denoting the 8 possible neighbor positions.</p><p><img src="E:\笔记\markdown\reference\picture\ss-context-prediction-architecture-165042676291739.png" alt="Architecture for Context Prediction"></p><p><strong>Papers</strong>:<br><a href="https://arxiv.org/abs/1505.05192">Unsupervised Visual Representation Learning by Context Prediction</a></p><h5 id="3-Geometric-Transformation-Recognition"><a href="#3-Geometric-Transformation-Recognition" class="headerlink" title="3. Geometric Transformation Recognition"></a>3. <a href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#3-geometric-transformation-recognition"><strong>Geometric Transformation Recognition</strong></a></h5><p>Formulation:</p><blockquote><p>What if we prepared training pairs of (rotated-image, rotation-angle) by randomly rotating images by (0, 90, 180, 270) from large, unlabeled image collection?</p></blockquote><p><img src="E:\笔记\markdown\reference\picture\ss-geometric-transformation-gen-165042676291741.png" alt="Training Data for Geometric Transformation"></p><p>To solve this pre-text task, <a href="https://arxiv.org/abs/1803.07728">Gidaris et al.</a> propose an architecture where a rotated image is passed through a ConvNet and the network has to classify it into 4 classes(0/90/270/360 degrees).<img src="E:\笔记\markdown\reference\picture\ss-geometric-transformation-architecture-165042676291743.png" alt="Architecture for Geometric Transformation Predction"></p><p>Though a very simple idea, the model has to understand location, types and pose of objects in an image to solve this task and as such, the representations learned are useful for downstream tasks.</p><p><strong>Papers</strong>:<br><a href="https://arxiv.org/abs/1803.07728">Unsupervised Representation Learning by Predicting Image Rotations</a></p><h4 id="Pattern-3-Automatic-Label-Generation"><a href="#Pattern-3-Automatic-Label-Generation" class="headerlink" title="Pattern 3: Automatic Label Generation"></a>Pattern 3: Automatic Label Generation</h4><h5 id="1-Image-Clustering"><a href="#1-Image-Clustering" class="headerlink" title="1. Image Clustering"></a>1. <a href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#1-image-clustering"><strong>Image Clustering</strong></a></h5><p>Formulation:</p><blockquote><p>What if we prepared training pairs of (image, cluster-number) by performing clustering on large, unlabeled image collection?</p></blockquote><p><img src="E:\笔记\markdown\reference\picture\ss-image-clustering-gen-165042676291845.png" alt="Training Data for Image Clustering"></p><p>To solve this pre-text task, <a href="https://arxiv.org/abs/1807.05520">Caron et al.</a> propose an architecture called deep clustering. Here, the images are first clustered and the clusters are used as classes. The task of the ConvNet is to predict the cluster label for an input image.<img src="E:\笔记\markdown\reference\picture\ss-deep-clustering-architecture-165042676291847.png" alt="Architecture for Deep Clustering"></p><p><strong>Papers</strong>:</p><ul><li><a href="https://amitness.com/2020/04/deepcluster/">Deep clustering for unsupervised learning of visual features</a></li><li><a href="https://amitness.com/2020/04/illustrated-self-labelling/">Self-labelling via simultaneous clustering and representation learning</a></li><li><a href="https://arxiv.org/abs/1608.08792">CliqueCNN: Deep Unsupervised Exemplar Learning</a></li></ul><h5 id="2-Synthetic-Imagery"><a href="#2-Synthetic-Imagery" class="headerlink" title="2. Synthetic Imagery"></a>2. <a href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#2-synthetic-imagery"><strong>Synthetic Imagery</strong></a></h5><p>Formulation:</p><blockquote><p>What if we prepared training pairs of (image, properties) by generating synthetic images using game engines and adapting it to real images?</p></blockquote><p><img src="E:\笔记\markdown\reference\picture\synthetic-imagery-data-165042676291849.png" alt="Training Data for Sythetic Imagery"></p><p>To solve this pre-text task, <a href="https://arxiv.org/pdf/1711.09082.pdf">Ren et al.</a> propose an architecture where weight-shared ConvNets are trained on both synthetic and real images and then a discriminator learns to classify whether ConvNet features fed to it is of a synthetic image or a real image. Due to adversarial nature, the shared representations between real and synthetic images get better.<img src="E:\笔记\markdown\reference\picture\ss-synthetic-image-architecture-165042676291851.png" alt="Architecture for Synthetic Image Training"></p><h3 id="B-Self-Supervised-Learning-From-Video"><a href="#B-Self-Supervised-Learning-From-Video" class="headerlink" title="B. Self-Supervised Learning From Video"></a>B. Self-Supervised Learning From Video</h3><h4 id="1-Frame-Order-Verification"><a href="#1-Frame-Order-Verification" class="headerlink" title="1. Frame Order Verification"></a>1. <a href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#1-frame-order-verification"><strong>Frame Order Verification</strong></a></h4><p>Formulation:</p><blockquote><p>What if we prepared training pairs of (video frames, correct/incorrect order) by shuffling frames from videos of objects in motion?</p></blockquote><p><img src="E:\笔记\markdown\reference\picture\ss-frame-order-data-gen-165042676291853.png" alt="Training Data for Video Order"></p><p>To solve this pre-text task, <a href="https://arxiv.org/pdf/1711.09082.pdf">Misra et al.</a> propose an architecture where video frames are passed through weight-shared ConvNets and the model has to figure out whether the frames are in the correct order or not. In doing so, the model learns not just spatial features but also takes into account temporal features.</p><p><img src="E:\笔记\markdown\reference\picture\ss-temporal-order-architecture-165042676291855.png" alt="Architecture for Frame Order Verification"></p><p><strong>Papers</strong>:</p><ul><li><a href="https://arxiv.org/abs/1603.08561">Shuffle and Learn: Unsupervised Learning using Temporal Order Verification</a></li><li><a href="https://arxiv.org/abs/1611.06646">Self-Supervised Video Representation Learning With Odd-One-Out Networks</a></li></ul><h2 id="Citation-Info-BibTex"><a href="#Citation-Info-BibTex" class="headerlink" title="Citation Info (BibTex)"></a><a href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#citation-info-bibtex">Citation Info (BibTex)</a></h2><p>If you found this blog post useful, please consider citing it as:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@misc&#123;chaudhary2020selfsupervised,</span><br><span class="line">  title   = &#123;The Illustrated Self-Supervised Learning&#125;,</span><br><span class="line">  author  = &#123;Amit Chaudhary&#125;,</span><br><span class="line">  year    = 2020,</span><br><span class="line">  note    = &#123;\url&#123;https://amitness.com/2020/02/illustrated-self-supervised-learning&#125;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="References"><a href="#References" class="headerlink" title="References"></a><a href="https://amitness.com/2020/02/illustrated-self-supervised-learning/#references">References</a></h2><ul><li>Jing, et al. “<a href="https://arxiv.org/abs/1902.06162">Self-Supervised Visual Feature Learning with Deep Neural Networks: A Survey.</a>”</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;The-Illustrated-Self-Supervised-Learning&quot;&gt;&lt;a href=&quot;#The-Illustrated-Self-Supervised-Learning&quot; class=&quot;headerlink&quot; title=&quot;The Illustra</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://jpccc.github.io/2022/04/20/%E8%87%AA%E7%9B%91%E7%9D%A3/"/>
    <id>https://jpccc.github.io/2022/04/20/%E8%87%AA%E7%9B%91%E7%9D%A3/</id>
    <published>2022-04-20T03:52:22.314Z</published>
    <updated>2022-04-20T03:52:22.314Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://jpccc.github.io/2022/04/11/%E5%9B%BE%E5%83%8F%E7%9A%84%E6%93%8D%E4%BD%9C/"/>
    <id>https://jpccc.github.io/2022/04/11/%E5%9B%BE%E5%83%8F%E7%9A%84%E6%93%8D%E4%BD%9C/</id>
    <published>2022-04-11T02:32:32.733Z</published>
    <updated>2022-05-09T06:00:08.883Z</updated>
    
    <content type="html"><![CDATA[<h2 id="PIL"><a href="#PIL" class="headerlink" title="PIL"></a>PIL</h2><blockquote><p>  PIL，全称 Python Imaging Library，是一个功能非常强大而且简单易用的图像处理库,已经是Python平台事实上的图像处理标准库了。PIL功能非常强大，但API却非常简单易用。但是，由于 PIL 仅支持到Python 2.7，加上年久失修，于是一群志愿者在 PIL 的基础上创建了兼容 Python 3 的版本，名字叫 Pillow ，我们可以通过安装 Pillow 来使用 PIL。</p></blockquote><h3 id="1-pip-安装-pillow"><a href="#1-pip-安装-pillow" class="headerlink" title="1. pip 安装 pillow"></a>1. pip 安装 pillow</h3><p>在 Ubuntu 下通过一个简单的命令<code>pip3 install pillow</code>即可成功安装库。</p><p>如果遇到<code>Permission denied</code>安装失败，请加上<code>sudo</code>重试。</p><h3 id="2-打开、保存、显示图片"><a href="#2-打开、保存、显示图片" class="headerlink" title="2. 打开、保存、显示图片"></a>2. 打开、保存、显示图片</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">image = Image.<span class="built_in">open</span>(<span class="string">&#x27;cat.jpg&#x27;</span>)</span><br><span class="line">image.show()</span><br><span class="line">image.save(<span class="string">&#x27;1.jpg&#x27;</span>,<span class="string">&#x27;jpeg&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(image.mode, image.size, image.<span class="built_in">format</span>)</span><br><span class="line"><span class="comment"># RGB (481, 321) JPEG</span></span><br></pre></td></tr></table></figure><ul><li>  mode 属性为图片的模式，RGB 代表彩色图像，L 代表光照图像也即灰度图像等</li><li>  size 属性为图片的大小(宽度，长度)</li><li>  format 属性为图片的格式，如常见的 PNG、JPEG 等</li></ul><h3 id="3-转换图片模式"><a href="#3-转换图片模式" class="headerlink" title="3. 转换图片模式"></a>3. 转换图片模式</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">image.show()</span><br><span class="line">grey_image = image.convert(<span class="string">&#x27;L&#x27;</span>)</span><br><span class="line">grey_image.show()</span><br></pre></td></tr></table></figure><p><img src="E:\笔记\markdown\reference\picture\v2-fa0ae3e48ef7a9645c252b49c16fce06_720w.jpg" alt="img"></p><p><img src="E:\笔记\markdown\reference\picture\v2-0e23c82abff1c3673d55358c7e6f1155_720w.jpg" alt="img"></p><ul><li>  任何支持的图片模式都可以直接转为彩色模式或者灰度模式，但是，若是想转化为其他模式，则需要借助一个中间模式（通常是彩色）来进行过转</li></ul><h3 id="4-通道分离合并"><a href="#4-通道分离合并" class="headerlink" title="4. 通道分离合并"></a>4. 通道分离合并</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">r, g, b = image.split()</span><br><span class="line">im = Image.merge(<span class="string">&#x27;RGB&#x27;</span>, (b, g, r))</span><br></pre></td></tr></table></figure><ul><li>  彩色图像可以分离出 R、G、B 通道，但若是灰度图像，则返回灰度图像本身。然后，可以将 R、G、B 通道按照一定的顺序再合并成彩色图像。</li></ul><h3 id="5-图片裁剪、旋转和改变大小"><a href="#5-图片裁剪、旋转和改变大小" class="headerlink" title="5. 图片裁剪、旋转和改变大小"></a>5. 图片裁剪、旋转和改变大小</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">box = (<span class="number">100</span>, <span class="number">100</span>, <span class="number">300</span>, <span class="number">300</span>)</span><br><span class="line">region = image.crop(box)</span><br><span class="line">region = region.transpose(Image.ROTATE_180)</span><br><span class="line">image.paste(region, box)</span><br><span class="line">image.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image, ImageFilter</span><br><span class="line"><span class="comment"># 打开一个jpg图像文件，注意是当前路径:</span></span><br><span class="line">im = Image.<span class="built_in">open</span>(<span class="string">&#x27;test.jpg&#x27;</span>)</span><br><span class="line"><span class="comment"># 应用模糊滤镜:</span></span><br><span class="line">im2 = im.<span class="built_in">filter</span>(ImageFilter.BLUR)</span><br></pre></td></tr></table></figure><p><img src="E:\笔记\markdown\reference\picture\v2-6eb679436bfcefcf51d57c95d5e1ef10_720w.jpg" alt="img"></p><ul><li>  通过定义一个 4 元组，依次为左上角 X 坐标、Y 坐标，右下角 X 坐标、Y 坐标，可以対原图片的某一区域进行裁剪，然后进行一定处理后可以在原位置粘贴回去。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">im = image.resize((300, 300))</span><br><span class="line">im = image.rotate(45)  # 逆时针旋转 45 度</span><br><span class="line">im = image.transpose(Image.FLIP_LEFT_RIGHT) # 左右翻转</span><br><span class="line">im = im.transpose(Image.FLIP_TOP_BOTTOM)# 上下翻转</span><br><span class="line"># 缩放到50%:</span><br><span class="line">im.thumbnail((w//2, h//2))</span><br></pre></td></tr></table></figure><h3 id="6-像素值操作"><a href="#6-像素值操作" class="headerlink" title="6. 像素值操作"></a>6. 像素值操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">out = image.point(<span class="keyword">lambda</span> i: i * <span class="number">1.2</span>) <span class="comment"># 对每个像素值乘以 1.2</span></span><br><span class="line">source = image.split()</span><br><span class="line">out = source[<span class="number">0</span>].point(<span class="keyword">lambda</span> i: i &gt; <span class="number">128</span> <span class="keyword">and</span> <span class="number">255</span>) <span class="comment"># 对 R 通道进行二值化</span></span><br></pre></td></tr></table></figure><p><img src="E:\笔记\markdown\reference\picture\v2-8317adc4f04515dec04330c85bd332c7_720w.jpg" alt="img"></p><ul><li>  i &gt; 128 and 255，当 i &lt;= 128 时，返回 False 也即 0,；反之返回 255 。</li></ul><h3 id="7-和-Numpy-数组之间的转化"><a href="#7-和-Numpy-数组之间的转化" class="headerlink" title="7. 和 Numpy 数组之间的转化"></a>7. 和 Numpy 数组之间的转化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">array = np.array(image)</span><br><span class="line"><span class="built_in">print</span>(array.shape) <span class="comment">#(321, 481, 3)</span></span><br><span class="line">image = Image.fromarray(array)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#tesnor 转pil</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">image = x.cpu().clone()</span><br><span class="line">image = image.squeeze(<span class="number">0</span>)  <span class="comment"># 压缩一维</span></span><br><span class="line">image = transforms.ToPILImage()(image)  <span class="comment"># 自动转换为0-255</span></span><br><span class="line"><span class="comment"># image.show()</span></span><br><span class="line">file = <span class="string">&quot;./pic/&quot;</span> + <span class="built_in">str</span>(step) + <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">Image.Image.save(image, file)</span><br></pre></td></tr></table></figure><h3 id="8-绘图"><a href="#8-绘图" class="headerlink" title="8.绘图"></a>8.绘图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image, ImageDraw, ImageFont, ImageFilter</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机字母:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rndChar</span>():</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">chr</span>(random.randint(<span class="number">65</span>, <span class="number">90</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机颜色1:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rndColor</span>():</span></span><br><span class="line">    <span class="keyword">return</span> (random.randint(<span class="number">64</span>, <span class="number">255</span>), random.randint(<span class="number">64</span>, <span class="number">255</span>), random.randint(<span class="number">64</span>, <span class="number">255</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机颜色2:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rndColor2</span>():</span></span><br><span class="line">    <span class="keyword">return</span> (random.randint(<span class="number">32</span>, <span class="number">127</span>), random.randint(<span class="number">32</span>, <span class="number">127</span>), random.randint(<span class="number">32</span>, <span class="number">127</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 240 x 60:</span></span><br><span class="line">width = <span class="number">60</span> * <span class="number">4</span></span><br><span class="line">height = <span class="number">60</span></span><br><span class="line">image = Image.new(<span class="string">&#x27;RGB&#x27;</span>, (width, height), (<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>))</span><br><span class="line"><span class="comment"># 创建Font对象:</span></span><br><span class="line">font = ImageFont.truetype(<span class="string">&#x27;Arial.ttf&#x27;</span>, <span class="number">36</span>)</span><br><span class="line"><span class="comment"># 创建Draw对象:</span></span><br><span class="line">draw = ImageDraw.Draw(image)</span><br><span class="line"><span class="comment"># 填充每个像素:</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(width):</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(height):</span><br><span class="line">        draw.point((x, y), fill=rndColor())</span><br><span class="line"><span class="comment"># 输出文字:</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">    draw.text((<span class="number">60</span> * t + <span class="number">10</span>, <span class="number">10</span>), rndChar(), font=font, fill=rndColor2())</span><br><span class="line"><span class="comment"># 模糊:</span></span><br><span class="line">image = image.<span class="built_in">filter</span>(ImageFilter.BLUR)</span><br><span class="line">image.save(<span class="string">&#x27;code.jpg&#x27;</span>, <span class="string">&#x27;jpeg&#x27;</span>)</span><br></pre></td></tr></table></figure><p><a href="https://pillow.readthedocs.org/">官方文档</a></p><h2 id="plt"><a href="#plt" class="headerlink" title="plt"></a>plt</h2><h2 id="opencv"><a href="#opencv" class="headerlink" title="opencv"></a>opencv</h2><p>openCV具体笔记见对应的ipynb文件</p><p>注意：</p><ol><li> ​    opencv的读取格式为：W,H,C，其中C是BGR的，plt等是RGB的。</li></ol><h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><h3 id="一、-概念"><a href="#一、-概念" class="headerlink" title="一、 概念"></a>一、 概念</h3><p>在图像分类任务中，图像数据的增广是一种常用的<strong>正则化方法</strong>，主要用于增加训练数据集，让数据集尽可能的多样化，使得训练的模型具有更强的<strong>泛化能力</strong>，常用于数据量不足或者模型参数较多的场景。除了 ImageNet 分类任务标准数据增广方法外，还有8种数据增广方式非常常用，这里对其进行简单的介绍和对比，大家也可以将这些增广方法应用到自己的任务中，以获得模型精度的提升。这8种数据增广方式在ImageNet上的精度指标如 <strong>图1</strong> 所示。</p><p><img src="https://paddlepedia.readthedocs.io/en/latest/_images/main_image_aug.png" alt="img"></p><center>图一 8种数据增广方法</center><h3 id="二、常用数据增广方法"><a href="#二、常用数据增广方法" class="headerlink" title="二、常用数据增广方法"></a>二、<a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#id2">常用数据增广方法</a></h3><blockquote><p>注：如果没有特殊说明，本章节中所有示例为 ImageNet 分类，并且假设最终输入网络的数据维为：[batch-size, 3, 224, 224]</p></blockquote><p>在ImageNet 分类任务中，训练阶段的标准数据增广方法为以下几步：</p><ol><li><p>图像解码：简写为 <code>ImageDecode</code></p></li><li><p>随机裁剪到长宽均为 224 的图像：简写为 <code>RandCrop</code></p></li><li><p>水平方向随机翻转：简写为 <code>RandFlip</code></p></li><li><p>图像数据的归一化：简写为 <code>Normalize</code></p></li><li><p>图像数据的重排，<code>[224, 224, 3]</code> 变为 <code>[3, 224, 224]</code>：简写为 <code>Transpose</code></p></li><li><p>多幅图像数据组成 batch 数据，如 <code>batch-size</code> 个 <code>[3, 224, 224]</code> 的图像数据拼组成 <code>[batch-size, 3, 224, 224]</code>：简写为 <code>Batch</code></p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基于pytorch框架实现以上基本的图像增广操作</span></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>相比于上述标准的图像增广方法，研究者也提出了很多改进的图像增广策略，这些策略均是在标准增广方法的不同阶段插入一定的操作，基于这些策略操作所处的不同阶段，我们将其分为了三类：</p><ol><li><p>对 <code>RandCrop</code> (上述的阶段2)后的 224 的图像进行一些变换: AutoAugment，RandAugment</p></li><li><p>对<code>Transpose</code> (上述的阶段5)后的 224 的图像进行一些裁剪: CutOut，RandErasing，HideAndSeek，GridMask</p></li><li><p>对 <code>Batch</code>(上述的阶段6) 后的数据进行混合: Mixup，Cutmix</p></li></ol><p>增广后的可视化效果如 <strong>图2</strong> 所示。</p><p><img src="E:\笔记\markdown\reference\picture\image_aug_samples_s.jpg" alt="图2 数据增广后可视化"></p><center> 图2 数据增广后可视化</center><p>下文将介绍这些策略的原理与使用方法，其中，每种数据增广策略的参考论文与参考开源代码均在下面的介绍中列出。</p><p>以 <strong>图3</strong> 为测试图像，第三节将基于测试图像进行变换，并将变换后的效果进行可视化。</p><blockquote><p>由于<code>RandCrop</code>是随机裁剪，变换前后的图像内容可能会有一定的差别，无法直观地对比变换前后的图像。因此，本节将 <code>RandCrop</code> 替换为 <code>Resize</code>。</p></blockquote><p><img src="E:\笔记\markdown\reference\picture\test_baseline.jpeg" alt="图3 测试图像"></p><center>图3 测试图像</center><h3 id="三、图像变换类"><a href="#三、图像变换类" class="headerlink" title="三、图像变换类"></a>三、<a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#id3">图像变换类</a></h3><p>图像变换类指的是对 <code>RandCrop</code> 后的224 的图像进行一些变换，主要包括：</p><ul><li>AutoAugment[1]</li><li>RandAugment[2]</li></ul><h4 id="3-1-AutoAugment"><a href="#3-1-AutoAugment" class="headerlink" title="3.1 AutoAugment"></a>3.1 <a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#autoaugment">AutoAugment</a></h4><p>论文地址：<a href="https://arxiv.org/abs/1805.09501v1">https://arxiv.org/abs/1805.09501v1</a></p><p>开源代码github地址：<a href="https://github.com/DeepVoltaire/AutoAugment">https://github.com/DeepVoltaire/AutoAugment</a></p><p>不同于常规的人工设计图像增广方式，AutoAugment 是在一系列图像增广子策略的搜索空间中通过搜索算法找到的适合特定数据集的图像增广方案。针对 ImageNet 数据集，最终搜索出来的数据增广方案包含 25 个子策略组合，每个子策略中都包含两种变换，针对每幅图像都随机的挑选一个子策略组合，然后以一定的概率来决定是否执行子策略中的每种变换。</p><p>结果如 <strong>图4</strong> 所示。</p><p><img src="E:\笔记\markdown\reference\picture\test_autoaugment.jpeg" alt="图4 AutoAugment后图像可视化"></p><center>图4 AutoAugment后图像可视化</center><h4 id="3-2-RandAugment"><a href="#3-2-RandAugment" class="headerlink" title="3.2 RandAugment"></a>3.2 <a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#randaugment">RandAugment</a></h4><p>论文地址：<a href="https://arxiv.org/pdf/1909.13719.pdf">https://arxiv.org/pdf/1909.13719.pdf</a></p><p>开源代码github地址：<a href="https://github.com/heartInsert/randaugment">https://github.com/heartInsert/randaugment</a></p><p><code>AutoAugment</code> 的搜索方法比较暴力，直接在数据集上搜索针对该数据集的最优策略，其计算量很大。在 <code>RandAugment</code> 文章中作者发现，一方面，针对越大的模型，越大的数据集，使用 <code>AutoAugment</code> 方式搜索到的增广方式产生的收益也就越小；另一方面，这种搜索出的最优策略是针对该数据集的，其迁移能力较差，并不太适合迁移到其他数据集上。</p><p>在 <code>RandAugment</code> 中，作者提出了一种随机增广的方式，不再像 <code>AutoAugment</code> 中那样使用特定的概率确定是否使用某种子策略，而是所有的子策略都会以同样的概率被选择到，论文中的实验也表明这种数据增广方式即使在大模型的训练中也具有很好的效果。</p><p>结果如 <strong>图5</strong> 所示。</p><p><img src="E:\笔记\markdown\reference\picture\test_randaugment.jpeg" alt="图5 RandAugment后图像可视化"></p><center>图5 RandAugment后图像可视化</center><h3 id="四、图像裁剪类"><a href="#四、图像裁剪类" class="headerlink" title="四、图像裁剪类"></a>四、<a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#id4">图像裁剪类</a></h3><p>图像裁剪类主要是对<code>Transpose</code> 后的 224 的图像进行一些裁剪，并将裁剪区域的像素值置为特定的常数（默认为0），主要包括：</p><ul><li>CutOut[3]</li><li>RandErasing[4]</li><li>HideAndSeek[5]</li><li>GridMask[6]</li></ul><p>图像裁剪的这些增广并非一定要放在归一化之后，也有不少实现是放在归一化之前的，也就是直接对 uint8 的图像进行操作，两种方式的差别是：如果直接对 uint8 的图像进行操作，那么再经过归一化之后被裁剪的区域将不再是纯黑或纯白（减均值除方差之后像素值不为0）。而对归一后之后的数据进行操作，裁剪的区域会是纯黑或纯白。</p><p>上述的裁剪变换思路是相同的，都是为了解决训练出的模型在有遮挡数据上泛化能力较差的问题，不同的是他们的裁剪方式、区域不太一样。</p><h4 id="4-1-Cutout"><a href="#4-1-Cutout" class="headerlink" title="4.1 Cutout"></a>4.1 <a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#cutout">Cutout</a></h4><p>论文地址：<a href="https://arxiv.org/abs/1708.04552">https://arxiv.org/abs/1708.04552</a></p><p>开源代码github地址：<a href="https://github.com/uoguelph-mlrg/Cutout">https://github.com/uoguelph-mlrg/Cutout</a></p><p>Cutout 可以理解为 Dropout 的一种扩展操作，不同的是 Dropout 是对图像经过网络后生成的特征进行遮挡，而 Cutout 是直接对输入的图像进行遮挡，相对于Dropout，Cutout 对噪声的鲁棒性更好。作者在论文中也进行了说明，这样做法有以下两点优势：(1) 通过 Cutout 可以模拟真实场景中主体被部分遮挡时的分类场景；(2) 可以促进模型充分利用图像中更多的内容来进行分类，防止网络只关注显著性的图像区域，从而发生过拟合。</p><p>结果如 <strong>图6</strong> 所示。</p><p><img src="E:\笔记\markdown\reference\picture\test_cutout.jpeg" alt="图6 Cutout后图像可视化"></p><center>图6 Cutout后图像可视化</center><h4 id="4-2-RandomErasing"><a href="#4-2-RandomErasing" class="headerlink" title="4.2 RandomErasing"></a>4.2 <a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#randomerasing">RandomErasing</a></h4><p>论文地址：<a href="https://arxiv.org/pdf/1708.04896.pdf">https://arxiv.org/pdf/1708.04896.pdf</a></p><p>开源代码github地址：<a href="https://github.com/zhunzhong07/Random-Erasing">https://github.com/zhunzhong07/Random-Erasing</a></p><p><code>RandomErasing</code> 与 <code>Cutout</code> 方法类似，同样是为了解决训练出的模型在有遮挡数据上泛化能力较差的问题，作者在论文中也指出，随机裁剪的方式与随机水平翻转具有一定的互补性。作者也在行人再识别（REID）上验证了该方法的有效性。与<code>Cutout</code>不同的是，在<code>RandomErasing</code>中，图片以一定的概率接受该种预处理方法，生成掩码的尺寸大小与长宽比也是根据预设的超参数随机生成。</p><p>结果如 <strong>图7</strong> 所示。</p><p><img src="E:\笔记\markdown\reference\picture\test_randomerassing.jpeg" alt="图7 RandomErasing后图像可视化"></p><center>图7 RandomErasing后图像可视化</center><h4 id="4-3-HideAndSeek"><a href="#4-3-HideAndSeek" class="headerlink" title="4.3 HideAndSeek"></a>4.3 <a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#hideandseek">HideAndSeek</a></h4><p>论文地址：<a href="https://arxiv.org/pdf/1811.02545.pdf">https://arxiv.org/pdf/1811.02545.pdf</a></p><p>开源代码github地址：<a href="https://github.com/kkanshul/Hide-and-Seek">https://github.com/kkanshul/Hide-and-Seek</a></p><p><code>HideAndSeek</code>论文将图像分为若干块区域(patch)，对于每块区域，都以一定的概率生成掩码，不同区域的掩码含义如 <strong>图8</strong> 所示。</p><p><img src="E:\笔记\markdown\reference\picture\hide-and-seek-visual.png" alt="图8 HideAndSeek分块掩码图"></p><center>图8 HideAndSeek分块掩码图</center><p>结果如 <strong>图9</strong> 所示。</p><p><img src="E:\笔记\markdown\reference\picture\test_hideandseek.jpeg" alt="图9 HideAndSeek后图像可视化"></p><center>图9 HideAndSeek后图像可视化</center><h4 id="4-4-GridMask"><a href="#4-4-GridMask" class="headerlink" title="4.4 GridMask"></a>4.4 <a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#gridmask">GridMask</a></h4><p>论文地址：<a href="https://arxiv.org/abs/2001.04086">https://arxiv.org/abs/2001.04086</a></p><p>开源代码github地址：<a href="https://github.com/akuxcw/GridMask">https://github.com/akuxcw/GridMask</a></p><p>作者在论文中指出，此前存在的基于对图像 crop 的方法存在两个问题，如 <strong>图10</strong> 所示：</p><ol><li><p>过度删除区域可能造成目标主体大部分甚至全部被删除，或者导致上下文信息的丢失，导致增广后的数据成为噪声数据；</p></li><li><p>保留过多的区域，对目标主体及上下文基本产生不了什么影响，失去增广的意义。</p></li></ol><p><img src="E:\笔记\markdown\reference\picture\gridmask-0.png" alt="图10 增广后的噪声数据"></p><center>图10 增广后的噪声数据</center><p>因此如果避免过度删除或过度保留成为需要解决的核心问题。</p><p><code>GridMask</code>是通过生成一个与原图分辨率相同的掩码，并将掩码进行随机翻转，与原图相乘，从而得到增广后的图像，通过超参数控制生成的掩码网格的大小。</p><p>在训练过程中，有两种以下使用方法：</p><ol><li>设置一个概率p，从训练开始就对图片以概率p使用<code>GridMask</code>进行增广。</li><li>一开始设置增广概率为0，随着迭代轮数增加，对训练图片进行<code>GridMask</code>增广的概率逐渐增大，最后变为p。</li></ol><p>论文中验证上述第二种方法的训练效果更好一些。</p><p>结果如 <strong>图11</strong> 所示。</p><p><img src="E:\笔记\markdown\reference\picture\test_gridmask.jpeg" alt="图11 GridMask后图像可视化"></p><center>图11 GridMask后图像可视化</center><h3 id="五、图像混叠"><a href="#五、图像混叠" class="headerlink" title="五、图像混叠"></a>五、<a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#id5">图像混叠</a></h3><p>图像混叠主要对 <code>Batch</code> 后的数据进行混合，包括：</p><ul><li>Mixup[7]</li><li>Cutmix[8]</li></ul><p>前文所述的图像变换与图像裁剪都是针对单幅图像进行的操作，而图像混叠是对两幅图像进行融合，生成一幅图像，两种方法的主要区别为混叠的方式不太一样。</p><h4 id="5-1-Mixup"><a href="#5-1-Mixup" class="headerlink" title="5.1 Mixup"></a>5.1 <a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#mixup">Mixup</a></h4><p>论文地址：<a href="https://arxiv.org/pdf/1710.09412.pdf">https://arxiv.org/pdf/1710.09412.pdf</a></p><p>开源代码github地址：<a href="https://github.com/facebookresearch/mixup-cifar10">https://github.com/facebookresearch/mixup-cifar10</a></p><p>Mixup 是最先提出的图像混叠增广方案，其原理简单、方便实现，不仅在图像分类上，在目标检测上也取得了不错的效果。为了便于实现，通常只对一个 batch 内的数据进行混叠，在 <code>Cutmix</code> 中也是如此。</p><p>如下是 <code>imaug</code> 中的实现，需要指出的是，下述实现会出现对同一幅进行相加的情况，也就是最终得到的图和原图一样，随着 <code>batch-size</code> 的增加这种情况出现的概率也会逐渐减小。</p><p>结果如 <strong>图12</strong> 所示。</p><p><img src="E:\笔记\markdown\reference\picture\test_mixup.png" alt="图12 Mixup后图像可视化"></p><center>图12 Mixup后图像可视化</center><h4 id="5-2-Cutmix"><a href="#5-2-Cutmix" class="headerlink" title="5.2 Cutmix"></a>5.2 <a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#cutmix">Cutmix</a></h4><p>论文地址：<a href="https://arxiv.org/pdf/1905.04899v2.pdf">https://arxiv.org/pdf/1905.04899v2.pdf</a></p><p>开源代码github地址：<a href="https://github.com/clovaai/CutMix-PyTorch">https://github.com/clovaai/CutMix-PyTorch</a></p><p>与 <code>Mixup</code> 直接对两幅图进行相加不一样，<code>Cutmix</code> 是从一幅图中随机裁剪出一个 <code>ROI</code>，然后覆盖当前图像中对应的区域。</p><p>结果如 <strong>图13</strong> 所示。</p><p><img src="E:\笔记\markdown\reference\picture\test_cutmix.png" alt="图13 Cutmix后图像可视化"></p><center>图13 Cutmix后图像可视化</center><h3 id="六、实验"><a href="#六、实验" class="headerlink" title="六、实验"></a>六、<a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#id6">实验</a></h3><p>基于PaddleClas套件，使用上述几种数据增广方法在ImageNet1k数据集上进行了实验测试，每个方法的分类精度如下。</p><table><thead><tr><th>模型</th><th>初始学习率策略</th><th>l2 decay</th><th>batch size</th><th>epoch</th><th>数据变化策略</th><th>Top1 Acc</th><th>论文中结论</th></tr></thead><tbody><tr><td>ResNet50</td><td>0.1/cosine_decay</td><td>0.0001</td><td>256</td><td>300</td><td>标准变换</td><td>0.7731</td><td>-</td></tr><tr><td>ResNet50</td><td>0.1/cosine_decay</td><td>0.0001</td><td>256</td><td>300</td><td>AutoAugment</td><td>0.7795</td><td>0.7763</td></tr><tr><td>ResNet50</td><td>0.1/cosine_decay</td><td>0.0001</td><td>256</td><td>300</td><td>mixup</td><td>0.7828</td><td>0.7790</td></tr><tr><td>ResNet50</td><td>0.1/cosine_decay</td><td>0.0001</td><td>256</td><td>300</td><td>cutmix</td><td>0.7839</td><td>0.7860</td></tr><tr><td>ResNet50</td><td>0.1/cosine_decay</td><td>0.0001</td><td>256</td><td>300</td><td>cutout</td><td>0.7801</td><td>-</td></tr><tr><td>ResNet50</td><td>0.1/cosine_decay</td><td>0.0001</td><td>256</td><td>300</td><td>gridmask</td><td>0.7785</td><td>0.7790</td></tr><tr><td>ResNet50</td><td>0.1/cosine_decay</td><td>0.0001</td><td>256</td><td>300</td><td>random-augment</td><td>0.7770</td><td>0.7760</td></tr><tr><td>ResNet50</td><td>0.1/cosine_decay</td><td>0.0001</td><td>256</td><td>300</td><td>random erasing</td><td>0.7791</td><td>-</td></tr><tr><td>ResNet50</td><td>0.1/cosine_decay</td><td>0.0001</td><td>256</td><td>300</td><td>hide and seek</td><td>0.7743</td><td>0.7720</td></tr></tbody></table><p><strong>注意</strong>：</p><ul><li>在实验中，为了便于对比，将l2 decay固定设置为1e-4，在实际使用中，推荐尝试使用更小的l2 decay。结合数据增广，发现将l2 decay由1e-4减小为7e-5均能带来至少0.3~0.5%的精度提升。</li><li>在使用数据增广后，由于训练数据更难，所以训练损失函数可能较大，训练集的准确率相对较低，但其拥有更好的泛化能力，所以验证集的准确率相对较高。</li><li>在使用数据增广后，模型可能会趋于欠拟合状态，建议可以适当的调小<code>l2_decay</code>的值来获得更高的验证集准确率。</li></ul><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a><a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#id7">参考文献</a></h3><p>[1] <a href="https://arxiv.org/abs/1805.09501v1">Autoaugment: Learning augmentation strategies from data</a></p><p>[2] <a href="https://arxiv.org/pdf/1909.13719.pdf">Randaugment: Practical automated data augmentation with a reduced search space</a></p><p>[3] <a href="https://arxiv.org/abs/1708.04552">Improved regularization of convolutional neural networks with cutout</a></p><p>[4] <a href="https://arxiv.org/pdf/1708.04896.pdf">Random erasing data augmentation</a></p><p>[5] <a href="https://arxiv.org/pdf/1811.02545.pdf">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</a></p><p>[6] <a href="https://arxiv.org/abs/2001.04086">GridMask Data Augmentation</a></p><p>[7] <a href="https://arxiv.org/pdf/1710.09412.pdf">mixup: Beyond empirical risk minimization</a></p><p>[8] <a href="https://arxiv.org/pdf/1905.04899v2.pdf">Cutmix: Regularization strategy to train strong classifiers with localizable features</a>)</p><h3 id="实验中用到的增强方法总结"><a href="#实验中用到的增强方法总结" class="headerlink" title="实验中用到的增强方法总结"></a>实验中用到的增强方法总结</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># transforms 类</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transforms</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        transform= [</span><br><span class="line">                transforms.RandomResizedCrop(size=args.image_size,scale=(<span class="number">0.2</span>, <span class="number">1.0</span>),ratio=(<span class="number">3</span> / <span class="number">4</span>, <span class="number">4</span> / <span class="number">3</span>)),</span><br><span class="line">                transforms.ColorJitter(brightness=<span class="number">0.4</span>, contrast=<span class="number">0.4</span>, saturation=<span class="number">0.4</span>),</span><br><span class="line">                transforms.RandomGrayscale(p=<span class="number">0.2</span>),</span><br><span class="line">                transforms.ToTensor(),</span><br><span class="line">                transforms.Normalize(mean=[<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>],std=[<span class="number">0.2470</span>, <span class="number">0.2435</span>, <span class="number">0.2616</span>])</span><br><span class="line">        ]</span><br><span class="line">        self.train_transform = transforms.Compose(transform)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.train_transform(x),self.train_transform(x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 调用方法一</span></span><br><span class="line">dataset = CIFAR100(root=args.dataset_root,train=<span class="literal">False</span>,download=<span class="literal">True</span>,transform=Transforms())</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;PIL&quot;&gt;&lt;a href=&quot;#PIL&quot; class=&quot;headerlink&quot; title=&quot;PIL&quot;&gt;&lt;/a&gt;PIL&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;  PIL，全称 Python Imaging Library，是一个功能非常强大而且简单易用的图像处理库</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>特色包</title>
    <link href="https://jpccc.github.io/2022/04/07/%E7%89%B9%E8%89%B2%E5%8C%85/"/>
    <id>https://jpccc.github.io/2022/04/07/%E7%89%B9%E8%89%B2%E5%8C%85/</id>
    <published>2022-04-07T10:10:44.000Z</published>
    <updated>2022-04-10T04:05:07.816Z</updated>
    
    <content type="html"><![CDATA[<h1 id="控制台颜色"><a href="#控制台颜色" class="headerlink" title="控制台颜色"></a>控制台颜色</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> termcolor <span class="keyword">import</span> colored</span><br><span class="line"><span class="built_in">print</span>(colored(<span class="string">&#x27;Fill memory bank for mining the nearest neighbors (train) ...&#x27;</span>, <span class="string">&#x27;blue&#x27;</span>))</span><br></pre></td></tr></table></figure><h1 id="Logging"><a href="#Logging" class="headerlink" title="Logging"></a>Logging</h1><blockquote><p>  This module defines functions and classes which implement a flexible event logging system for applications and libraries.</p><p>  Python logging 模块定义了为应用程序和库实现灵活的事件日志记录的函数和类。</p></blockquote><p>程序开发过程中，很多程序都有记录日志的需求，并且日志包含的信息有正常的程序访问日志还可能有错误、警告等信息输出，Python 的 logging 模块提供了标准的日志接口，可以通过它存储各种格式的日志,日志记录提供了一组便利功能，用于简单的日志记录用法。</p><ul><li>  使用 Python Logging 模块的主要好处是所有 Python 模块都可以参与日志记录</li><li>  Logging 模块提供了大量具有灵活性的功能</li></ul><p><strong>日志记录函数以它们用来跟踪的事件的级别或严重性命名。下面描述了标准级别及其适用性（从高到低的顺序）：</strong></p><table><thead><tr><th>日志等级(level)</th><th>描述</th></tr></thead><tbody><tr><td></td><td></td></tr></tbody></table><p><strong>日志级别等级排序</strong>：critical &gt; error &gt; warning &gt; info &gt; debug</p><p><strong>级别越高打印的日志越少，反之亦然，即</strong></p><ul><li>  debug : 打印全部的日志( notset 等同于 debug )</li><li>  info : 打印 info, warning, error, critical 级别的日志</li><li>  warning : 打印 warning, error, critical 级别的日志</li><li>  error : 打印 error, critical 级别的日志</li><li>  critical : 打印 critical 级别</li></ul><h2 id="一、-Logging-模块日志记录方式"><a href="#一、-Logging-模块日志记录方式" class="headerlink" title="一、 Logging 模块日志记录方式"></a><strong>一、 Logging 模块日志记录方式</strong></h2><p>Logging 模块提供了两种日志记录方式：</p><ul><li>  一种方式是使用 Logging 提供的模块级别的函数</li><li>  另一种方式是使用 Logging 日志系统的四大组件记录</li></ul><h2 id="1、Logging-定义的模块级别函数"><a href="#1、Logging-定义的模块级别函数" class="headerlink" title="1、Logging 定义的模块级别函数"></a><strong>1、Logging 定义的模块级别函数</strong></h2><table><thead><tr><th>函数</th><th>说明</th></tr></thead></table><p>简单打印日志：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印日志级别</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_logging</span>():</span></span><br><span class="line">    logging.debug(<span class="string">&#x27;Python debug&#x27;</span>)</span><br><span class="line">    logging.info(<span class="string">&#x27;Python info&#x27;</span>)</span><br><span class="line">    logging.warning(<span class="string">&#x27;Python warning&#x27;</span>)</span><br><span class="line">    logging.error(<span class="string">&#x27;Python Error&#x27;</span>)</span><br><span class="line">    logging.critical(<span class="string">&#x27;Python critical&#x27;</span>)</span><br><span class="line"></span><br><span class="line">test_logging()</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">WARNING:root:Python warning</span><br><span class="line">ERROR:root:Python Error</span><br><span class="line">CRITICAL:root:Python critical</span><br></pre></td></tr></table></figure><p>当指定一个日志级别之后，会记录大于或等于这个日志级别的日志信息，小于的将会被丢弃， ==默认情况下日志打印只显示大于等于 WARNING 级别的日志。==</p><h3 id="1-1-设置日志显示级别"><a href="#1-1-设置日志显示级别" class="headerlink" title="1.1 设置日志显示级别"></a><strong>1.1 设置日志显示级别</strong></h3><p>通过 logging.basicConfig() 可以设置 root 的日志级别，和日志输出格式。</p><p><strong>logging.basicConfig() 关键字参数</strong>：</p><table><thead><tr><th>关键字</th><th>描述</th></tr></thead><tbody><tr><td></td><td></td></tr></tbody></table><p><strong>format 格式</strong></p><table><thead><tr><th>格式</th><th>描述</th></tr></thead><tbody><tr><td></td><td></td></tr></tbody></table><p><strong>注意</strong>：Logging.basicConfig() 需要在开头就设置，在中间设置并无作用</p><p><strong>实例</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import logging</span><br><span class="line"></span><br><span class="line"># 打印日志级别</span><br><span class="line">def test():</span><br><span class="line">    logging.basicConfig(level=logging.DEBUG)</span><br><span class="line">    logging.debug(&#x27;Python debug&#x27;)</span><br><span class="line">    logging.info(&#x27;Python info&#x27;)</span><br><span class="line">    logging.warning(&#x27;Python warning&#x27;)</span><br><span class="line">    logging.error(&#x27;Python Error&#x27;)</span><br><span class="line">    logging.critical(&#x27;Python critical&#x27;)</span><br><span class="line">    logging.log(2,&#x27;test&#x27;)</span><br><span class="line">test()</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DEBUG:root:Python debug</span><br><span class="line">INFO:root:Python info</span><br><span class="line">WARNING:root:Python warning</span><br><span class="line">ERROR:root:Python Error</span><br><span class="line">CRITICAL:root:Python critical</span><br></pre></td></tr></table></figure><h3 id="1-2-将日志信息记录到文件"><a href="#1-2-将日志信息记录到文件" class="headerlink" title="1.2 将日志信息记录到文件"></a><strong>1.2 将日志信息记录到文件</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 日志信息记录到文件</span><br><span class="line">logging.basicConfig(filename=&#x27;F:/example.log&#x27;, level=logging.DEBUG)</span><br><span class="line">logging.debug(&#x27;This message should go to the log file&#x27;)</span><br><span class="line">logging.info(&#x27;So should this&#x27;)</span><br><span class="line">logging.warning(&#x27;And this, too&#x27;)</span><br></pre></td></tr></table></figure><p>在相应的路径下会有 example.log 日志文件，内容如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DEBUG:root:This message should go to the log file</span><br><span class="line">INFO:root:So should this</span><br><span class="line">WARNING:root:And this, too</span><br></pre></td></tr></table></figure><h3 id="1-3-多个模块记录日志信息"><a href="#1-3-多个模块记录日志信息" class="headerlink" title="1.3 多个模块记录日志信息"></a><strong>1.3 多个模块记录日志信息</strong></h3><p>如果程序包含多个模块，则用以下实例来显示日志信息： 实例中有两个模块，一个模块通过导入另一个模块的方式用日志显示另一个模块的信息：</p><p><strong>myapp.py 模块</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import logging</span><br><span class="line">import mylib</span><br><span class="line">def main():</span><br><span class="line">    logging.basicConfig(filename=&#x27;myapp.log&#x27;,level=logging.DEBUG)</span><br><span class="line">    logging.info(&#x27;Started&#x27;)</span><br><span class="line">    mylib.do_something()</span><br><span class="line">    logging.info(&#x27;Finished&#x27;)</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p><strong>mylib.py 模块</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import logging</span><br><span class="line"></span><br><span class="line">def do_something():</span><br><span class="line">    logging.info(&#x27;Doing something&#x27;)</span><br></pre></td></tr></table></figure><p>执行 myapp.py 模块会打印相应日志，在文件 myapp.log 中显示信息如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">INFO:root:Started</span><br><span class="line">INFO:root:Doing something</span><br><span class="line">INFO:root:Finishe</span><br></pre></td></tr></table></figure><h3 id="1-4-显示信息的日期及更改显示消息格式"><a href="#1-4-显示信息的日期及更改显示消息格式" class="headerlink" title="1.4 显示信息的日期及更改显示消息格式"></a><strong>1.4 显示信息的日期及更改显示消息格式</strong></h3><p><strong>显示消息日期</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import logging</span><br><span class="line"># 显示消息时间</span><br><span class="line">logging.basicConfig(format=&#x27;%(asctime)s %(message)s&#x27;)</span><br><span class="line">logging.warning(&#x27;is when this event was logged.&#x27;)</span><br><span class="line"></span><br><span class="line">logging.basicConfig(format=&#x27;%(asctime)s %(message)s&#x27;, datefmt=&#x27;%m/%d/%Y %I:%M:%S %p&#x27;)</span><br><span class="line">logging.warning(&#x27;is when this event was logged.&#x27;)</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2019-10-16 18:57:45,988 is when this event was logged.</span><br><span class="line">2019-10-16 18:57:45,988 is when this event was logged.</span><br></pre></td></tr></table></figure><p><strong>更改显示消息格式</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import logging</span><br><span class="line"># 更改显示消息的格式</span><br><span class="line">logging.basicConfig(format=&#x27;%(levelname)s:%(message)s&#x27;,level=logging.DEBUG)</span><br><span class="line">logging.debug(&#x27;Python message format Debug&#x27;)</span><br><span class="line">logging.info(&#x27;Python message format Info&#x27;)</span><br><span class="line">logging.warning(&#x27;Python message format Warning&#x27;)</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DEBUG:Python message format Debug</span><br><span class="line">INFO:Python message format Info</span><br><span class="line">WARNING:Python message format Warning</span><br></pre></td></tr></table></figure><p>==注意==：显示结果只显示级别和具体信息，之前显示的 “根” 已经消失，重新定义的格式修改了默认输出方式。</p><h2 id="2、logging-模块四大组件"><a href="#2、logging-模块四大组件" class="headerlink" title="2、logging 模块四大组件"></a><strong>2、logging 模块四大组件</strong></h2><table><thead><tr><th>组件名称</th><th>对应类名</th><th>功能描述</th></tr></thead><tbody><tr><td></td><td></td><td></td></tr></tbody></table><h3 id="2-1-日志器-Logger"><a href="#2-1-日志器-Logger" class="headerlink" title="2.1 日志器- Logger"></a><strong>2.1 日志器- Logger</strong></h3><p>Logger 持有日志记录器的方法，日志记录器不直接实例化，而是通过模块级函数 logger.getlogger (name) 来实例化,使用相同的名称多次调用 getLogger() 总是会返回对相同 Logger 对象的引用。</p><ul><li>  应用程序代码能直接调用日志接口。</li><li>  Logger最常用的操作有两类：配置和发送日志消息。</li><li>  初始化 logger = logging.getLogger(“endlesscode”)，获取 logger 对象，getLogger() 方法后面最好加上所要日志记录的模块名字，配置文件和打印日志格式中的 %(name)s 对应的是这里的模块名字，如果不指定name则返回root对象。</li><li>  logger.setLevel(logging.DEBUG)，Logging 中有 NOTSET &lt; DEBUG &lt; INFO &lt; WARNING &lt; ERROR &lt; CRITICAL这几种级别，日志会记录设置级别以上的日志</li><li>  多次使用相同的name调用 getLogger 方法返回同一个 looger 对象；</li></ul><p>Logger是一个树形层级结构，在使用接口 debug，info，warn，error，critical 之前必须创建 Logger 实例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">创建方法: logger = logging.getLogger(logger_name)</span><br></pre></td></tr></table></figure><p>创建Logger实例后，可以使用以下方法进行日志级别设置，增加处理器 Handler：</p><ul><li>  logger.setLevel(logging.ERROR) # 设置日志级别为 ERROR，即只有日志级别大于等于 ERROR 的日志才会输出</li><li>  logger.addHandler(handler_name) # 为 Logger 实例增加一个处理器</li><li>  logger.removeHandler(handler_name) # 为 Logger 实例删除一个处理器</li></ul><h3 id="2-2-处理器-Handler"><a href="#2-2-处理器-Handler" class="headerlink" title="2.2 处理器- Handler"></a><strong>2.2 处理器- Handler</strong></h3><p>Handler 处理器类型有很多种，比较常用的有三个，StreamHandler，FileHandler，NullHandler</p><p><strong>StreamHandler</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">创建方法：sh = logging.StreamHandler(stream=None)</span><br></pre></td></tr></table></figure><p>创建 StreamHandler 之后，可以通过使用以下方法设置日志级别，设置格式化器 Formatter，增加或删除过滤器 Filter：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ch.setLevel(logging.WARN) # 指定日志级别，低于WARN级别的日志将被忽略</span><br><span class="line"></span><br><span class="line">ch.setFormatter(formatter_name) # 设置一个格式化器formatter</span><br><span class="line"></span><br><span class="line">ch.addFilter(filter_name) # 增加一个过滤器，可以增加多个</span><br><span class="line"> </span><br><span class="line">ch.removeFilter(filter_name) # 删除一个过滤器</span><br></pre></td></tr></table></figure><h3 id="2-3-过滤器-Filter"><a href="#2-3-过滤器-Filter" class="headerlink" title="2.3 过滤器- Filter"></a><strong>2.3 过滤器- Filter</strong></h3><p>Handlers 和 Loggers 可以使用 Filters 来完成比级别更复杂的过滤。 Filter 基类只允许特定 Logger 层次以下的事件。 例如用 ‘A.B’ 初始化的 Filter 允许Logger ‘A.B’, ‘A.B.C’, ‘A.B.C.D’, ‘A.B.D’ 等记录的事件，logger‘A.BB’, ‘B.A.B’ 等就不行。 如果用空字符串来初始化，所有的事件都接受。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">创建方法: filter = logging.Filter(name=&#x27;&#x27;)</span><br></pre></td></tr></table></figure><h3 id="2-4-格式器-Formatter"><a href="#2-4-格式器-Formatter" class="headerlink" title="2.4 格式器- Formatter"></a><strong>2.4 格式器- Formatter</strong></h3><p>使用Formatter对象设置日志信息最后的规则、结构和内容，默认的时间格式为%Y-%m-%d %H:%M:%S。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">创建方法: formatter = logging.Formatter(fmt=None, datefmt=None)</span><br></pre></td></tr></table></figure><p>其中，fmt 是消息的格式化字符串，datefmt 是日期字符串。如果不指明 fmt，将使用 ‘%(message)s’ 。如果不指明 datefmt，将使用 ISO8601 日期格式。</p><h3 id="2-5-组件之间的关联关系"><a href="#2-5-组件之间的关联关系" class="headerlink" title="2.5 组件之间的关联关系"></a><strong>2.5 组件之间的关联关系</strong></h3><ul><li>  日志器（logger）需要通过处理器（handler）将日志信息输出到目标位置，不同的处理器（handler）可以将日志输出到不同的位置；</li><li>  日志器（logger）可以设置多个处理器（handler）将同一条日志记录输出到不同的位置；</li><li>  每个处理器（handler）都可以设置自己的过滤器（filter）实现日志过滤，从而只保留感兴趣的日志；</li><li>  每个处理器（handler）都可以设置自己的格式器（formatter）实现同一条日志以不同的格式输出到不同的地方。</li></ul><p>简明了说就是：日志器（logger）是入口，真正干活儿的是处理器（handler），处理器（handler）还可以通过过滤器（filter）和格式器（formatter）对要输出的日志内容做过滤和格式化等处理操作。</p><ul><li>  Logger 可以包含一个或多个 Handler 和 Filter</li><li>  Logger 与 Handler 或 Fitler 是一对多的关系</li><li>  一个 Logger 实例可以新增多 个 Handler，一个 Handler 可以新增多个格式化器或多个过滤器，而且日志级别将会继承。</li></ul><h2 id="二、Logging-日志工作流程"><a href="#二、Logging-日志工作流程" class="headerlink" title="二、Logging 日志工作流程"></a><strong>二、Logging 日志工作流程</strong></h2><h2 id="1、Logging-模块使用过程"><a href="#1、Logging-模块使用过程" class="headerlink" title="1、Logging 模块使用过程"></a><strong>1、Logging 模块使用过程</strong></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1）第一次导入 logging 模块或使用 reload 函数重新导入 logging 模块，logging 模块中的代码将被执行，这个过程中将产生 logging 日志系统的默认配置。</span><br><span class="line"></span><br><span class="line">2）自定义配置(可选),logging标准模块支持三种配置方式: dictConfig，fileConfig，listen。其中，dictConfig 是通过一个字典进行配置 Logger，Handler，Filter，Formatter；fileConfig 则是通过一个文件进行配置；而 listen 则监听一个网络端口，通过接收网络数据来进行配置。当然，除了以上集体化配置外，也可以直接调用 Logger，Handler 等对象中的方法在代码中来显式配置。</span><br><span class="line"></span><br><span class="line">3）使用 logging 模块的全局作用域中的 getLogger 函数来得到一个 Logger 对象实例(其参数即是一个字符串，表示 Logger 对象实例的名字，即通过该名字来得到相应的 Logger 对象实例)。</span><br><span class="line"></span><br><span class="line">4）使用 Logger 对象中的 debug，info，error，warn，critical 等方法记录日志信息。</span><br></pre></td></tr></table></figure><h2 id="2、Logging-模块处理流程"><a href="#2、Logging-模块处理流程" class="headerlink" title="2、Logging 模块处理流程"></a><strong>2、Logging 模块处理流程</strong></h2><p>流程描述：</p><ol><li> 判断日志的等级是否大于 Logger 对象的等级，如果大于，则往下执行，否则，流程结束。</li><li> 产生日志：第一步，判断是否有异常，如果有，则添加异常信息。 第二步，处理日志记录方法(如 debug，info 等)中的占位符，即一般的字符串格式化处理。</li><li> 使用注册到 Logger 对象中的 Filters 进行过滤。如果有多个过滤器，则依次过滤；只要有一个过滤器返回假，则过滤结束，且该日志信息将丢弃，不再处理，而处理流程也至此结束。否则，处理流程往下执行。</li><li> 在当前 Logger 对象中查找 Handlers，如果找不到任何 Handler，则往上到该 Logger 对象的父 Logger 中查找；如果找到一个或多个 Handler，则依次用 Handler 来处理日志信息。但在每个 Handler 处理日志信息过程中，会首先判断日志信息的等级是否大于该 Handler 的等级，如果大于，则往下执行(由 Logger 对象进入 Handler 对象中)，否则，处理流程结束。</li><li> 执行 Handler 对象中的 filter 方法，该方法会依次执行注册到该 Handler 对象中的 Filter。如果有一个 Filter 判断该日志信息为假，则此后的所有 Filter 都不再执行，而直接将该日志信息丢弃，处理流程结束。</li><li> 使用 Formatter 类格式化最终的输出结果。 注：Formatter 同上述第 2 步的字符串格式化不同，它会添加额外的信息，比如日志产生的时间，产生日志的源代码所在的源文件的路径等等。</li><li> 真正地输出日志信息(到网络，文件，终端，邮件等)。至于输出到哪个目的地，由 Handler 的种类来决定。</li></ol><h2 id="三、配置日志"><a href="#三、配置日志" class="headerlink" title="三、配置日志"></a><strong>三、配置日志</strong></h2><p>程序员可以通过三种方式配置日志记录：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1、使用配置方法的 Python 代码显式创建记录器，处理程序和格式化程序。</span><br><span class="line"></span><br><span class="line">2、创建日志记录配置文件并使用该 fileConfig() 功能读取它。</span><br><span class="line"></span><br><span class="line">3、创建配置信息字典并将其传递给 dictConfig()函数。</span><br></pre></td></tr></table></figure><p>下面使用 Python 代码配置一个非常简单的记录器，一个控制台处理程序和一个简单的格式化程序：</p><p><strong>logging.conf 配置文件</strong>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[loggers]</span><br><span class="line">keys=root,simpleExample</span><br><span class="line"></span><br><span class="line">[handlers]</span><br><span class="line">keys=consoleHandler</span><br><span class="line"></span><br><span class="line">[formatters]</span><br><span class="line">keys=simpleFormatter</span><br><span class="line"></span><br><span class="line">[logger_root]</span><br><span class="line">level=DEBUG</span><br><span class="line">handlers=consoleHandler</span><br><span class="line"></span><br><span class="line">[logger_simpleExample]</span><br><span class="line">level=DEBUG</span><br><span class="line">handlers=consoleHandler</span><br><span class="line">qualname=simpleExample</span><br><span class="line">propagate=0</span><br><span class="line"></span><br><span class="line">[handler_consoleHandler]</span><br><span class="line">class=StreamHandler</span><br><span class="line">level=DEBUG</span><br><span class="line">formatter=simpleFormatter</span><br><span class="line">args=(sys.stdout,)</span><br><span class="line"></span><br><span class="line">[formatter_simpleFormatter]</span><br><span class="line">format=%(asctime)s - %(name)s - %(levelname)s - %(message)s</span><br><span class="line">datefmt=</span><br></pre></td></tr></table></figure><p><strong>config_logging.py 配置器</strong>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import logging</span><br><span class="line"># create logger</span><br><span class="line">logger = logging.getLogger(&#x27;simple_example&#x27;)</span><br><span class="line">logger.setLevel(logging.DEBUG)</span><br><span class="line"></span><br><span class="line"># create console handler and set level to debug</span><br><span class="line">ch = logging.StreamHandler()</span><br><span class="line">ch.setLevel(logging.DEBUG)</span><br><span class="line"></span><br><span class="line"># create formatter</span><br><span class="line">formatter = logging.Formatter(&#x27;%(asctime)s - %(name)s - %(levelname)s - %(message)s&#x27;)</span><br><span class="line"></span><br><span class="line"># add formatter to ch</span><br><span class="line">ch.setFormatter(formatter)</span><br><span class="line"></span><br><span class="line"># add ch to logger</span><br><span class="line">logger.addHandler(ch)</span><br><span class="line"></span><br><span class="line"># &#x27;application&#x27; code</span><br><span class="line">logger.debug(&#x27;debug message&#x27;)</span><br><span class="line">logger.info(&#x27;info message&#x27;)</span><br><span class="line">logger.warning(&#x27;warn message&#x27;)</span><br><span class="line">logger.error(&#x27;error message&#x27;)</span><br><span class="line">logger.critical(&#x27;critical message&#x27;)</span><br></pre></td></tr></table></figure><p><strong>recorder 记录器</strong>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import logging</span><br><span class="line">import logging.config</span><br><span class="line"></span><br><span class="line">logging.config.fileConfig(&#x27;logging.conf&#x27;)</span><br><span class="line"></span><br><span class="line"># create logger</span><br><span class="line">logger = logging.getLogger(&#x27;simpleExample&#x27;)</span><br><span class="line"></span><br><span class="line"># &#x27;application&#x27; code</span><br><span class="line">logger.debug(&#x27;debug message&#x27;)</span><br><span class="line">logger.info(&#x27;info message&#x27;)</span><br><span class="line">logger.warning(&#x27;warn message&#x27;)</span><br><span class="line">logger.error(&#x27;error message&#x27;)</span><br><span class="line">logger.critical(&#x27;critical message&#x27;)</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2019-10-16 19:45:34,440 - simple_example - DEBUG - debug message</span><br><span class="line">2019-10-16 19:45:34,440 - simple_example - INFO - info message</span><br><span class="line">2019-10-16 19:45:34,440 - simple_example - WARNING - warn message</span><br><span class="line">2019-10-16 19:45:34,440 - simple_example - ERROR - error message</span><br><span class="line">2019-10-16 19:45:34,441 - simple_example - CRITICAL - critical message</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><p>本章节给大家介绍了 Python 标准库中 Logging 模块的详细介绍与使用，对 Python 工程师使用该模块提供更好的支撑</p><p>参考： <a href="https://link.zhihu.com/?target=https://docs.python.org/3.6/library/logging.html?highlight=logging%23integration-with-the-warnings-module">https://docs.python.org/3.6/library/logging.html?highlight=logging#integration-with-the-warnings-module</a> <a href="https://link.zhihu.com/?target=https://www.jianshu.com/p/feb86c06c4f4">https://www.jianshu.com/p/feb86c06c4f4</a><br><a href="https://link.zhihu.com/?target=https://cloud.tencent.com/developer/article/1354396">https://cloud.tencent.com/devel</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;控制台颜色&quot;&gt;&lt;a href=&quot;#控制台颜色&quot; class=&quot;headerlink&quot; title=&quot;控制台颜色&quot;&gt;&lt;/a&gt;控制台颜色&lt;/h1&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter</summary>
      
    
    
    
    
    <category term="python" scheme="https://jpccc.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python语法</title>
    <link href="https://jpccc.github.io/2022/04/07/python%E8%AF%AD%E6%B3%95/"/>
    <id>https://jpccc.github.io/2022/04/07/python%E8%AF%AD%E6%B3%95/</id>
    <published>2022-04-07T08:10:44.000Z</published>
    <updated>2022-05-09T02:15:48.227Z</updated>
    
    <content type="html"><![CDATA[<h2 id="矩阵的操作"><a href="#矩阵的操作" class="headerlink" title="矩阵的操作"></a>矩阵的操作</h2><h3 id="view-和reshape"><a href="#view-和reshape" class="headerlink" title="view()和reshape"></a>view()和reshape</h3><p>view操作后的变量还是连续的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]],</span><br><span class="line">                  [[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],[<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]]</span><br><span class="line">                 ])</span><br><span class="line">b = a.view(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">b = tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>],</span><br><span class="line">        [<span class="number">5</span>,  <span class="number">6</span> ,<span class="number">7</span>,<span class="number">8</span>],</span><br><span class="line">        [<span class="number">9</span>,<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br></pre></td></tr></table></figure><p>张量a的size是2x2x3，使用view函数后，先将这12个元素排成一列，然后将其依次填充至新的4x3的张量中：</p><p><img src="E:\笔记\markdown\reference\picture\v2-acfb79d45817e3080ddcac7666d61f09_b.jpg" alt="img"></p><p>为了更细致，我们需要描述一下它们的具体操作流程（这并不是源码的流程，只是为了便于理解、记忆），因为二维比较直观，如果维度比较高的话，可能还是不够直观，心里老是有疑虑，还是以上面例子为例，张量a的每一个元素都有一个index，例如1的index是(0,0,0)，7的index是(1,0,0)，11的index是(1,1,1)……在拉成列向量排列时，排列规则是这样的，以(0,0,0)开始，维度从最后一维循环到第一维，在每一维内以升序将所有元素排成一列，即：</p><p><img src="E:\笔记\markdown\reference\picture\v2-0c59291b64850293305ab879087ea32d_b.jpg" alt="img"></p><p>然后依据新的size对每个元素给予新的index，仍然是维度从最后一维循环到第一维，在每一维内升序，即:</p><p><img src="E:\笔记\markdown\reference\picture\v2-ab9112d1c2f5bb21d1b3d164bffa5e95_b.jpg" alt="img"></p><blockquote><p>  torch的view()与reshape()方法都可以用来重塑tensor的shape，区别就是使用的条件不一样。view()方法只适用于满足连续性条件的tensor，并且该操作不会开辟新的内存空间，只是产生了对原存储空间的一个新别称和引用，返回值是视图。而reshape()方法的返回值既可以是视图，也可以是副本，当满足连续性条件时返回view，否则返回副本[ 此时等价于先调用contiguous()方法在使用view() ]。因此当不确能否使用view时，可以使用reshape。如果只是想简单地重塑一个tensor的shape，那么就是用reshape，但是如果需要考虑内存的开销而且要确保重塑后的tensor与之前的tensor共享存储空间，那就使用view()。</p></blockquote><h3 id="permute-和transpose"><a href="#permute-和transpose" class="headerlink" title="permute()和transpose()"></a>permute()和transpose()</h3><p>transpose()也是转置操作，与permute不同的是，transpose()每次只能进行二维转置，而permute()每次可转置多个维度().</p><p>接下来，说一下permute()，函数的参数为新的维度顺序，例如想交换第一维与第三维的index，则<a href="https://www.zhihu.com/search?q=tensor.permute&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:%2288311093%22%7D">tensor.permute</a>(2,1,0)，同样举一个简单第二维与第三维的例子，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]],[[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],[<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]]])</span><br><span class="line">b = a.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">b = tensor([[[ <span class="number">1</span>,  <span class="number">4</span>],</span><br><span class="line">         [ <span class="number">2</span>,  <span class="number">5</span>],</span><br><span class="line">         [ <span class="number">3</span>,  <span class="number">6</span>]],</span><br><span class="line">        [[ <span class="number">7</span>, <span class="number">10</span>],</span><br><span class="line">         [ <span class="number">8</span>, <span class="number">11</span>],</span><br><span class="line">         [ <span class="number">9</span>, <span class="number">12</span>]]])</span><br></pre></td></tr></table></figure><p><img src="E:\笔记\markdown\reference\picture\v2-6bdd87e65b64534339d3efad9ae4eae5_b.jpg" alt="img"></p><p>当维度比较大，交换维度的结果并不直观，我们还是说下permute到底做了什么，示意图如下。</p><p>需要说明一下，这里面的流程其实就是更改每个元素的index而已，上述例子中只是交换坐标，</p><p><img src="E:\笔记\markdown\reference\picture\v2-ec4167d8c19713e232ab3f5ba35ed10d_b.jpg" alt="img"></p><p>然后按照新的index更改下各元素的位置并展示出即可。</p><h3 id="squeeze-和unsqueeze-函数功能"><a href="#squeeze-和unsqueeze-函数功能" class="headerlink" title="squeeze()和unsqueeze()函数功能"></a>squeeze()和unsqueeze()函数功能</h3><h4 id="1-squeeze-dim-："><a href="#1-squeeze-dim-：" class="headerlink" title="1.squeeze(dim)："></a>1.squeeze(dim)：</h4><p>给张量tensor降维，但不是啥张量都可以用这两个函数来降维。dim指定需要降维的维度，这个维度的值必须为1才能被降维。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a=torch.rand(<span class="number">2</span>,<span class="number">1</span>,<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(a.shape)</span><br><span class="line"><span class="built_in">print</span>(a.squeeze(<span class="number">1</span>).shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#torch.Size([2, 1, 4, 1])</span></span><br><span class="line"><span class="comment">#torch.Size([2, 4, 1])</span></span><br></pre></td></tr></table></figure><p>2.unsqueeze(dim)</p><p>与squeeze(dim)相反，在dim维度上添加一个维度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a=torch.rand(<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(a.shape)</span><br><span class="line"><span class="built_in">print</span>(a.unsqueeze(<span class="number">1</span>).shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#torch.Size([2, 4])</span></span><br><span class="line"><span class="comment">#torch.Size([2, 1, 4])</span></span><br></pre></td></tr></table></figure><h2 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h2><p>在 Python 中，* 和 ** 具有语法多义性，具体来说是有四类用法。</p><span id="more"></span><ol><li>算数运算</li></ol><p>​    *代表乘法</p><p>​    **代表乘方</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">2</span> * <span class="number">5</span></span><br><span class="line"><span class="number">10</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">2</span> ** <span class="number">5</span></span><br><span class="line"><span class="number">32</span></span><br></pre></td></tr></table></figure><ol start="2"><li>函数形参</li></ol><p>*args 和 **kwargs 主要用于函数定义。</p><p>你可以将不定数量的参数传递给一个函数。不定的意思是：预先并不知道, 函数使用者会传递多少个参数给你, 所以在这个场景下使用这两个关键字。其实并不是必须写成 *args 和 **kwargs。  *(星号) 才是必须的. 你也可以写成 *ar  和 **k 。而写成 *args 和**kwargs 只是一个通俗的命名约定。</p><ul><li>  python函数传递参数的方式有两种：</li></ul><p>​                位置参数（positional argument）</p><p>​                关键词参数（keyword argument）</p><ul><li>  *args 与 **kwargs 的区别，两者都是 python 中的可变参数：</li></ul><p>​                *args 表示任何多个无名参数，它本质是一个 tuple<br>​                        ** kwargs 表示关键字参数，它本质上是一个 dict<br>如果同时使用 *args 和 **kwargs 时，必须 *args 参数列要在 **kwargs 之前。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">fun</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line"><span class="meta">... </span>    <span class="built_in">print</span>(<span class="string">&#x27;args=&#x27;</span>, args)</span><br><span class="line"><span class="meta">... </span>    <span class="built_in">print</span>(<span class="string">&#x27;kwargs=&#x27;</span>, kwargs)</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fun(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, A=<span class="string">&#x27;a&#x27;</span>, B=<span class="string">&#x27;b&#x27;</span>, C=<span class="string">&#x27;c&#x27;</span>, D=<span class="string">&#x27;d&#x27;</span>)</span><br><span class="line">args= (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">kwargs= &#123;<span class="string">&#x27;A&#x27;</span>: <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;B&#x27;</span>: <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;C&#x27;</span>: <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;D&#x27;</span>: <span class="string">&#x27;d&#x27;</span>&#125;</span><br></pre></td></tr></table></figure><p>使用*args：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">fun</span>(<span class="params">name, *args</span>):</span></span><br><span class="line"><span class="meta">... </span>    <span class="built_in">print</span>(<span class="string">&#x27;你好:&#x27;</span>, name)</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">for</span> i <span class="keyword">in</span> args:</span><br><span class="line"><span class="meta">... </span>        <span class="built_in">print</span>(<span class="string">&quot;你的宠物有:&quot;</span>, i)</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fun(<span class="string">&quot;Geek&quot;</span>, <span class="string">&quot;dog&quot;</span>, <span class="string">&quot;cat&quot;</span>)</span><br><span class="line">你好: Geek</span><br><span class="line">你的宠物有: dog</span><br><span class="line">你的宠物有: cat</span><br></pre></td></tr></table></figure><p>使用 **kwargs:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">fun</span>(<span class="params">**kwargs</span>):</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">for</span> key, value <span class="keyword">in</span> kwargs.items():</span><br><span class="line"><span class="meta">... </span>        <span class="built_in">print</span>(<span class="string">&quot;&#123;0&#125; 喜欢 &#123;1&#125;&quot;</span>.<span class="built_in">format</span>(key, value))</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fun(Geek=<span class="string">&quot;cat&quot;</span>, cat=<span class="string">&quot;box&quot;</span>)</span><br><span class="line">Geek 喜欢 cat</span><br><span class="line">cat 喜欢 box</span><br></pre></td></tr></table></figure><ol start="3"><li>函数实参</li></ol><p>如果函数的形参是定长参数，也可以使用 *args 和 **kwargs 调用函数，类似对元组和字典进行解引用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">fun</span>(<span class="params">**kwargs</span>):</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">for</span> key, value <span class="keyword">in</span> kwargs.items():</span><br><span class="line"><span class="meta">... </span>        <span class="built_in">print</span>(<span class="string">&quot;&#123;0&#125; 喜欢 &#123;1&#125;&quot;</span>.<span class="built_in">format</span>(key, value))</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fun(Geek=<span class="string">&quot;cat&quot;</span>, cat=<span class="string">&quot;box&quot;</span>)</span><br><span class="line">Geek 喜欢 cat</span><br><span class="line">cat 喜欢 box</span><br></pre></td></tr></table></figure><ol start="4"><li>序列解包</li></ol><p>序列解包 往期博客有写过，这里只列出一个例子，序列解包没有 **。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a, b, *c = <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a  </span><br><span class="line"><span class="number">0</span>  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b  </span><br><span class="line"><span class="number">1</span>  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c  </span><br><span class="line">[<span class="number">2</span>, <span class="number">3</span>]</span><br></pre></td></tr></table></figure><h2 id="TSNE可视化"><a href="#TSNE可视化" class="headerlink" title="TSNE可视化"></a>TSNE可视化</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.manifold import TSNE</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">f = &quot;predict_label.csv&quot;</span><br><span class="line">t = &quot;true_label.csv&quot;</span><br><span class="line"></span><br><span class="line">with open(f,encoding = &#x27;utf-8&#x27;) as f:</span><br><span class="line">    feature = np.loadtxt(f,delimiter = &quot;,&quot;)</span><br><span class="line">with open(t,encoding = &#x27;utf-8&#x27;) as f:</span><br><span class="line">    label = np.loadtxt(t,delimiter = &quot;,&quot;)</span><br><span class="line"></span><br><span class="line">tsne = TSNE(perplexity=30, n_components=2, init=&#x27;pca&#x27;, n_iter=1000) # TSNE降维，降到2</span><br><span class="line">Xpr = tsne.fit_transform(feature)</span><br><span class="line"></span><br><span class="line">x_min, x_max = Xpr.min(0), Xpr.max(0)</span><br><span class="line">Xpr = (Xpr - x_min) / (x_max - x_min)  # 归一化</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">for i in range(10000):</span><br><span class="line">    plt.text(Xpr[i, 0], Xpr[i, 1], str(label[i]),color=plt.cm.Set1(label[i]),fontdict=&#123;&#x27;weight&#x27;: &#x27;bold&#x27;, &#x27;size&#x27;: 9&#125;)</span><br><span class="line">plt.xticks([])</span><br><span class="line">plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="copy与deepcopy-拷贝与深拷贝"><a href="#copy与deepcopy-拷贝与深拷贝" class="headerlink" title="copy与deepcopy (拷贝与深拷贝)"></a>copy与deepcopy (拷贝与深拷贝)</h2><p>python 中的copy与deepcopy是内存数据的操作，但是两个函数有一定的区别。</p><h3 id="1-copy"><a href="#1-copy" class="headerlink" title="1.copy"></a>1.copy</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="built_in">list</span> = [<span class="number">1</span>, [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">list1 = copy.copy(<span class="built_in">list</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> <span class="built_in">id</span>(<span class="built_in">list</span>)</span><br><span class="line"><span class="built_in">print</span> <span class="built_in">id</span>(list1)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> <span class="built_in">id</span>(<span class="built_in">list</span>[<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span> <span class="built_in">id</span>(list1[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">list</span>[<span class="number">2</span>] = <span class="number">100</span></span><br><span class="line"><span class="built_in">list</span>[<span class="number">1</span>][<span class="number">0</span>] = <span class="number">44</span></span><br><span class="line"><span class="built_in">print</span> <span class="built_in">list</span></span><br><span class="line"><span class="built_in">print</span> list1</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line"><span class="number">46925320</span></span><br><span class="line"><span class="number">46967368</span></span><br><span class="line"><span class="number">46912776</span></span><br><span class="line"><span class="number">46912776</span></span><br><span class="line">[<span class="number">1</span>, [<span class="number">44</span>, <span class="number">5</span>, <span class="number">6</span>], <span class="number">100</span>, <span class="number">3</span>]</span><br><span class="line">[<span class="number">1</span>, [<span class="number">44</span>, <span class="number">5</span>, <span class="number">6</span>], <span class="number">2</span>, <span class="number">3</span>]</span><br></pre></td></tr></table></figure><p>经过copy操作的得两个list，list1拥有两个同的地址（46925320和46967368），修改list时不会影响list1的值，但是 list中间的子列表[4,5,6]在list和list1中有相同的地址46912776，所以在修改list中的子列表会影响到list1中的子列表。</p><h3 id="2-deepcopy"><a href="#2-deepcopy" class="headerlink" title="2.deepcopy"></a>2.deepcopy</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import copy</span><br><span class="line">list = [1, [4, 5, 6], 2, 3]</span><br><span class="line">list2 = copy.deepcopy(list)</span><br><span class="line"></span><br><span class="line">print id(list)</span><br><span class="line">print id(list[1])</span><br><span class="line">print id(list2)</span><br><span class="line">print id(list2[1])</span><br><span class="line"></span><br><span class="line">list[2] = 100</span><br><span class="line">list[1][0] = 44</span><br><span class="line">print list</span><br><span class="line">print list2</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">59508232</span><br><span class="line">59495688</span><br><span class="line">59509384</span><br><span class="line">59508168</span><br><span class="line">[1, [44, 5, 6], 100, 3]</span><br><span class="line">[1, [4, 5, 6], 2, 3]</span><br></pre></td></tr></table></figure><p>经过deepcopy的list与list2用有不用的地址59508232，59509384，其中的子列表页拥有不同的地址， 所以不论怎样修改list都不用影响到list2。</p><p>结论：</p><p>经过copy操作的两个数据对象拥有不同的得地址空间 ，但是这个数据对象如果是内嵌了其他的复杂数据对象，这个内嵌的数据对象在两个数据对象中拥有相同的地址空间，修改其中的值会互相印象。经过deepcopy的操作的不管是内层还是外层数据对象都拥有不同的地址空间，修改其中的值不会对两个对象都造成影响。</p><p>附：单变量用‘=’赋值不存在这个问题，因为用等号修改相当于重新赋值新的变量或常量，但并未涉及修改对应变量的值（即未对对应内存元素进行操作）。</p><h2 id="Python中os-sep的用法"><a href="#Python中os-sep的用法" class="headerlink" title="Python中os.sep的用法"></a>Python中os.sep的用法</h2><p>python是跨平台的。在Windows上，文件的路径分隔符是’&#39;，在Linux上是’/‘。</p><p>为了让代码在不同的平台上都能运行，那么路径应该写’&#39;还是’/‘呢？</p><p>使用os.sep的话，就不用考虑这个了，os.sep根据你所处的平台，自动采用相应的分隔符号。</p><p>举例</p><p>Linux系统某个路径，/usr/share/python,那么上面的os.sep就是‘/’</p><p>windows系统某个路径，C：\Users\Public\Desktop,那么上面的os.sep就是‘\’.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_dir = os.sep.join([&#x27;hello&#x27;, &#x27;world&#x27;])</span><br><span class="line">hello/world或者hello\world</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;矩阵的操作&quot;&gt;&lt;a href=&quot;#矩阵的操作&quot; class=&quot;headerlink&quot; title=&quot;矩阵的操作&quot;&gt;&lt;/a&gt;矩阵的操作&lt;/h2&gt;&lt;h3 id=&quot;view-和reshape&quot;&gt;&lt;a href=&quot;#view-和reshape&quot; class=&quot;headerlink&quot; title=&quot;view()和reshape&quot;&gt;&lt;/a&gt;view()和reshape&lt;/h3&gt;&lt;p&gt;view操作后的变量还是连续的。&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;a = torch.tensor([[[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;],[&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;]],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                  [[&lt;span class=&quot;number&quot;&gt;7&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;9&lt;/span&gt;],[&lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;11&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;12&lt;/span&gt;]]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                 ])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;b = a.view(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;b = tensor([[ &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,  &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,  &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;,  &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        [&lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;,  &lt;span class=&quot;number&quot;&gt;6&lt;/span&gt; ,&lt;span class=&quot;number&quot;&gt;7&lt;/span&gt;,	&lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        [&lt;span class=&quot;number&quot;&gt;9&lt;/span&gt;,	&lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;11&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;12&lt;/span&gt;]])&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;张量a的size是2x2x3，使用view函数后，先将这12个元素排成一列，然后将其依次填充至新的4x3的张量中：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;E:\笔记\markdown\reference\picture\v2-acfb79d45817e3080ddcac7666d61f09_b.jpg&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
&lt;p&gt;为了更细致，我们需要描述一下它们的具体操作流程（这并不是源码的流程，只是为了便于理解、记忆），因为二维比较直观，如果维度比较高的话，可能还是不够直观，心里老是有疑虑，还是以上面例子为例，张量a的每一个元素都有一个index，例如1的index是(0,0,0)，7的index是(1,0,0)，11的index是(1,1,1)……在拉成列向量排列时，排列规则是这样的，以(0,0,0)开始，维度从最后一维循环到第一维，在每一维内以升序将所有元素排成一列，即：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;E:\笔记\markdown\reference\picture\v2-0c59291b64850293305ab879087ea32d_b.jpg&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
&lt;p&gt;然后依据新的size对每个元素给予新的index，仍然是维度从最后一维循环到第一维，在每一维内升序，即:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;E:\笔记\markdown\reference\picture\v2-ab9112d1c2f5bb21d1b3d164bffa5e95_b.jpg&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;  torch的view()与reshape()方法都可以用来重塑tensor的shape，区别就是使用的条件不一样。view()方法只适用于满足连续性条件的tensor，并且该操作不会开辟新的内存空间，只是产生了对原存储空间的一个新别称和引用，返回值是视图。而reshape()方法的返回值既可以是视图，也可以是副本，当满足连续性条件时返回view，否则返回副本[ 此时等价于先调用contiguous()方法在使用view() ]。因此当不确能否使用view时，可以使用reshape。如果只是想简单地重塑一个tensor的shape，那么就是用reshape，但是如果需要考虑内存的开销而且要确保重塑后的tensor与之前的tensor共享存储空间，那就使用view()。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;permute-和transpose&quot;&gt;&lt;a href=&quot;#permute-和transpose&quot; class=&quot;headerlink&quot; title=&quot;permute()和transpose()&quot;&gt;&lt;/a&gt;permute()和transpose()&lt;/h3&gt;&lt;p&gt;transpose()也是转置操作，与permute不同的是，transpose()每次只能进行二维转置，而permute()每次可转置多个维度().&lt;/p&gt;
&lt;p&gt;接下来，说一下permute()，函数的参数为新的维度顺序，例如想交换第一维与第三维的index，则&lt;a href=&quot;https://www.zhihu.com/search?q=tensor.permute&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:%2288311093%22%7D&quot;&gt;tensor.permute&lt;/a&gt;(2,1,0)，同样举一个简单第二维与第三维的例子，&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;a = torch.tensor([[[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;],[&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;]],[[&lt;span class=&quot;number&quot;&gt;7&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;9&lt;/span&gt;],[&lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;11&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;12&lt;/span&gt;]]])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;b = a.permute(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;b = tensor([[[ &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,  &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;         [ &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,  &lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;         [ &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;,  &lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;]],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        [[ &lt;span class=&quot;number&quot;&gt;7&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;         [ &lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;11&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;         [ &lt;span class=&quot;number&quot;&gt;9&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;12&lt;/span&gt;]]])&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;E:\笔记\markdown\reference\picture\v2-6bdd87e65b64534339d3efad9ae4eae5_b.jpg&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
&lt;p&gt;当维度比较大，交换维度的结果并不直观，我们还是说下permute到底做了什么，示意图如下。&lt;/p&gt;
&lt;p&gt;需要说明一下，这里面的流程其实就是更改每个元素的index而已，上述例子中只是交换坐标，&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;E:\笔记\markdown\reference\picture\v2-ec4167d8c19713e232ab3f5ba35ed10d_b.jpg&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
&lt;p&gt;然后按照新的index更改下各元素的位置并展示出即可。&lt;/p&gt;
&lt;h3 id=&quot;squeeze-和unsqueeze-函数功能&quot;&gt;&lt;a href=&quot;#squeeze-和unsqueeze-函数功能&quot; class=&quot;headerlink&quot; title=&quot;squeeze()和unsqueeze()函数功能&quot;&gt;&lt;/a&gt;squeeze()和unsqueeze()函数功能&lt;/h3&gt;&lt;h4 id=&quot;1-squeeze-dim-：&quot;&gt;&lt;a href=&quot;#1-squeeze-dim-：&quot; class=&quot;headerlink&quot; title=&quot;1.squeeze(dim)：&quot;&gt;&lt;/a&gt;1.squeeze(dim)：&lt;/h4&gt;&lt;p&gt;给张量tensor降维，但不是啥张量都可以用这两个函数来降维。dim指定需要降维的维度，这个维度的值必须为1才能被降维。&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;a=torch.rand(&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;print&lt;/span&gt;(a.shape)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;print&lt;/span&gt;(a.squeeze(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;).shape)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#torch.Size([2, 1, 4, 1])&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#torch.Size([2, 4, 1])&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;2.unsqueeze(dim)&lt;/p&gt;
&lt;p&gt;与squeeze(dim)相反，在dim维度上添加一个维度。&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;a=torch.rand(&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;print&lt;/span&gt;(a.shape)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;print&lt;/span&gt;(a.unsqueeze(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;).shape)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#torch.Size([2, 4])&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#torch.Size([2, 1, 4])&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;





&lt;h2 id=&quot;运算符&quot;&gt;&lt;a href=&quot;#运算符&quot; class=&quot;headerlink&quot; title=&quot;运算符&quot;&gt;&lt;/a&gt;运算符&lt;/h2&gt;&lt;p&gt;在 Python 中，* 和 ** 具有语法多义性，具体来说是有四类用法。&lt;/p&gt;</summary>
    
    
    
    
    <category term="python" scheme="https://jpccc.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>聚类</title>
    <link href="https://jpccc.github.io/2022/04/07/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    <id>https://jpccc.github.io/2022/04/07/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/</id>
    <published>2022-04-07T08:10:44.000Z</published>
    <updated>2022-04-15T13:05:58.134Z</updated>
    
    <content type="html"><![CDATA[<p>可以使用模块 <a href="http://scikit-learn.org.cn/lists/3.html#sklearn.cluster%EF%BC%9A%E8%81%9A%E7%B1%BB"><code>sklearn.cluster</code></a> 对未标记的数据进行 <a href="https://en.wikipedia.org/wiki/Cluster_analysis">聚类(Clustering)</a> 。</p><p>每个聚类算法都有两个变体:一个是类，它实现了 <code>fit</code> 方法来学习训练数据上的簇，另一个是函数，给定训练数据，返回对应于不同簇的整数标签数组。对于类，可以在 <code>labels_</code> 属性中找到训练数据上的标签。</p><blockquote><p>  <strong>输入数据</strong></p><p>  特别需要注意的一点是，本模块实现的算法可以采用不同类型的矩阵作为输入。所有算法的调用接口都接受 shape<code>[n_samples, n_features]</code> 的标准数据矩阵。 这些矩阵可以通过使用 <a href="http://scikit-learn.org.cn/lists/3.html#sklearn.feature_extraction%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96"><code>sklearn.feature_extraction</code></a> 模块中的类获得。对于 <a href="https://scikit-learn.org.cn/view/370.html"><code>AffinityPropagation</code></a>, <a href="https://scikit-learn.org.cn/view/391.html"><code>SpectralClustering</code></a> 和 <a href="https://scikit-learn.org.cn/view/379.html"><code>DBSCAN</code></a> 也可以输入 shape <code>[n_samples, n_samples]</code> 的相似矩阵，可以通过 <a href="http://scikit-learn.org.cn/lists/3.html#sklearn.metrics%EF%BC%9A%E6%8C%87%E6%A0%87"><code>sklearn.metrics.pairwise</code></a> 模块中的函数获得。</p></blockquote><h3 id="聚类方法概述"><a href="#聚类方法概述" class="headerlink" title="聚类方法概述"></a>聚类方法概述</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;可以使用模块 &lt;a href=&quot;http://scikit-learn.org.cn/lists/3.html#sklearn.cluster%EF%BC%9A%E8%81%9A%E7%B1%BB&quot;&gt;&lt;code&gt;sklearn.cluster&lt;/code&gt;&lt;/a&gt; 对未标记</summary>
      
    
    
    
    
    <category term="算法" scheme="https://jpccc.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>pytorch使用汇总</title>
    <link href="https://jpccc.github.io/2022/04/07/pytorch%E4%BD%BF%E7%94%A8%E6%B1%87%E6%80%BB/"/>
    <id>https://jpccc.github.io/2022/04/07/pytorch%E4%BD%BF%E7%94%A8%E6%B1%87%E6%80%BB/</id>
    <published>2022-04-07T04:44:13.000Z</published>
    <updated>2022-05-09T13:45:26.263Z</updated>
    
    <content type="html"><![CDATA[<h2 id="日志输出"><a href="#日志输出" class="headerlink" title="日志输出"></a>日志输出</h2><p>利用logging模块在控制台实时打印并及时记录运行日志。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from config import  *</span><br><span class="line">import logging  # 引入logging模块</span><br><span class="line">import os.path</span><br><span class="line">class Logger:</span><br><span class="line">    def __init__(self,mode=&#x27;w&#x27;):</span><br><span class="line">        # 第一步，创建一个logger</span><br><span class="line">        self.logger = logging.getLogger()</span><br><span class="line">        self.logger.setLevel(logging.INFO)  # Log等级总开关</span><br><span class="line">        # 第二步，创建一个handler，用于写入日志文件</span><br><span class="line">        rq = time.strftime(&#x27;%Y%m%d%H%M&#x27;, time.localtime(time.time()))</span><br><span class="line">        log_path = os.getcwd() + &#x27;/Logs/&#x27;</span><br><span class="line">        log_name = log_path + rq + &#x27;.log&#x27;</span><br><span class="line">        logfile = log_name</span><br><span class="line">        fh = logging.FileHandler(logfile, mode=mode)</span><br><span class="line">        fh.setLevel(logging.DEBUG)  # 输出到file的log等级的开关</span><br><span class="line">        # 第三步，定义handler的输出格式</span><br><span class="line">        formatter = logging.Formatter(&quot;%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s&quot;)</span><br><span class="line">        fh.setFormatter(formatter)</span><br><span class="line">        # 第四步，将logger添加到handler里面</span><br><span class="line">        self.logger.addHandler(fh)</span><br><span class="line">        ch = logging.StreamHandler()</span><br><span class="line">        ch.setLevel(logging.INFO)  # 输出到console的log等级的开关</span><br><span class="line">        ch.setFormatter(formatter)</span><br><span class="line">        self.logger.addHandler(ch)</span><br></pre></td></tr></table></figure><h2 id="模型的保存和读取"><a href="#模型的保存和读取" class="headerlink" title="模型的保存和读取"></a>模型的保存和读取</h2><p>​    </p><table><thead><tr><th>保存</th><th align="left">（1）torch.save(net.state_dict(),保存路径) 。（2）多卡训练时，要用 torch.save(net.module.state_dict(),’./model/best.pth’)</th></tr></thead><tbody><tr><td>读取</td><td align="left">（1）net.load_state_dict(torch.load(‘best.pth’))。（2）并行时map_location=device.type在读取模型的时候一定要加上。即：    model.load_state_dict(torch.load(‘model/self_train_bestv2.pth’,map_location=’cuda’))</td></tr><tr><td></td><td align="left">torch.save(model.state_dict(),’checkpoint_0.tar’,_use_new_zipfile_serialization=False) #解决版本不兼容</td></tr></tbody></table><span id="more"></span><h3 id="网络参数和参数学习率的修改"><a href="#网络参数和参数学习率的修改" class="headerlink" title="网络参数和参数学习率的修改"></a>网络参数和参数学习率的修改</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">if use_resnet:</span><br><span class="line">resnet = models.resnet50(pretrained=True)</span><br><span class="line">new_state_dict = resnet.state_dict()</span><br><span class="line">dd = net.state_dict()</span><br><span class="line">for k in new_state_dict.keys():</span><br><span class="line">    print(k)</span><br><span class="line">    if k in dd.keys() and not k.startswith(&#x27;fc&#x27;):</span><br><span class="line">        print(&#x27;yes&#x27;)</span><br><span class="line">        dd[k] = new_state_dict[k]</span><br><span class="line">net.load_state_dict(dd)</span><br><span class="line"></span><br><span class="line">different learning rate</span><br><span class="line">params=[]</span><br><span class="line">params_dict = dict(net.named_parameters())</span><br><span class="line">print(learning_rate*1,learning_rate)</span><br><span class="line">for key,value in params_dict.items():</span><br><span class="line">    print(key)</span><br><span class="line">    if key.startswith(&#x27;features&#x27;):</span><br><span class="line">        params += [&#123;&#x27;params&#x27;:[value],&#x27;lr&#x27;:learning_rate*1&#125;]</span><br><span class="line">    else:</span><br><span class="line">        params += [&#123;&#x27;params&#x27;:[value],&#x27;lr&#x27;:learning_rate&#125;]</span><br><span class="line">创建优化器的时候设置了，上面的操作就没必要了</span><br></pre></td></tr></table></figure><h2 id="数据的保存和读取"><a href="#数据的保存和读取" class="headerlink" title="数据的保存和读取"></a>数据的保存和读取</h2><p>DataLoader的pin_memory和data.cuda(non_blocking=True)搭配使用</p><p><a href="https://so.csdn.net/so/search?q=PyTorch&spm=1001.2101.3001.7020">PyTorch</a>的DataLoader有一个参数pin_memory，使用固定内存，并使用non_blocking=True来并行处理数据传输。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span> x = x.cuda(non_blocking=<span class="literal">True</span>)</span><br><span class="line"><span class="number">2.</span> 进行一些和x无关的操作</span><br><span class="line"><span class="number">3.</span> 执行和x有关的操作</span><br></pre></td></tr></table></figure><p>在<code>non_blocking=true</code>下，<code>1</code>不会阻塞<code>2</code>，<code>1</code>和<code>2</code>并行。</p><p>这样将数据从CPU移动到<a href="https://so.csdn.net/so/search?q=GPU&spm=1001.2101.3001.7020">GPU</a>的时候，它是异步的。在它传输的时候，CPU还可以干其他的事情（不依赖于数据的事情）</p><h2 id="GPU的并行计算"><a href="#GPU的并行计算" class="headerlink" title="GPU的并行计算"></a>GPU的并行计算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">   device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">   device_ids = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">   model = torch.nn.DataParallel(model, device_ids=device_ids)</span><br><span class="line">   model.to(device)</span><br><span class="line">   data.to(device)</span><br><span class="line">   output=model(data)</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">   parser.add_argument(<span class="string">&quot;-g&quot;</span>, <span class="string">&quot;--gpus&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&quot;&quot;</span>)<span class="comment">#gpu的数量</span></span><br><span class="line">   <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">       net = torch.nn.DataParallel(net,device_ids=<span class="built_in">range</span>(<span class="built_in">len</span>(args.gpus.split(<span class="string">&quot;,&quot;</span>))))</span><br><span class="line">       torch.backends.cudnn.benchmark = <span class="literal">True</span></span><br></pre></td></tr></table></figure><h2 id="Wandb的使用"><a href="#Wandb的使用" class="headerlink" title="Wandb的使用"></a>Wandb的使用</h2><pre><code>import wandbwandb.init(project=&#39;yolov2&#39;)config = wandb.configconfig.learning_rate = 0.01wandb.log(&#123;&quot;loss&quot;: loss&#125;)</code></pre><h2 id="torchsummary"><a href="#torchsummary" class="headerlink" title="torchsummary"></a>torchsummary</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 安装：pip install torchsummary</span><br><span class="line"></span><br><span class="line">from torchsummary import summary</span><br><span class="line">from torchvision.models import resnet18</span><br><span class="line">model = resnet18()</span><br><span class="line">summary(model, input_size=[(3, 256, 256)], batch_size=2, device=&quot;cpu&quot;)</span><br></pre></td></tr></table></figure><h2 id="Tensor-board的使用"><a href="#Tensor-board的使用" class="headerlink" title="Tensor board的使用"></a>Tensor board的使用</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line">writer = SummaryWriter()</span><br><span class="line">writer.add_scalar(&#x27;loss&#x27;, loss, global_step=n_iter)  #loss是纵坐标，global_step是横坐标。</span><br></pre></td></tr></table></figure><h2 id="seed设置"><a href="#seed设置" class="headerlink" title="seed设置"></a>seed设置</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(args.seed)</span><br><span class="line">torch.cuda.manual_seed_all(args.seed)</span><br><span class="line">np.random.seed(args.seed)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="import-类"><a href="#import-类" class="headerlink" title="import 类"></a>import 类</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#用法1</span><br><span class="line">    import 包名</span><br><span class="line">    在包内的__init__.py文件中导入需要导入的类</span><br><span class="line">    那么就可以在import所在的文件中使用Init文件中导入的类</span><br><span class="line">#用法2</span><br><span class="line">如果在包内的某个pu文件想导入其它包的文件，路径从包名开始写</span><br><span class="line">例如： from model.resnet import Resnet18</span><br><span class="line"></span><br><span class="line">#vscode项目的文件的访问</span><br><span class="line">所有文件都能访问到最外层目录(项目所在的主目录)</span><br></pre></td></tr></table></figure><h2 id="计算过程中的梯度问题"><a href="#计算过程中的梯度问题" class="headerlink" title="计算过程中的梯度问题"></a>计算过程中的梯度问题</h2><p>requires_grad=True 要求计算梯度<br>        requires_grad=False 不要求计算梯度<br>        with torch.no_grad()或者@torch.no_grad()中的数据不需要计算梯度，也不会进行反向传播。<br>        tensor.data 近似tensor.detach()，都是让tensor不进行导数的计算，但是，tensor的值不能修改，否则求梯度时会报错。</p><p>==不计算梯度能够加快运行速度。==</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### model.eval()和with torch.no_grad()的区别</span></span><br><span class="line"></span><br><span class="line">在PyTorch中进行validation时，会使用model.<span class="built_in">eval</span>()切换到测试模式，在该模式下，</span><br><span class="line"></span><br><span class="line">主要用于通知dropout层和batchnorm层在train和<span class="built_in">eval</span>模式间切换</span><br><span class="line"></span><br><span class="line">在train模式下，dropout网络层会按照设定的参数p设置保留激活单元的概率（保留概率=p); batchnorm层会继续计算数据的mean和var等参数并更新。</span><br><span class="line"></span><br><span class="line">在<span class="built_in">eval</span>模式下，dropout层会让所有的激活单元都通过，而batchnorm层会停止计算和更新mean和var，直接使用在训练阶段已经学出的mean和var值。</span><br><span class="line"></span><br><span class="line">该模式不会影响各层的gradient计算行为，即gradient计算和存储与training模式一样，只是不进行反传（backprobagation）</span><br><span class="line"></span><br><span class="line">而<span class="keyword">with</span> torch.no_grad()则主要是用于停止autograd模块的工作，以起到加速和节省显存的作用，具体行为就是停止gradient计算，从而节省了GPU算力和显存，但是并不会影响dropout和batchnorm层的行为。</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()                                <span class="comment"># 测试模式</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval</span>():</span></span><br><span class="line">    model.<span class="built_in">eval</span>()  </span><br><span class="line">...</span><br></pre></td></tr></table></figure><h2 id="python-下划线开头和结尾-的元素"><a href="#python-下划线开头和结尾-的元素" class="headerlink" title="python(下划线开头和结尾)的元素"></a>python(下划线开头和结尾)的元素</h2><pre><code>事实上，当我们向文件导入某个模块时，导入的是该模块中那些名称不以下划线（单下划线“_”或者双下划线“__”）开头的变量、函数和类。因此，如果我们不想模块文件中的某个成员被引入到其它文件中使用，可以在其名称前添加下划线。</code></pre><h2 id="open函数"><a href="#open函数" class="headerlink" title="open函数"></a>open函数</h2><p>读取文本文件时，不要使用rb模式打开文件，而应该仅使用r模式。</p><table><thead><tr><th align="left">读取方式</th><th>解读</th></tr></thead><tbody><tr><td align="left">r</td><td>r是推荐使用的打开文本文件的模式。因为使用此模式打开文本文件时，python默认为我们做了一些处理，比如：假设在windows下，将本来应该读入的换行符\r\n处理成\n,方便我们处理。（值得一提的是，当你将\n写入文件时，python也会默认将其替换成\r\n，如果你是win系统的话）<br/>补充：其实是启用了通用换行符支持（UNS），它默认开启。</td></tr><tr><td align="left">rb</td><td>使用rb：则python不会对文本文件预处理了，你读入的\r\n依然是\r\n.</td></tr></tbody></table><h2 id="输入的图像变量需要设置梯度吗"><a href="#输入的图像变量需要设置梯度吗" class="headerlink" title="输入的图像变量需要设置梯度吗"></a>输入的图像变量需要设置梯度吗</h2><pre><code>不需要，每个网络参数都有梯度grad，因为其需要根据此来更新梯度pytorch不会记录中间变量的梯度，如果需要的话，要另外设置但是resnet等模型中的卷积层等参数，其是有梯度的，每个参数都有grad</code></pre><h2 id="F和nn"><a href="#F和nn" class="headerlink" title="F和nn"></a>F和nn</h2><pre><code>F.relu()和nn.Function()的区别：一个是函数一个是类方法。</code></pre><h2 id="读取网络参数"><a href="#读取网络参数" class="headerlink" title="读取网络参数"></a>读取网络参数</h2><p>​    </p><h2 id="contiguous"><a href="#contiguous" class="headerlink" title="contiguous"></a>contiguous</h2><pre><code>如果想要变得连续使用contiguous方法，如果Tensor不是连续的，则会重新开辟一块内存空间保证数据是在内存中是连续的，如果Tensor是连续的，则contiguous无操作。</code></pre><p>注意梯度反向传播的时候，所有的变量的应该在gpu或者同时在cpu上。</p><h2 id="提高gpu利用率"><a href="#提高gpu利用率" class="headerlink" title="提高gpu利用率"></a>提高gpu利用率</h2><pre><code>pytorch跑Unet代码，gpu利用率在0%-20%闪现，主要问题是GPU一直在等cpu处理的数据传输过去。利用top查看cup的利用率也是从0省道100%且显然cup的线程并不多，能处理出的数据也不多。在一般的程序中，除了加载从dataloader中数据和model的运行需要gpu，其余更多的dataset、dataloader、loss的计算和日志的输出很多部分都需要cup的计算。所以，可以提升的方面包括 从class dataset的优化、dataloader的优化和其他部分代码的优化。当然代码的优化是一个长期的考验代码能力的问题。那么短期的提升在于对dataloader的优化：    1.batchsize调大 提高GPU内存占用率    2.num_works 调到适当值，一般情况下为8、16是比较合适的值。太小就会出现我上述讲道的一些问题。太大的话cpu线程增加会导致gpu的利用率降低。因为模型需要将数据平均分配到几个子线程去进行预处理，分发等数据操作，设高了反而影响效率。    3.pin_memory =True 省掉了将数据从CPU传入到缓存RAM里面，再给传输到GPU上；为True时是直接映射到GPU的相关内存块上，省掉了一点数据传输时间。</code></pre><h2 id="vscode的目录访问"><a href="#vscode的目录访问" class="headerlink" title="vscode的目录访问"></a>vscode的目录访问</h2><h2 id="pytorch的GPU设置理解"><a href="#pytorch的GPU设置理解" class="headerlink" title="pytorch的GPU设置理解"></a><a href="./reference/gpu.md">pytorch的GPU设置</a>理解</h2><pre><code>cudnn.deterministic = Truecudnn.benchmark = True</code></pre><h2 id="torch-tensor与torch-Tensor的区别"><a href="#torch-tensor与torch-Tensor的区别" class="headerlink" title="torch.tensor与torch.Tensor的区别"></a><strong>torch.tensor与torch.Tensor的区别</strong></h2><p>​    细心的读者可能注意到了，通过Tensor建立数组有torch.tensor([1,2])或torch.Tensor([1,2])两种方式。那么，这两种方式有什么区别呢？</p><p>​    （1）torch.tensor是从数据中推断数据类型，而torch.Tensor是torch.empty(会随机产生垃圾数组，详见实例)和torch.tensor之间的一种混合。但是，当传入数据时，torch.Tensor使用全局默认dtype(FloatTensor)；</p><h2 id="tensor类型和形状"><a href="#tensor类型和形状" class="headerlink" title="tensor类型和形状"></a>tensor类型和形状</h2><p>​    查看tensor类型最好用tensor.type()函数，而不要用type(tensor)！</p><p>tensor.shape和tensor.size（）都可以返回tensor的形状，前一个是属性，后一个是方法。</p><h2 id="1-基本配置"><a href="#1-基本配置" class="headerlink" title="1. 基本配置"></a>1. 基本配置</h2><h3 id="导入包和版本查询"><a href="#导入包和版本查询" class="headerlink" title="导入包和版本查询"></a>导入包和版本查询</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torchvision</span><br><span class="line">print(torch.__version__)</span><br><span class="line">print(torch.version.cuda)</span><br><span class="line">print(torch.backends.cudnn.version())</span><br><span class="line">print(torch.cuda.get_device_name(0))</span><br></pre></td></tr></table></figure><h3 id="可复现性"><a href="#可复现性" class="headerlink" title="可复现性"></a>可复现性</h3><p>在硬件设备（CPU、GPU）不同时，完全的可复现性无法保证，即使随机种子相同。但是，在同一个设备上，应该保证可复现性。具体做法是，在程序开始的时候固定torch的随机种子，同时也把numpy的随机种子固定。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(0)</span><br><span class="line">torch.manual_seed(0)</span><br><span class="line">torch.cuda.manual_seed_all(0)</span><br><span class="line"></span><br><span class="line">torch.backends.cudnn.deterministic = True</span><br><span class="line">torch.backends.cudnn.benchmark = False</span><br></pre></td></tr></table></figure><h3 id="显卡设置"><a href="#显卡设置" class="headerlink" title="显卡设置"></a>显卡设置</h3><p>如果只需要一张显卡</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Device configuration</span><br><span class="line">device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)</span><br></pre></td></tr></table></figure><p>如果需要指定多张显卡，比如0，1号显卡。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">os.environ[&#x27;CUDA_VISIBLE_DEVICES&#x27;] = &#x27;0,1&#x27;</span><br></pre></td></tr></table></figure><p>也可以在命令行运行代码时设置显卡：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0,1 python train.py</span><br></pre></td></tr></table></figure><p>清除显存</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.empty_cache()</span><br></pre></td></tr></table></figure><p>也可以使用在命令行重置GPU的指令</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi --gpu-reset -i [gpu_id]</span><br></pre></td></tr></table></figure><h2 id="2-张量-Tensor-处理"><a href="#2-张量-Tensor-处理" class="headerlink" title="2. 张量(Tensor)处理"></a>2. 张量(Tensor)处理</h2><h3 id="张量的数据类型"><a href="#张量的数据类型" class="headerlink" title="张量的数据类型"></a>张量的数据类型</h3><p>PyTorch有9种CPU张量类型和9种GPU张量类型。</p><p><img src="E:\笔记\markdown\reference\picture\640.jpeg" alt="图片"></p><h3 id="张量基本信息"><a href="#张量基本信息" class="headerlink" title="张量基本信息"></a>张量基本信息</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.randn(3,4,5)</span><br><span class="line">print(tensor.type())  # 数据类型</span><br><span class="line">print(tensor.size())  # 张量的shape，是个元组</span><br><span class="line">print(tensor.dim())   # 维度的数量</span><br></pre></td></tr></table></figure><h3 id="命名张量"><a href="#命名张量" class="headerlink" title="命名张量"></a>命名张量</h3><p>张量命名是一个非常有用的方法，这样可以方便地使用维度的名字来做索引或其他操作，大大提高了可读性、易用性，防止出错。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 在PyTorch 1.3之前，需要使用注释</span><br><span class="line"># Tensor[N, C, H, W]</span><br><span class="line">images = torch.randn(32, 3, 56, 56)</span><br><span class="line">images.sum(dim=1)</span><br><span class="line">images.select(dim=1, index=0)</span><br><span class="line"></span><br><span class="line"># PyTorch 1.3之后</span><br><span class="line">NCHW = [‘N’, ‘C’, ‘H’, ‘W’]</span><br><span class="line">images = torch.randn(32, 3, 56, 56, names=NCHW)</span><br><span class="line">images.sum(&#x27;C&#x27;)</span><br><span class="line">images.select(&#x27;C&#x27;, index=0)</span><br><span class="line"># 也可以这么设置</span><br><span class="line">tensor = torch.rand(3,4,1,2,names=(&#x27;C&#x27;, &#x27;N&#x27;, &#x27;H&#x27;, &#x27;W&#x27;))</span><br><span class="line"># 使用align_to可以对维度方便地排序</span><br><span class="line">tensor = tensor.align_to(&#x27;N&#x27;, &#x27;C&#x27;, &#x27;H&#x27;, &#x27;W&#x27;)</span><br></pre></td></tr></table></figure><h3 id="数据类型转换"><a href="#数据类型转换" class="headerlink" title="数据类型转换"></a>数据类型转换</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 设置默认类型，pytorch中的FloatTensor远远快于DoubleTensor</span><br><span class="line">torch.set_default_tensor_type(torch.FloatTensor)</span><br><span class="line"></span><br><span class="line"># 类型转换</span><br><span class="line">tensor = tensor.cuda()</span><br><span class="line">tensor = tensor.cpu()</span><br><span class="line">tensor = tensor.float()</span><br><span class="line">tensor = tensor.long()</span><br></pre></td></tr></table></figure><h3 id="torch-Tensor与np-ndarray转换"><a href="#torch-Tensor与np-ndarray转换" class="headerlink" title="torch.Tensor与np.ndarray转换"></a><strong>torch.Tensor与np.ndarray转换</strong></h3><p>除了CharTensor，其他所有CPU上的张量都支持转换为numpy格式然后再转换回来。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ndarray = tensor.cpu().numpy()</span><br><span class="line">tensor = torch.from_numpy(ndarray).float()</span><br><span class="line">tensor = torch.from_numpy(ndarray.copy()).float() # If ndarray has negative stride.</span><br></pre></td></tr></table></figure><h3 id="Torch-tensor与PIL-Image转换"><a href="#Torch-tensor与PIL-Image转换" class="headerlink" title="Torch.tensor与PIL.Image转换"></a><strong>Torch.tensor与PIL.Image转换</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># pytorch中的张量默认采用[N, C, H, W]的顺序，并且数据范围在[0,1]，需要进行转置和规范化</span><br><span class="line"># torch.Tensor -&gt; PIL.Image</span><br><span class="line">image = PIL.Image.fromarray(torch.clamp(tensor*255, min=0, max=255).byte().permute(1,2,0).cpu().numpy())</span><br><span class="line">image = torchvision.transforms.functional.to_pil_image(tensor)  # Equivalently way</span><br><span class="line"></span><br><span class="line"># PIL.Image -&gt; torch.Tensor</span><br><span class="line">path = r&#x27;./figure.jpg&#x27;</span><br><span class="line">tensor = torch.from_numpy(np.asarray(PIL.Image.open(path))).permute(2,0,1).float() / 255</span><br><span class="line">tensor = torchvision.transforms.functional.to_tensor(PIL.Image.open(path)) # Equivalently way</span><br></pre></td></tr></table></figure><h3 id="np-ndarray与PIL-Image的转换"><a href="#np-ndarray与PIL-Image的转换" class="headerlink" title="np.ndarray与PIL.Image的转换"></a><strong>np.ndarray与PIL.Image的转换</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">image = PIL.Image.fromarray(ndarray.astype(np.uint8))</span><br><span class="line">ndarray = np.asarray(PIL.Image.open(path))</span><br></pre></td></tr></table></figure><h3 id="从只包含一个元素的张量中提取值"><a href="#从只包含一个元素的张量中提取值" class="headerlink" title="从只包含一个元素的张量中提取值"></a>从只包含一个元素的张量中提取值</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">value = torch.rand(1).item()</span><br></pre></td></tr></table></figure><h3 id="张量形变"><a href="#张量形变" class="headerlink" title="张量形变"></a>张量形变</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 在将卷积层输入全连接层的情况下通常需要对张量做形变处理，</span><br><span class="line"># 相比torch.view，torch.reshape可以自动处理输入张量不连续的情况。</span><br><span class="line">tensor = torch.rand(2,3,4)</span><br><span class="line">shape = (6, 4)</span><br><span class="line">tensor = torch.reshape(tensor, shape)</span><br></pre></td></tr></table></figure><h3 id="打乱顺序"><a href="#打乱顺序" class="headerlink" title="打乱顺序"></a>打乱顺序</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor = tensor[torch.randperm(tensor.size(0))]  # 打乱第一个维度</span><br></pre></td></tr></table></figure><h3 id="水平翻转"><a href="#水平翻转" class="headerlink" title="水平翻转"></a>水平翻转</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># pytorch不支持tensor[::-1]这样的负步长操作，水平翻转可以通过张量索引实现</span><br><span class="line"># 假设张量的维度为[N, D, H, W].</span><br><span class="line">tensor = tensor[:,:,:,torch.arange(tensor.size(3) - 1, -1, -1).long()]</span><br></pre></td></tr></table></figure><h3 id="复制张量"><a href="#复制张量" class="headerlink" title="复制张量"></a>复制张量</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># Operation                 |  New/Shared memory | Still in computation graph |</span><br><span class="line">tensor.clone()            # |        New         |          Yes               |</span><br><span class="line">tensor.detach()           # |      Shared        |          No                |</span><br><span class="line">tensor.detach.clone()()   # |        New         |          No                |</span><br></pre></td></tr></table></figure><h3 id="张量拼接"><a href="#张量拼接" class="headerlink" title="张量拼接"></a>张量拼接</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">注意torch.cat和torch.stack的区别在于torch.cat沿着给定的维度拼接，</span><br><span class="line">而torch.stack会新增一维。例如当参数是3个10x5的张量，torch.cat的结果是30x5的张量，</span><br><span class="line">而torch.stack的结果是3x10x5的张量。</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">tensor = torch.cat(list_of_tensors, dim=0)</span><br><span class="line">tensor = torch.stack(list_of_tensors, dim=0)</span><br></pre></td></tr></table></figure><h3 id="将整数标签转为one-hot编码"><a href="#将整数标签转为one-hot编码" class="headerlink" title="将整数标签转为one-hot编码"></a>将整数标签转为one-hot编码</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># pytorch的标记默认从0开始</span><br><span class="line">tensor = torch.tensor([0, 2, 1, 3])</span><br><span class="line">N = tensor.size(0)</span><br><span class="line">num_classes = 4</span><br><span class="line">one_hot = torch.zeros(N, num_classes).long()</span><br><span class="line">one_hot.scatter_(dim=1, index=torch.unsqueeze(tensor, dim=1), src=torch.ones(N, num_classes).long())</span><br></pre></td></tr></table></figure><h3 id="得到非零元素"><a href="#得到非零元素" class="headerlink" title="得到非零元素"></a>得到非零元素</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.nonzero(tensor)               # index of non-zero elements</span><br><span class="line">torch.nonzero(tensor==0)            # index of zero elements</span><br><span class="line">torch.nonzero(tensor).size(0)       # number of non-zero elements</span><br><span class="line">torch.nonzero(tensor == 0).size(0)  # number of zero elements</span><br></pre></td></tr></table></figure><h3 id="判断两个张量相等"><a href="#判断两个张量相等" class="headerlink" title="判断两个张量相等"></a>判断两个张量相等</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.allclose(tensor1, tensor2)  # float tensor</span><br><span class="line">torch.equal(tensor1, tensor2)     # int tensor</span><br></pre></td></tr></table></figure><h3 id="张量扩展"><a href="#张量扩展" class="headerlink" title="张量扩展"></a>张量扩展</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Expand tensor of shape 64*512 to shape 64*512*7*7.</span><br><span class="line">tensor = torch.rand(64,512)</span><br><span class="line">torch.reshape(tensor, (64, 512, 1, 1)).expand(64, 512, 7, 7)</span><br></pre></td></tr></table></figure><h3 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Matrix multiplcation: (m*n) * (n*p) * -&gt; (m*p).</span><br><span class="line">result = torch.mm(tensor1, tensor2)</span><br><span class="line"></span><br><span class="line"># Batch matrix multiplication: (b*m*n) * (b*n*p) -&gt; (b*m*p)</span><br><span class="line">result = torch.bmm(tensor1, tensor2)</span><br><span class="line"></span><br><span class="line"># Element-wise multiplication.</span><br><span class="line">result = tensor1 * tensor2</span><br></pre></td></tr></table></figure><h3 id="计算两组数据之间的两两欧式距离"><a href="#计算两组数据之间的两两欧式距离" class="headerlink" title="计算两组数据之间的两两欧式距离"></a>计算两组数据之间的两两欧式距离</h3><p>利用broadcast机制</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dist = torch.sqrt(torch.sum((X1[:,None,:] - X2) ** 2, dim=2))</span><br></pre></td></tr></table></figure><h2 id="3-模型定义和操作"><a href="#3-模型定义和操作" class="headerlink" title="3. 模型定义和操作"></a>3. 模型定义和操作</h2><h3 id="一个简单两层卷积网络的示例"><a href="#一个简单两层卷积网络的示例" class="headerlink" title="一个简单两层卷积网络的示例"></a>一个简单两层卷积网络的示例</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># convolutional neural network (2 convolutional layers)</span><br><span class="line">class ConvNet(nn.Module):   </span><br><span class="line">    def __init__(self, num_classes=10):   </span><br><span class="line">        super(ConvNet, self).__init__()    </span><br><span class="line">        self.layer1 = nn.Sequential(     </span><br><span class="line">            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),           </span><br><span class="line">            nn.BatchNorm2d(16),     </span><br><span class="line">            nn.ReLU(),      </span><br><span class="line">            nn.MaxPool2d(kernel_size=2, stride=2))     </span><br><span class="line">        self.layer2 = nn.Sequential(       </span><br><span class="line">            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),         </span><br><span class="line">            nn.BatchNorm2d(32),    </span><br><span class="line">            nn.ReLU(),        </span><br><span class="line">            nn.MaxPool2d(kernel_size=2, stride=2))   </span><br><span class="line">        self.fc = nn.Linear(7*7*32, num_classes)   </span><br><span class="line">        </span><br><span class="line">    def forward(self, x):   </span><br><span class="line">        out = self.layer1(x)    </span><br><span class="line">        out = self.layer2(out)     </span><br><span class="line">        out = out.reshape(out.size(0), -1)     </span><br><span class="line">        out = self.fc(out)   </span><br><span class="line">        return out</span><br><span class="line">        </span><br><span class="line">model = ConvNet(num_classes).to(device)</span><br></pre></td></tr></table></figure><p>卷积层的计算和展示可以用这个网站辅助。</p><h3 id="双线性汇合（bilinear-pooling）"><a href="#双线性汇合（bilinear-pooling）" class="headerlink" title="双线性汇合（bilinear pooling）"></a>双线性汇合（bilinear pooling）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X = torch.reshape(N, D, H * W)       # Assume X has shape N*D*H*W</span><br><span class="line">X = torch.bmm(X, torch.transpose(X, 1, 2)) / (H * W)  # Bilinear pooling</span><br><span class="line">assert X.size() == (N, D, D)</span><br><span class="line">X = torch.reshape(X, (N, D * D))</span><br><span class="line">X = torch.sign(X) * torch.sqrt(torch.abs(X) + 1e-5)   # Signed-sqrt normalization</span><br><span class="line">X = torch.nn.functional.normalize(X)                  # L2 normalization</span><br></pre></td></tr></table></figure><h3 id="多卡同步-BN（Batch-normalization）"><a href="#多卡同步-BN（Batch-normalization）" class="headerlink" title="多卡同步 BN（Batch normalization）"></a>多卡同步 BN（Batch normalization）</h3><p>当使用 torch.nn.DataParallel 将代码运行在多张 GPU 卡上时，PyTorch 的 BN 层默认操作是各卡上数据独立地计算均值和标准差，同步 BN 使用所有卡上的数据一起计算 BN 层的均值和标准差，缓解了当批量大小（batch size）比较小时对均值和标准差估计不准的情况，是在目标检测等任务中一个有效的提升性能的技巧。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sync_bn = torch.nn.SyncBatchNorm(num_features, eps=1e-05, momentum=0.1, affine=True,             </span><br><span class="line">                                 track_running_stats=True)</span><br></pre></td></tr></table></figure><h3 id="将已有网络的所有BN层改为同步BN层"><a href="#将已有网络的所有BN层改为同步BN层" class="headerlink" title="将已有网络的所有BN层改为同步BN层"></a>将已有网络的所有BN层改为同步BN层</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def convertBNtoSyncBN(module, process_group=None):   </span><br><span class="line">    &#x27;&#x27;&#x27;Recursively replace all BN layers to SyncBN layer.  </span><br><span class="line">    </span><br><span class="line">    Args:     </span><br><span class="line">        module[torch.nn.Module]. Network  </span><br><span class="line">    &#x27;&#x27;&#x27; </span><br><span class="line">    if isinstance(module, torch.nn.modules.batchnorm._BatchNorm):      </span><br><span class="line">        sync_bn = torch.nn.SyncBatchNorm(module.num_features, module.eps, module.momentum,                                         </span><br><span class="line">                                         module.affine, module.track_running_stats, process_group)     </span><br><span class="line">        sync_bn.running_mean = module.running_mean      </span><br><span class="line">        sync_bn.running_var = module.running_var     </span><br><span class="line">        if module.affine:       </span><br><span class="line">            sync_bn.weight = module.weight.clone().detach() </span><br><span class="line">            sync_bn.bias = module.bias.clone().detach()   </span><br><span class="line">        return sync_bn  </span><br><span class="line">    else:    </span><br><span class="line">        for name, child_module in module.named_children():           </span><br><span class="line">            setattr(module, name) = convert_syncbn_model(child_module, process_group=process_group))    </span><br><span class="line">        return module</span><br></pre></td></tr></table></figure><h3 id="类似-BN-滑动平均"><a href="#类似-BN-滑动平均" class="headerlink" title="类似 BN 滑动平均"></a>类似 BN 滑动平均</h3><p>如果要实现类似 BN 滑动平均的操作，在 forward 函数中要使用原地（inplace）操作给滑动平均赋值。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">class BN(torch.nn.Module)  </span><br><span class="line">    def __init__(self):     </span><br><span class="line">        ...     </span><br><span class="line">        self.register_buffer(&#x27;running_mean&#x27;, torch.zeros(num_features))</span><br><span class="line">        </span><br><span class="line">    def forward(self, X):   </span><br><span class="line">        ...      </span><br><span class="line">        self.running_mean += momentum * (current - self.running_mean)</span><br></pre></td></tr></table></figure><h3 id="计算模型整体参数量"><a href="#计算模型整体参数量" class="headerlink" title="计算模型整体参数量"></a>计算模型整体参数量</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">num_parameters = sum(torch.numel(parameter) for parameter in model.parameters())</span><br></pre></td></tr></table></figure><h3 id="查看网络中的参数"><a href="#查看网络中的参数" class="headerlink" title="查看网络中的参数"></a>查看网络中的参数</h3><p>可以通过model.state_dict()或者model.named_parameters()函数查看现在的全部可训练参数（包括通过继承得到的父类中的参数）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">params = list(model.named_parameters())</span><br><span class="line">(name, param) = params[28]</span><br><span class="line">print(name)</span><br><span class="line">print(param.grad)</span><br><span class="line">print(&#x27;-------------------------------------------------&#x27;)</span><br><span class="line">(name2, param2) = params[29]</span><br><span class="line">print(name2)</span><br><span class="line">print(param2.grad)</span><br><span class="line">print(&#x27;----------------------------------------------------&#x27;)</span><br><span class="line">(name1, param1) = params[30]</span><br><span class="line">print(name1)</span><br><span class="line">print(param1.grad)</span><br></pre></td></tr></table></figure><h3 id="模型可视化（使用pytorchviz）"><a href="#模型可视化（使用pytorchviz）" class="headerlink" title="模型可视化（使用pytorchviz）"></a>模型可视化（使用pytorchviz）</h3><p>szagoruyko/pytorchvizgithub.com</p><h3 id="类似-Keras-的-model-summary-输出模型信息，使用pytorch-summary"><a href="#类似-Keras-的-model-summary-输出模型信息，使用pytorch-summary" class="headerlink" title="类似 Keras 的 model.summary() 输出模型信息，使用pytorch-summary"></a>类似 Keras 的 model.summary() 输出模型信息，使用pytorch-summary</h3><p>sksq96/pytorch-summarygithub.com</p><p><strong>模型权重初始化</strong></p><p>注意 model.modules() 和 model.children() 的区别：model.modules() 会迭代地遍历模型的所有子层，而 model.children() 只会遍历模型下的一层。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># Common practise for initialization.</span><br><span class="line">for layer in model.modules(): </span><br><span class="line">    if isinstance(layer, torch.nn.Conv2d):    </span><br><span class="line">        torch.nn.init.kaiming_normal_(layer.weight, mode=&#x27;fan_out&#x27;,                                </span><br><span class="line">                                       nonlinearity=&#x27;relu&#x27;) </span><br><span class="line">        if layer.bias is not None:    </span><br><span class="line">            torch.nn.init.constant_(layer.bias, val=0.0)  </span><br><span class="line">    elif isinstance(layer, torch.nn.BatchNorm2d):     </span><br><span class="line">        torch.nn.init.constant_(layer.weight, val=1.0)  </span><br><span class="line">        torch.nn.init.constant_(layer.bias, val=0.0)   </span><br><span class="line">    elif isinstance(layer, torch.nn.Linear):     </span><br><span class="line">        torch.nn.init.xavier_normal_(layer.weight)    </span><br><span class="line">        if layer.bias is not None:      </span><br><span class="line">            torch.nn.init.constant_(layer.bias, val=0.0)</span><br><span class="line"># Initialization with given tensor.</span><br><span class="line">layer.weight = torch.nn.Parameter(tensor)</span><br></pre></td></tr></table></figure><h3 id="提取模型中的某一层"><a href="#提取模型中的某一层" class="headerlink" title="提取模型中的某一层"></a>提取模型中的某一层</h3><p>modules()会返回模型中所有模块的迭代器，它能够访问到最内层，比如self.layer1.conv1这个模块，还有一个与它们相对应的是name_children()属性以及named_modules(),这两个不仅会返回模块的迭代器，还会返回网络层的名字。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 取模型中的前两层</span><br><span class="line">new_model = nn.Sequential(*list(model.children())[:2] </span><br><span class="line"># 如果希望提取出模型中的所有卷积层，可以像下面这样操作：</span><br><span class="line">for layer in model.named_modules():   </span><br><span class="line">    if isinstance(layer[1],nn.Conv2d):    </span><br><span class="line">         conv_model.add_module(layer[0],layer[1])</span><br></pre></td></tr></table></figure><h3 id="部分层使用预训练模型"><a href="#部分层使用预训练模型" class="headerlink" title="部分层使用预训练模型"></a>部分层使用预训练模型</h3><p>注意如果保存的模型是 torch.nn.DataParallel，则当前的模型也需要是</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(&#x27;model.pth&#x27;), strict=False)</span><br></pre></td></tr></table></figure><h3 id="将在-GPU-保存的模型加载到-CPU"><a href="#将在-GPU-保存的模型加载到-CPU" class="headerlink" title="将在 GPU 保存的模型加载到 CPU"></a>将在 GPU 保存的模型加载到 CPU</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(&#x27;model.pth&#x27;, map_location=&#x27;cpu&#x27;))</span><br></pre></td></tr></table></figure><h3 id="导入另一个模型的相同部分到新的模型"><a href="#导入另一个模型的相同部分到新的模型" class="headerlink" title="导入另一个模型的相同部分到新的模型"></a>导入另一个模型的相同部分到新的模型</h3><p>模型导入参数时，如果两个模型结构不一致，则直接导入参数会报错。用下面方法可以把另一个模型的相同的部分导入到新的模型中。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># model_new代表新的模型</span><br><span class="line"># model_saved代表其他模型，比如用torch.load导入的已保存的模型</span><br><span class="line">model_new_dict = model_new.state_dict()</span><br><span class="line">model_common_dict = &#123;k:v for k, v in model_saved.items() if k in model_new_dict.keys()&#125;</span><br><span class="line">model_new_dict.update(model_common_dict)</span><br><span class="line">model_new.load_state_dict(model_new_dict)</span><br></pre></td></tr></table></figure><h2 id="4-数据处理"><a href="#4-数据处理" class="headerlink" title="4. 数据处理"></a>4. 数据处理</h2><h3 id="计算数据集的均值和标准差"><a href="#计算数据集的均值和标准差" class="headerlink" title="计算数据集的均值和标准差"></a>计算数据集的均值和标准差</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import cv2</span><br><span class="line">import numpy as np</span><br><span class="line">from torch.utils.data import Dataset</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line">def compute_mean_and_std(dataset):  </span><br><span class="line">    # 输入PyTorch的dataset，输出均值和标准差   </span><br><span class="line">    mean_r = 0  </span><br><span class="line">    mean_g = 0   </span><br><span class="line">    mean_b = 0 </span><br><span class="line">    </span><br><span class="line">    for img, _ in dataset:    </span><br><span class="line">        img = np.asarray(img) # change PIL Image to numpy array     </span><br><span class="line">        mean_b += np.mean(img[:, :, 0])     </span><br><span class="line">        mean_g += np.mean(img[:, :, 1])   </span><br><span class="line">        mean_r += np.mean(img[:, :, 2])  </span><br><span class="line">        </span><br><span class="line">    mean_b /= len(dataset)  </span><br><span class="line">    mean_g /= len(dataset)   </span><br><span class="line">    mean_r /= len(dataset)  </span><br><span class="line">    </span><br><span class="line">    diff_r = 0    </span><br><span class="line">    diff_g = 0  </span><br><span class="line">    diff_b = 0  </span><br><span class="line">    </span><br><span class="line">    N = 0  </span><br><span class="line">    </span><br><span class="line">    for img, _ in dataset:   </span><br><span class="line">        img = np.asarray(img)      </span><br><span class="line">        </span><br><span class="line">        diff_b += np.sum(np.power(img[:, :, 0] - mean_b, 2))</span><br><span class="line">        diff_g += np.sum(np.power(img[:, :, 1] - mean_g, 2))</span><br><span class="line">        diff_r += np.sum(np.power(img[:, :, 2] - mean_r, 2)) </span><br><span class="line">        </span><br><span class="line">        N += np.prod(img[:, :, 0].shape)  </span><br><span class="line">    </span><br><span class="line">    std_b = np.sqrt(diff_b / N)  </span><br><span class="line">    std_g = np.sqrt(diff_g / N)  </span><br><span class="line">    std_r = np.sqrt(diff_r / N) </span><br><span class="line">    </span><br><span class="line">    mean = (mean_b.item() / 255.0, mean_g.item() / 255.0, mean_r.item() / 255.0) </span><br><span class="line">    std = (std_b.item() / 255.0, std_g.item() / 255.0, std_r.item() / 255.0) </span><br><span class="line">    return mean, std</span><br></pre></td></tr></table></figure><h3 id="得到视频数据基本信息"><a href="#得到视频数据基本信息" class="headerlink" title="得到视频数据基本信息"></a>得到视频数据基本信息</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import cv2</span><br><span class="line">video = cv2.VideoCapture(mp4_path)</span><br><span class="line">height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))</span><br><span class="line">width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))</span><br><span class="line">num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))</span><br><span class="line">fps = int(video.get(cv2.CAP_PROP_FPS))</span><br><span class="line">video.release()</span><br></pre></td></tr></table></figure><h3 id="TSN-每段（segment）采样一帧视频"><a href="#TSN-每段（segment）采样一帧视频" class="headerlink" title="TSN 每段（segment）采样一帧视频"></a>TSN 每段（segment）采样一帧视频</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">K = self._num_segments</span><br><span class="line">if is_train: </span><br><span class="line">    if num_frames &gt; K:    </span><br><span class="line">        # Random index for each segment.     </span><br><span class="line">        frame_indices = torch.randint(       </span><br><span class="line">            high=num_frames // K, size=(K,), dtype=torch.long)      </span><br><span class="line">        frame_indices += num_frames // K * torch.arange(K) </span><br><span class="line">    else:      </span><br><span class="line">        frame_indices = torch.randint(      </span><br><span class="line">            high=num_frames, size=(K - num_frames,), dtype=torch.long)     </span><br><span class="line">        frame_indices = torch.sort(torch.cat((       </span><br><span class="line">            torch.arange(num_frames), frame_indices)))[0]</span><br><span class="line">else:   </span><br><span class="line">    if num_frames &gt; K:  </span><br><span class="line">        # Middle index for each segment.    </span><br><span class="line">        frame_indices = num_frames / K // 2     </span><br><span class="line">        frame_indices += num_frames // K * torch.arange(K)  </span><br><span class="line">    else:    </span><br><span class="line">        frame_indices = torch.sort(torch.cat((      </span><br><span class="line">            torch.arange(num_frames), torch.arange(K - num_frames))))[0]</span><br><span class="line">assert frame_indices.size() == (K,)</span><br><span class="line">return [frame_indices[i] for i in range(K)]</span><br></pre></td></tr></table></figure><h3 id="常用训练和验证数据预处理"><a href="#常用训练和验证数据预处理" class="headerlink" title="常用训练和验证数据预处理"></a>常用训练和验证数据预处理</h3><p>其中 ToTensor 操作会将 PIL.Image 或形状为 H×W×D，数值范围为 [0, 255] 的 np.ndarray 转换为形状为 D×H×W，数值范围为 [0.0, 1.0] 的 torch.Tensor。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">train_transform = torchvision.transforms.Compose([ </span><br><span class="line">    torchvision.transforms.RandomResizedCrop(size=224,                                       </span><br><span class="line">                                             scale=(0.08, 1.0)),   </span><br><span class="line">    torchvision.transforms.RandomHorizontalFlip(),   </span><br><span class="line">    torchvision.transforms.ToTensor(),  </span><br><span class="line">    torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),                                </span><br><span class="line">                                     std=(0.229, 0.224, 0.225)</span><br><span class="line">]) </span><br><span class="line">val_transform = torchvision.transforms.Compose([  </span><br><span class="line">   torchvision.transforms.Resize(256),  </span><br><span class="line">   torchvision.transforms.CenterCrop(224),   </span><br><span class="line">   torchvision.transforms.ToTensor(), </span><br><span class="line">   torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),                                  </span><br><span class="line">                                    std=(0.229, 0.224, 0.225)),</span><br><span class="line">])</span><br></pre></td></tr></table></figure><h2 id="5-模型训练和测试"><a href="#5-模型训练和测试" class="headerlink" title="5. 模型训练和测试"></a>5. 模型训练和测试</h2><h3 id="分类模型训练代码"><a href="#分类模型训练代码" class="headerlink" title="分类模型训练代码"></a>分类模型训练代码</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># Loss and optimizer</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"># Train the model</span><br><span class="line">total_step = len(train_loader)</span><br><span class="line">for epoch in range(num_epochs):  </span><br><span class="line">    for i ,(images, labels) in enumerate(train_loader):    </span><br><span class="line">        images = images.to(device)    </span><br><span class="line">        labels = labels.to(device)      </span><br><span class="line">        </span><br><span class="line">        # Forward pass    </span><br><span class="line">        outputs = model(images)       </span><br><span class="line">        loss = criterion(outputs, labels)     </span><br><span class="line">        </span><br><span class="line">        # Backward and optimizer   </span><br><span class="line">        optimizer.zero_grad()     </span><br><span class="line">        loss.backward() </span><br><span class="line">        optimizer.step()      </span><br><span class="line">        </span><br><span class="line">        if (i+1) % 100 == 0:    </span><br><span class="line">            print(&#x27;Epoch: [&#123;&#125;/&#123;&#125;], Step: [&#123;&#125;/&#123;&#125;], Loss: &#123;&#125;&#x27;                 </span><br><span class="line">                  .format(epoch+1, num_epochs, i+1, total_step, loss.item()))</span><br></pre></td></tr></table></figure><h3 id="分类模型测试代码"><a href="#分类模型测试代码" class="headerlink" title="分类模型测试代码"></a>分类模型测试代码</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># Test the model</span><br><span class="line">model.eval()  # eval mode(batch norm uses moving mean/variance             </span><br><span class="line">              #instead of mini-batch mean/variance)</span><br><span class="line">with torch.no_grad(): </span><br><span class="line">    correct = 0  </span><br><span class="line">    total = 0 </span><br><span class="line">    for images, labels in test_loader:   </span><br><span class="line">        images = images.to(device)     </span><br><span class="line">        labels = labels.to(device)    </span><br><span class="line">        outputs = model(images)      </span><br><span class="line">        _, predicted = torch.max(outputs.data, 1)    </span><br><span class="line">        total += labels.size(0)      </span><br><span class="line">        correct += (predicted == labels).sum().item()   </span><br><span class="line">        </span><br><span class="line">        print(&#x27;Test accuracy of the model on the 10000 test images: &#123;&#125; %&#x27;      </span><br><span class="line">              .format(100 * correct / total))</span><br></pre></td></tr></table></figure><h3 id="自定义loss"><a href="#自定义loss" class="headerlink" title="自定义loss"></a>自定义loss</h3><p>继承torch.nn.Module类写自己的loss。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">class MyLoss(torch.nn.Moudle):  </span><br><span class="line">    def __init__(self):    </span><br><span class="line">        super(MyLoss, self).__init__()</span><br><span class="line">        </span><br><span class="line">    def forward(self, x, y):    </span><br><span class="line">        loss = torch.mean((x - y) ** 2)   </span><br><span class="line">        return loss</span><br></pre></td></tr></table></figure><h3 id="标签平滑（label-smoothing）"><a href="#标签平滑（label-smoothing）" class="headerlink" title="标签平滑（label smoothing）"></a>标签平滑（label smoothing）</h3><p>写一个label_smoothing.py的文件，然后在训练代码里引用，用LSR代替交叉熵损失即可。label_smoothing.py内容如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line">class LSR(nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self, e=0.1, reduction=&#x27;mean&#x27;):   </span><br><span class="line">        super().__init__()</span><br><span class="line">        </span><br><span class="line">        self.log_softmax = nn.LogSoftmax(dim=1)   </span><br><span class="line">        self.e = e     </span><br><span class="line">        self.reduction = reduction</span><br><span class="line">        </span><br><span class="line">    def _one_hot(self, labels, classes, value=1):   </span><br><span class="line">        &quot;&quot;&quot;        </span><br><span class="line">            Convert labels to one hot vectors</span><br><span class="line">            </span><br><span class="line">        Args:      </span><br><span class="line">            labels: torch tensor in format [label1, label2, label3, ...]       </span><br><span class="line">            classes: int, number of classes       </span><br><span class="line">            value: label value in one hot vector, default to 1</span><br><span class="line">            </span><br><span class="line">        Returns:        </span><br><span class="line">            return one hot format labels in shape [batchsize, classes]    </span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        </span><br><span class="line">        one_hot = torch.zeros(labels.size(0), classes)</span><br><span class="line">        </span><br><span class="line">        #labels and value_added  size must match     </span><br><span class="line">        labels = labels.view(labels.size(0), -1)   </span><br><span class="line">        value_added = torch.Tensor(labels.size(0), 1).fill_(value)</span><br><span class="line">        </span><br><span class="line">        value_added = value_added.to(labels.device)   </span><br><span class="line">        one_hot = one_hot.to(labels.device)</span><br><span class="line">        </span><br><span class="line">        one_hot.scatter_add_(1, labels, value_added)</span><br><span class="line">        </span><br><span class="line">        return one_hot</span><br><span class="line">        </span><br><span class="line">    def _smooth_label(self, target, length, smooth_factor): </span><br><span class="line">        &quot;&quot;&quot;convert targets to one-hot format, and smooth  </span><br><span class="line">        them.  </span><br><span class="line">        Args:      </span><br><span class="line">            target: target in form with [label1, label2, label_batchsize]        </span><br><span class="line">            length: length of one-hot format(number of classes)         </span><br><span class="line">            smooth_factor: smooth factor for label smooth</span><br><span class="line">       </span><br><span class="line">       Returns:        </span><br><span class="line">           smoothed labels in one hot format   </span><br><span class="line">       &quot;&quot;&quot;    </span><br><span class="line">       one_hot = self._one_hot(target, length, value=1 - smooth_factor)      </span><br><span class="line">       one_hot += smooth_factor / (length - 1)</span><br><span class="line">       </span><br><span class="line">       return one_hot.to(target.device)</span><br><span class="line">       </span><br><span class="line">    def forward(self, x, target):</span><br><span class="line">    </span><br><span class="line">        if x.size(0) != target.size(0):      </span><br><span class="line">            raise ValueError(&#x27;Expected input batchsize (&#123;&#125;) to match target batch_size(&#123;&#125;)&#x27;                </span><br><span class="line">                    .format(x.size(0), target.size(0)))</span><br><span class="line">                    </span><br><span class="line">        if x.dim() &lt; 2:       </span><br><span class="line">            raise ValueError(&#x27;Expected input tensor to have least 2 dimensions(got &#123;&#125;)&#x27;       </span><br><span class="line">                    .format(x.size(0)))</span><br><span class="line">                    </span><br><span class="line">        if x.dim() != 2:     </span><br><span class="line">            raise ValueError(&#x27;Only 2 dimension tensor are implemented, (got &#123;&#125;)&#x27;           </span><br><span class="line">                    .format(x.size()))</span><br><span class="line"></span><br><span class="line">        smoothed_target = self._smooth_label(target, x.size(1), self.e)     </span><br><span class="line">        x = self.log_softmax(x)   </span><br><span class="line">        loss = torch.sum(- x * smoothed_target, dim=1)</span><br><span class="line">        </span><br><span class="line">        if self.reduction == &#x27;none&#x27;:         </span><br><span class="line">            return loss</span><br><span class="line">            </span><br><span class="line">        elif self.reduction == &#x27;sum&#x27;:        </span><br><span class="line">            return torch.sum(loss)</span><br><span class="line">            </span><br><span class="line">        elif self.reduction == &#x27;mean&#x27;:      </span><br><span class="line">            return torch.mean(loss)</span><br><span class="line">            </span><br><span class="line">        else:         </span><br><span class="line">            raise ValueError(&#x27;unrecognized option, expect reduction to be one of none, mean, sum&#x27;)</span><br></pre></td></tr></table></figure><p>或者直接在训练文件里做label smoothing</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">for images, labels in train_loader:  </span><br><span class="line">    images, labels = images.cuda(), labels.cuda()  </span><br><span class="line">    N = labels.size(0) </span><br><span class="line">    # C is the number of classes.  </span><br><span class="line">    smoothed_labels = torch.full(size=(N, C), fill_value=0.1 / (C - 1)).cuda()  </span><br><span class="line">    smoothed_labels.scatter_(dim=1, index=torch.unsqueeze(labels, dim=1), value=0.9)</span><br><span class="line">    </span><br><span class="line">    score = model(images)  </span><br><span class="line">    log_prob = torch.nn.functional.log_softmax(score, dim=1)  </span><br><span class="line">    loss = -torch.sum(log_prob * smoothed_labels) / N   </span><br><span class="line">    optimizer.zero_grad()  </span><br><span class="line">    loss.backward() </span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure><h3 id="Mixup训练"><a href="#Mixup训练" class="headerlink" title="Mixup训练"></a>Mixup训练</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">beta_distribution = torch.distributions.beta.Beta(alpha, alpha)</span><br><span class="line">for images, labels in train_loader:  </span><br><span class="line">    images, labels = images.cuda(), labels.cuda()   </span><br><span class="line">    </span><br><span class="line">    # Mixup images and labels.  </span><br><span class="line">    lambda_ = beta_distribution.sample([]).item()   </span><br><span class="line">    index = torch.randperm(images.size(0)).cuda()  </span><br><span class="line">    mixed_images = lambda_ * images + (1 - lambda_) * images[index, :]  </span><br><span class="line">    label_a, label_b = labels, labels[index]  </span><br><span class="line">    </span><br><span class="line">    # Mixup loss.  </span><br><span class="line">    scores = model(mixed_images) </span><br><span class="line">    loss = (lambda_ * loss_function(scores, label_a)     </span><br><span class="line">            + (1 - lambda_) * loss_function(scores, label_b))  </span><br><span class="line">    optimizer.zero_grad() </span><br><span class="line">    loss.backward()  </span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure><h3 id="L1-正则化"><a href="#L1-正则化" class="headerlink" title="L1 正则化"></a>L1 正则化</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">l1_regularization = torch.nn.L1Loss(reduction=&#x27;sum&#x27;)</span><br><span class="line">loss = ...  # Standard cross-entropy loss</span><br><span class="line">for param in model.parameters():  </span><br><span class="line">    loss += torch.sum(torch.abs(param))</span><br><span class="line">loss.backward()</span><br></pre></td></tr></table></figure><h3 id="不对偏置项进行权重衰减（weight-decay）"><a href="#不对偏置项进行权重衰减（weight-decay）" class="headerlink" title="不对偏置项进行权重衰减（weight decay）"></a>不对偏置项进行权重衰减（weight decay）</h3><p>pytorch里的weight decay相当于l2正则</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bias_list = (param for name, param in model.named_parameters() if name[-4:] == &#x27;bias&#x27;)</span><br><span class="line">others_list = (param for name, param in model.named_parameters() if name[-4:] != &#x27;bias&#x27;)</span><br><span class="line">parameters = [&#123;&#x27;parameters&#x27;: bias_list, &#x27;weight_decay&#x27;: 0&#125;,                            </span><br><span class="line">              &#123;&#x27;parameters&#x27;: others_list&#125;]</span><br><span class="line">optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4)</span><br></pre></td></tr></table></figure><h3 id="梯度裁剪（gradient-clipping）"><a href="#梯度裁剪（gradient-clipping）" class="headerlink" title="梯度裁剪（gradient clipping）"></a>梯度裁剪（gradient clipping）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20)</span><br></pre></td></tr></table></figure><h3 id="得到当前学习率"><a href="#得到当前学习率" class="headerlink" title="得到当前学习率"></a>得到当前学习率</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># If there is one global learning rate (which is the common case).</span><br><span class="line">lr = next(iter(optimizer.param_groups))[&#x27;lr&#x27;]</span><br><span class="line"></span><br><span class="line"># If there are multiple learning rates for different layers.</span><br><span class="line">all_lr = []</span><br><span class="line">for param_group in optimizer.param_groups:  </span><br><span class="line">    all_lr.append(param_group[&#x27;lr&#x27;])</span><br></pre></td></tr></table></figure><p>另一种方法，在一个batch训练代码里，当前的lr是optimizer.param_groups[0][‘lr’]</p><h3 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># Reduce learning rate when validation accuarcy plateau.</span><br><span class="line">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=&#x27;max&#x27;, patience=5, verbose=True)</span><br><span class="line">for t in range(0, 80):  </span><br><span class="line">    train(...)   </span><br><span class="line">    val(...)  </span><br><span class="line">    scheduler.step(val_acc)</span><br><span class="line">    </span><br><span class="line"># Cosine annealing learning rate.</span><br><span class="line">scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=80)</span><br><span class="line"># Reduce learning rate by 10 at given epochs.</span><br><span class="line">scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 70], gamma=0.1)</span><br><span class="line">for t in range(0, 80):  </span><br><span class="line">    scheduler.step()    </span><br><span class="line">    train(...)  </span><br><span class="line">    val(...)</span><br><span class="line">    </span><br><span class="line"># Learning rate warmup by 10 epochs.</span><br><span class="line">scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: t / 10)</span><br><span class="line">for t in range(0, 10):  </span><br><span class="line">    scheduler.step()  </span><br><span class="line">    train(...)  </span><br><span class="line">    val(...)</span><br></pre></td></tr></table></figure><h3 id="优化器链式更新"><a href="#优化器链式更新" class="headerlink" title="优化器链式更新"></a>优化器链式更新</h3><p>从1.4版本开始，torch.optim.lr_scheduler 支持链式更新（chaining），即用户可以定义两个 schedulers，并交替在训练中使用。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.optim import SGD</span><br><span class="line">from torch.optim.lr_scheduler import ExponentialLR, StepLR</span><br><span class="line">model = [torch.nn.Parameter(torch.randn(2, 2, requires_grad=True))]</span><br><span class="line">optimizer = SGD(model, 0.1)</span><br><span class="line">scheduler1 = ExponentialLR(optimizer, gamma=0.9)</span><br><span class="line">scheduler2 = StepLR(optimizer, step_size=3, gamma=0.1)</span><br><span class="line">for epoch in range(4):  </span><br><span class="line">    print(epoch, scheduler2.get_last_lr()[0]) </span><br><span class="line">    optimizer.step()  </span><br><span class="line">    scheduler1.step()  </span><br><span class="line">    scheduler2.step()</span><br></pre></td></tr></table></figure><h3 id="模型训练可视化"><a href="#模型训练可视化" class="headerlink" title="模型训练可视化"></a>模型训练可视化</h3><p>PyTorch可以使用tensorboard来可视化训练过程。</p><p>安装和运行TensorBoard。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install tensorboard</span><br><span class="line">tensorboard --logdir=runs</span><br></pre></td></tr></table></figure><p>使用SummaryWriter类来收集和可视化相应的数据，放了方便查看，可以使用不同的文件夹，比如’Loss/train’和’Loss/test’。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter()</span><br><span class="line"></span><br><span class="line">for n_iter in range(100):  </span><br><span class="line">    writer.add_scalar(&#x27;Loss/train&#x27;, np.random.random(), n_iter)   </span><br><span class="line">    writer.add_scalar(&#x27;Loss/test&#x27;, np.random.random(), n_iter)   </span><br><span class="line">    writer.add_scalar(&#x27;Accuracy/train&#x27;, np.random.random(), n_iter)  </span><br><span class="line">    writer.add_scalar(&#x27;Accuracy/test&#x27;, np.random.random(), n_iter)</span><br></pre></td></tr></table></figure><h3 id="保存与加载断点"><a href="#保存与加载断点" class="headerlink" title="保存与加载断点"></a>保存与加载断点</h3><p>注意为了能够恢复训练，我们需要同时保存模型和优化器的状态，以及当前的训练轮数。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">start_epoch = 0</span><br><span class="line"># Load checkpoint.</span><br><span class="line">if resume: # resume为参数，第一次训练时设为0，中断再训练时设为1  </span><br><span class="line">    model_path = os.path.join(&#x27;model&#x27;, &#x27;best_checkpoint.pth.tar&#x27;)  </span><br><span class="line">    assert os.path.isfile(model_path) </span><br><span class="line">    checkpoint = torch.load(model_path)  </span><br><span class="line">    best_acc = checkpoint[&#x27;best_acc&#x27;]  </span><br><span class="line">    start_epoch = checkpoint[&#x27;epoch&#x27;]  </span><br><span class="line">    model.load_state_dict(checkpoint[&#x27;model&#x27;])   </span><br><span class="line">    optimizer.load_state_dict(checkpoint[&#x27;optimizer&#x27;])  </span><br><span class="line">    print(&#x27;Load checkpoint at epoch &#123;&#125;.&#x27;.format(start_epoch)) </span><br><span class="line">    print(&#x27;Best accuracy so far &#123;&#125;.&#x27;.format(best_acc))</span><br><span class="line">    </span><br><span class="line"># Train the model</span><br><span class="line">for epoch in range(start_epoch, num_epochs):    </span><br><span class="line">    ... </span><br><span class="line">    </span><br><span class="line">    # Test the model  </span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">    # save checkpoint  </span><br><span class="line">    is_best = current_acc &gt; best_acc   </span><br><span class="line">    best_acc = max(current_acc, best_acc) </span><br><span class="line">    checkpoint = &#123;   </span><br><span class="line">        &#x27;best_acc&#x27;: best_acc,   </span><br><span class="line">        &#x27;epoch&#x27;: epoch + 1,   </span><br><span class="line">        &#x27;model&#x27;: model.state_dict(),   </span><br><span class="line">        &#x27;optimizer&#x27;: optimizer.state_dict(),   </span><br><span class="line">    &#125;  </span><br><span class="line">    model_path = os.path.join(&#x27;model&#x27;, &#x27;checkpoint.pth.tar&#x27;)    </span><br><span class="line">    best_model_path = os.path.join(&#x27;model&#x27;, &#x27;best_checkpoint.pth.tar&#x27;) </span><br><span class="line">    torch.save(checkpoint, model_path)  </span><br><span class="line">    if is_best:    </span><br><span class="line">        shutil.copy(model_path, best_model_path)</span><br></pre></td></tr></table></figure><h3 id="提取-ImageNet-预训练模型某层的卷积特征"><a href="#提取-ImageNet-预训练模型某层的卷积特征" class="headerlink" title="提取 ImageNet 预训练模型某层的卷积特征"></a>提取 ImageNet 预训练模型某层的卷积特征</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># VGG-16 relu5-3 feature.</span><br><span class="line">model = torchvision.models.vgg16(pretrained=True).features[:-1]</span><br><span class="line"># VGG-16 pool5 feature.</span><br><span class="line">model = torchvision.models.vgg16(pretrained=True).features</span><br><span class="line"># VGG-16 fc7 feature.</span><br><span class="line">model = torchvision.models.vgg16(pretrained=True)</span><br><span class="line">model.classifier = torch.nn.Sequential(*list(model.classifier.children())[:-3])</span><br><span class="line"># ResNet GAP feature.</span><br><span class="line">model = torchvision.models.resnet18(pretrained=True)</span><br><span class="line">model = torch.nn.Sequential(collections.OrderedDict(   </span><br><span class="line">    list(model.named_children())[:-1]))</span><br><span class="line">    </span><br><span class="line">with torch.no_grad():  </span><br><span class="line">    model.eval()  </span><br><span class="line">    conv_representation = model(image)</span><br></pre></td></tr></table></figure><h3 id="提取-ImageNet-预训练模型多层的卷积特征"><a href="#提取-ImageNet-预训练模型多层的卷积特征" class="headerlink" title="提取 ImageNet 预训练模型多层的卷积特征"></a>提取 ImageNet 预训练模型多层的卷积特征</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">class FeatureExtractor(torch.nn.Module): </span><br><span class="line">    &quot;&quot;&quot;Helper class to extract several convolution features from the given   </span><br><span class="line">    pre-trained model.</span><br><span class="line">    </span><br><span class="line">    Attributes:   </span><br><span class="line">        _model, torch.nn.Module.    </span><br><span class="line">        _layers_to_extract, list&lt;str&gt; or set&lt;str&gt;</span><br><span class="line">        </span><br><span class="line">    Example:     </span><br><span class="line">        &gt;&gt;&gt; model = torchvision.models.resnet152(pretrained=True)     </span><br><span class="line">        &gt;&gt;&gt; model = torch.nn.Sequential(collections.OrderedDict(          </span><br><span class="line">                list(model.named_children())[:-1]))    </span><br><span class="line">        &gt;&gt;&gt; conv_representation = FeatureExtractor(     </span><br><span class="line">                pretrained_model=model,           </span><br><span class="line">                layers_to_extract=&#123;&#x27;layer1&#x27;, &#x27;layer2&#x27;, &#x27;layer3&#x27;, &#x27;layer4&#x27;&#125;)(image)  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    def __init__(self, pretrained_model, layers_to_extract):       </span><br><span class="line">        torch.nn.Module.__init__(self)   </span><br><span class="line">        self._model = pretrained_model   </span><br><span class="line">        self._model.eval()   </span><br><span class="line">        self._layers_to_extract = set(layers_to_extract)</span><br><span class="line">        </span><br><span class="line">    def forward(self, x):    </span><br><span class="line">        with torch.no_grad():     </span><br><span class="line">           conv_representation = []       </span><br><span class="line">           for name, layer in self._model.named_children():           </span><br><span class="line">               x = layer(x)       </span><br><span class="line">               if name in self._layers_to_extract:   </span><br><span class="line">                   conv_representation.append(x)    </span><br><span class="line">            return conv_representation</span><br></pre></td></tr></table></figure><h3 id="微调全连接层"><a href="#微调全连接层" class="headerlink" title="微调全连接层"></a>微调全连接层</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = torchvision.models.resnet18(pretrained=True)</span><br><span class="line">for param in model.parameters():  </span><br><span class="line">    param.requires_grad = False</span><br><span class="line">model.fc = nn.Linear(512, 100)  # Replace the last fc layer</span><br><span class="line">optimizer = torch.optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)</span><br></pre></td></tr></table></figure><h3 id="以较大学习率微调全连接层，较小学习率微调卷积层"><a href="#以较大学习率微调全连接层，较小学习率微调卷积层" class="headerlink" title="以较大学习率微调全连接层，较小学习率微调卷积层"></a>以较大学习率微调全连接层，较小学习率微调卷积层</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = torchvision.models.resnet18(pretrained=True)</span><br><span class="line">finetuned_parameters = list(map(id, model.fc.parameters()))</span><br><span class="line">conv_parameters = (p for p in model.parameters() if id(p) not in finetuned_parameters)</span><br><span class="line">parameters = [&#123;&#x27;params&#x27;: conv_parameters, &#x27;lr&#x27;: 1e-3&#125;,    </span><br><span class="line">              &#123;&#x27;params&#x27;: model.fc.parameters()&#125;]</span><br><span class="line">optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4)</span><br></pre></td></tr></table></figure><h2 id="ssh连接"><a href="#ssh连接" class="headerlink" title="ssh连接"></a>ssh连接</h2><p>ssh -L 16006:127.0.0.1:6006 <a href="mailto:&#x61;&#x63;&#x63;&#111;&#117;&#110;&#116;&#x40;&#x73;&#101;&#x72;&#118;&#101;&#x72;&#x2e;&#97;&#x64;&#100;&#x72;&#101;&#x73;&#x73;">&#x61;&#x63;&#x63;&#111;&#117;&#110;&#116;&#x40;&#x73;&#101;&#x72;&#118;&#101;&#x72;&#x2e;&#97;&#x64;&#100;&#x72;&#101;&#x73;&#x73;</a></p><p>服务器地址只写ip不写端口</p><p>16006是自己电脑的端口</p><p>6006是服务器上服务的端口。</p><h2 id="6-其他注意事项"><a href="#6-其他注意事项" class="headerlink" title="6. 其他注意事项"></a>6. 其他注意事项</h2><p>不要使用太大的线性层。因为nn.Linear(m,n)使用的是的内存，线性层太大很容易超出现有显存。</p><p>不要在太长的序列上使用RNN。因为RNN反向传播使用的是BPTT算法，其需要的内存和输入序列的长度呈线性关系。</p><p>model(x) 前用 model.train() 和 model.eval() 切换网络状态。</p><p>不需要计算梯度的代码块用 with torch.no_grad() 包含起来。</p><p>model.eval() 和 torch.no_grad() 的区别在于，model.eval() 是将网络切换为测试状态，例如 BN 和dropout在训练和测试阶段使用不同的计算方法。torch.no_grad() 是关闭 PyTorch 张量的自动求导机制，以减少存储使用和加速计算，得到的结果无法进行 loss.backward()。</p><p>model.zero_grad()会把整个模型的参数的梯度都归零, 而optimizer.zero_grad()只会把传入其中的参数的梯度归零.</p><p>torch.nn.CrossEntropyLoss 的输入不需要经过 Softmax。torch.nn.CrossEntropyLoss 等价于 torch.nn.functional.log_softmax + torch.nn.NLLLoss。</p><p>loss.backward() 前用 optimizer.zero_grad() 清除累积梯度。</p><p>torch.utils.data.DataLoader 中尽量设置 pin_memory=True，对特别小的数据集如 MNIST 设置 pin_memory=False 反而更快一些。num_workers 的设置需要在实验中找到最快的取值。</p><p>用 del 及时删除不用的中间变量，节约 GPU 存储。</p><p>使用 inplace 操作可节约 GPU 存储，如</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = torch.nn.functional.relu(x, inplace=True)</span><br></pre></td></tr></table></figure><p>减少 CPU 和 GPU 之间的数据传输。例如如果你想知道一个 epoch 中每个 mini-batch 的 loss 和准确率，先将它们累积在 GPU 中等一个 epoch 结束之后一起传输回 CPU 会比每个 mini-batch 都进行一次 GPU 到 CPU 的传输更快。</p><p>使用半精度浮点数 half() 会有一定的速度提升，具体效率依赖于 GPU 型号。需要小心数值精度过低带来的稳定性问题。</p><p>时常使用 assert tensor.size() == (N, D, H, W) 作为调试手段，确保张量维度和你设想中一致。</p><p>除了标记 y 外，尽量少使用一维张量，使用 n*1 的二维张量代替，可以避免一些意想不到的一维张量计算结果。</p><p>统计代码各部分耗时</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">with torch.autograd.profiler.profile(enabled=True, use_cuda=False) as profile:  </span><br><span class="line">    ...</span><br><span class="line">print(profile)</span><br><span class="line"></span><br><span class="line"># 或者在命令行运行</span><br><span class="line">python -m torch.utils.bottleneck main.py</span><br></pre></td></tr></table></figure><p>使用TorchSnooper来调试PyTorch代码，程序在执行的时候，就会自动 print 出来每一行的执行结果的 tensor 的形状、数据类型、设备、是否需要梯度的信息。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># pip install torchsnooper</span><br><span class="line">import torchsnooper</span><br><span class="line"></span><br><span class="line"># 对于函数，使用修饰器</span><br><span class="line">@torchsnooper.snoop()</span><br><span class="line"></span><br><span class="line"># 如果不是函数，使用 with 语句来激活 TorchSnooper，把训练的那个循环装进 with 语句中去。</span><br><span class="line">with torchsnooper.snoop():  </span><br><span class="line">    原本的代码</span><br></pre></td></tr></table></figure><p><a href="https://github.com/zasdfgbnm/TorchSnoopergithub.com">https://github.com/zasdfgbnm/TorchSnoopergithub.com</a></p><p>模型可解释性，使用captum库：<a href="https://captum.ai/captum.ai">https://captum.ai/captum.ai</a></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a><strong>参考资料</strong></h2><ol><li> 张皓：PyTorch Cookbook（常用代码段整理合集），<a href="https://zhuanlan.zhihu.com/p/59205847">https://zhuanlan.zhihu.com/p/59205847</a>?</li><li> PyTorch官方文档和示例</li><li> <a href="https://pytorch.org/docs/stable/notes/faq.html">https://pytorch.org/docs/stable/notes/faq.html</a></li><li> <a href="https://github.com/szagoruyko/pytorchviz">https://github.com/szagoruyko/pytorchviz</a></li><li> <a href="https://github.com/sksq96/pytorch-summary">https://github.com/sksq96/pytorch-summary</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;日志输出&quot;&gt;&lt;a href=&quot;#日志输出&quot; class=&quot;headerlink&quot; title=&quot;日志输出&quot;&gt;&lt;/a&gt;日志输出&lt;/h2&gt;&lt;p&gt;利用logging模块在控制台实时打印并及时记录运行日志。&lt;/p&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;from config import  *&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;import logging  # 引入logging模块&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;import os.path&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;class Logger:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    def __init__(self,mode=&amp;#x27;w&amp;#x27;):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        # 第一步，创建一个logger&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.logger = logging.getLogger()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.logger.setLevel(logging.INFO)  # Log等级总开关&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        # 第二步，创建一个handler，用于写入日志文件&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        rq = time.strftime(&amp;#x27;%Y%m%d%H%M&amp;#x27;, time.localtime(time.time()))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        log_path = os.getcwd() + &amp;#x27;/Logs/&amp;#x27;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        log_name = log_path + rq + &amp;#x27;.log&amp;#x27;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        logfile = log_name&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        fh = logging.FileHandler(logfile, mode=mode)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        fh.setLevel(logging.DEBUG)  # 输出到file的log等级的开关&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        # 第三步，定义handler的输出格式&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        formatter = logging.Formatter(&amp;quot;%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s&amp;quot;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        fh.setFormatter(formatter)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        # 第四步，将logger添加到handler里面&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.logger.addHandler(fh)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ch = logging.StreamHandler()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ch.setLevel(logging.INFO)  # 输出到console的log等级的开关&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ch.setFormatter(formatter)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.logger.addHandler(ch)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;h2 id=&quot;模型的保存和读取&quot;&gt;&lt;a href=&quot;#模型的保存和读取&quot; class=&quot;headerlink&quot; title=&quot;模型的保存和读取&quot;&gt;&lt;/a&gt;模型的保存和读取&lt;/h2&gt;&lt;p&gt;​    &lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;保存&lt;/th&gt;
&lt;th align=&quot;left&quot;&gt;（1）torch.save(net.state_dict(),保存路径) 。（2）多卡训练时，要用 torch.save(net.module.state_dict(),’./model/best.pth’)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;读取&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;（1）net.load_state_dict(torch.load(‘best.pth’))。（2）并行时map_location=device.type在读取模型的时候一定要加上。即：    model.load_state_dict(torch.load(‘model/self_train_bestv2.pth’,map_location=’cuda’))&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;torch.save(model.state_dict(),’checkpoint_0.tar’,_use_new_zipfile_serialization=False) #解决版本不兼容&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;</summary>
    
    
    
    
    <category term="pytorch" scheme="https://jpccc.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>深度学习训练tricks</title>
    <link href="https://jpccc.github.io/2022/04/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20tricks/"/>
    <id>https://jpccc.github.io/2022/04/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20tricks/</id>
    <published>2022-04-06T15:06:13.000Z</published>
    <updated>2022-04-07T08:44:25.458Z</updated>
    
    <content type="html"><![CDATA[<h1 id="深度神经网络模型训练中的-tricks（原理与代码汇总）"><a href="#深度神经网络模型训练中的-tricks（原理与代码汇总）" class="headerlink" title="深度神经网络模型训练中的 tricks（原理与代码汇总）"></a>深度神经网络模型训练中的 tricks（原理与代码汇总）</h1><p>本文总结了多种<strong>图像分类</strong>任务中的重要技巧，对于<strong>目标检测和图像分割</strong>等任务，也起到了不错的作用。</p><p>计算机视觉主要问题有图像分类、目标检测和图像分割等。针对图像分类任务，提升准确率的方法路线有两条，一个是模型的修改，另一个是各种数据处理和训练的技巧(tricks)。图像分类中的各种技巧对于目标检测、图像分割等任务也有很好的作用，因此值得好好总结。本文在精读论文的基础上，总结了图像分类任务的各种tricks如下：</p><span id="more"></span><h3 id="tricks合集"><a href="#tricks合集" class="headerlink" title="tricks合集"></a>tricks合集</h3><ul><li>Warmup</li><li>Linear scaling learning rate</li><li>Label-smoothing</li><li>Random image cropping and patching</li><li>Knowledge Distillation</li><li>Cutout</li><li>Random erasing</li><li>Cosine learning rate decay</li><li>Mixup training</li><li>AdaBoud</li><li>AutoAugment</li><li>其他经典的tricks</li></ul><h2 id="Warmup"><a href="#Warmup" class="headerlink" title="Warmup"></a>Warmup</h2><p>学习率是神经网络训练中最重要的超参数之一，针对学习率的技巧有很多。Warm up是在ResNet论文[1]中提到的一种学习率预热的方法。由于刚开始训练时模型的权重(weights)是随机初始化的(全部置为0是一个坑，原因见[2])，此时选择一个较大的学习率，可能会带来模型的不稳定。学习率预热就是在刚开始训练的时候先使用一个较小的学习率，训练一些epoches或iterations，等模型稳定时再修改为预先设置的学习率进行训练。论文[1]中使用一个110层的ResNet在cifar10上训练时，先用0.01的学习率训练直到训练误差低于80%(大概训练了400个iterations)，然后使用0.1的学习率进行训练。</p><p>上述的方法是constant warmup，18年Facebook又针对上面的warmup进行了改进[3]，因为从一个很小的学习率一下变为比较大的学习率可能会导致训练误差突然增大。论文[3]提出了gradual warmup来解决这个问题，即从最开始的小学习率开始，每个iteration增大一点，直到最初设置的比较大的学习率。</p><h2 id="Linear-scaling-learning-rate"><a href="#Linear-scaling-learning-rate" class="headerlink" title="Linear scaling learning rate"></a>Linear scaling learning rate</h2><p>Linear scaling learning rate是在论文[3]中针对比较大的batch size而提出的一种方法。</p><p>在凸优化问题中，随着批量的增加，收敛速度会降低，神经网络也有类似的实证结果。随着batch size的增大，处理相同数据量的速度会越来越快，但是达到相同精度所需要的epoch数量越来越多。也就是说，使用相同的epoch时，大batch size训练的模型与小batch size训练的模型相比，验证准确率会减小。</p><p>上面提到的gradual warmup是解决此问题的方法之一。另外，linear scaling learning rate也是一种有效的方法。在mini-batch SGD训练时，梯度下降的值是随机的，因为每一个batch的数据是随机选择的。增大batch size不会改变梯度的期望，但是会降低它的方差。也就是说，大batch size会降低梯度中的噪声，所以我们可以增大学习率来加快收敛。</p><p>具体做法很简单，比如ResNet原论文[1]中，batch size为256时选择的学习率是0.1，当我们把batch size变为一个较大的数b时，学习率应该变为 0.1 × b/256。</p><h2 id="Label-smoothing"><a href="#Label-smoothing" class="headerlink" title="Label-smoothing"></a>Label-smoothing</h2><p>在分类问题中，我们的最后一层一般是全连接层，然后对应标签的one-hot编码，即把对应类别的值编码为1，其他为0。这种编码方式和通过降低交叉熵损失来调整参数的方式结合起来，会有一些问题。这种方式会鼓励模型对不同类别的输出分数差异非常大，或者说，模型过分相信它的判断。但是，对于一个由多人标注的数据集，不同人标注的准则可能不同，每个人的标注也可能会有一些错误。<strong>模型对标签的过分相信会导致过拟合。</strong></p><p>标签平滑(Label-smoothing regularization,LSR)是应对该问题的有效方法之一，它的具体思想是降低我们对于标签的信任，例如我们可以将损失的目标值从1稍微降到0.9，或者将从0稍微升到0.1。标签平滑最早在inception-v2[4]中被提出，它将真实的概率改造为：</p><p><img src="https://jpccc.github.io/resource/deepLearning/640.jpeg" alt="图片"></p><p>其中，ε是一个小的常数，K是类别的数目，y是图片的真正的标签，i代表第i个类别，q_i是图片为第i类的概率。</p><p>总的来说，LSR是一种通过在标签y中加入噪声，实现对模型约束，降低模型过拟合程度的一种正则化方法。</p><p>LSR代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchimport torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSR</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, e=<span class="number">0.1</span>, reduction=<span class="string">&#x27;mean&#x27;</span></span>):</span>        </span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.log_softmax = nn.LogSoftmax(dim=<span class="number">1</span>)        </span><br><span class="line">        self.e = e        </span><br><span class="line">        self.reduction = reduction</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one_hot</span>(<span class="params">self, labels, classes, value=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;            </span></span><br><span class="line"><span class="string">        Convert labels to one hot vectors</span></span><br><span class="line"><span class="string">        Args:            </span></span><br><span class="line"><span class="string">        labels: torch tensor in format [label1, label2, label3, ...]            </span></span><br><span class="line"><span class="string">        classes: int, number of classes            </span></span><br><span class="line"><span class="string">        value: label value in one hot vector, default to 1</span></span><br><span class="line"><span class="string">        Returns: return one hot format labels in shape [batchsize, classes]        </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        one_hot = torch.zeros(labels.size(<span class="number">0</span>), classes)</span><br><span class="line">        <span class="comment">#labels and value_added  size must match        </span></span><br><span class="line">        labels = labels.view(labels.size(<span class="number">0</span>), -<span class="number">1</span>)        </span><br><span class="line">        value_added = torch.Tensor(labels.size(<span class="number">0</span>), <span class="number">1</span>).fill_(value)</span><br><span class="line">        value_added = value_added.to(labels.device)        </span><br><span class="line">        one_hot = one_hot.to(labels.device)</span><br><span class="line">        one_hot.scatter_add_(<span class="number">1</span>, labels, value_added)</span><br><span class="line"><span class="keyword">return</span> one_hot</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_smooth_label</span>(<span class="params">self, target, length, smooth_factor</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        convert targets to one-hot format, and smooth them.       </span></span><br><span class="line"><span class="string">        Args:            </span></span><br><span class="line"><span class="string">        target: target in form with [label1, label2, label_batchsize]            </span></span><br><span class="line"><span class="string">        length: length of one-hot format(number of classes)            </span></span><br><span class="line"><span class="string">        smooth_factor: smooth factor for label smooth</span></span><br><span class="line"><span class="string">        Returns:smoothed labels in one hot format        </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span>        </span><br><span class="line">        one_hot = self._one_hot(target, length, value=<span class="number">1</span>- smooth_factor)        </span><br><span class="line">        one_hot += smooth_factor / length</span><br><span class="line">        <span class="keyword">return</span> one_hot.to(target.device)</span><br></pre></td></tr></table></figure><h2 id="Random-image-cropping-and-patching"><a href="#Random-image-cropping-and-patching" class="headerlink" title="Random image cropping and patching"></a>Random image cropping and patching</h2><p>Random image cropping and patching (RICAP)[7]方法随机裁剪四个图片中的部分，然后把它们拼接为一个图片，同时混合这四个图片的标签。</p><p>RICAP在caifar10上达到了2.19%的错误率。</p><p><img src="https://jpccc.github.io/resource/deepLearning/640.jpeg" alt="图片"></p><p>如下图所示，Ix, Iy是原始图片的宽和高。w和h称为boundary position，它决定了四个裁剪得到的小图片的尺寸。w和h从beta分布Beta(β, β)中随机生成，β也是RICAP的超参数。最终拼接的图片尺寸和原图片尺寸保持一致。</p><p><img src="https://jpccc.github.io/resource/deepLearning/640-16493198579212.jpeg" alt="图片"></p><p><img src="https://jpccc.github.io/resource/deepLearning/640-16493198617344.jpeg" alt="图片"></p><h2 id="Knowledge-Distillation"><a href="#Knowledge-Distillation" class="headerlink" title="Knowledge Distillation"></a>Knowledge Distillation</h2><p>提高几乎所有机器学习算法性能的一种非常简单的方法是在相同的数据上训练许多不同的模型，然后对它们的预测进行平均。但是使用所有的模型集成进行预测是比较麻烦的，并且可能计算量太大而无法部署到大量用户。Knowledge Distillation(知识蒸馏)[8]方法就是应对这种问题的有效方法之一。</p><p>在知识蒸馏方法中，我们使用一个教师模型来帮助当前的模型（学生模型）训练。教师模型是一个较高准确率的预训练模型，因此学生模型可以在保持模型复杂度不变的情况下提升准确率。比如，可以使用ResNet-152作为教师模型来帮助学生模型ResNet-50训练。在训练过程中，我们会加一个蒸馏损失来惩罚学生模型和教师模型的输出之间的差异。</p><p>给定输入，假定p是真正的概率分布，z和r分别是学生模型和教师模型最后一个全连接层的输出。之前我们会用交叉熵损失l(p,softmax(z))来度量p和z之间的差异，这里的蒸馏损失同样用交叉熵。所以，使用知识蒸馏方法总的损失函数是</p><p><img src="https://jpccc.github.io/resource/deepLearning/640-16493202572062.jpeg" alt="图片"></p><p>上式中，第一项还是原来的损失函数，第二项是添加的用来惩罚学生模型和教师模型输出差异的蒸馏损失。其中，T是一个温度超参数，用来使softmax的输出更加平滑的。实验证明，用ResNet-152作为教师模型来训练ResNet-50，可以提高后者的准确率。</p><p>##Cutout</p><p>Cutout[9]是一种新的正则化方法。原理是在训练时随机把图片的一部分减掉，这样能提高模型的鲁棒性。它的来源是计算机视觉任务中经常遇到的物体遮挡问题。通过cutout生成一些类似被遮挡的物体，不仅可以让模型在遇到遮挡问题时表现更好，还能让模型在做决定时更多地考虑环境(context)。</p><p>效果如下图，每个图片的一小部分被cutout了。</p><p><img src="https://jpccc.github.io/resource/deepLearning/640-16493198658876.jpeg" alt="图片"></p><h2 id="Random-erasing"><a href="#Random-erasing" class="headerlink" title="Random erasing"></a>Random erasing</h2><p>Random erasing[6]其实和cutout非常类似，也是一种模拟物体遮挡情况的数据增强方法。区别在于，cutout是把图片中随机抽中的矩形区域的像素值置为0，相当于裁剪掉，random erasing是用随机数或者数据集中像素的平均值替换原来的像素值。而且，cutout每次裁剪掉的区域大小是固定的，Random erasing替换掉的区域大小是随机的。</p><p><img src="https://jpccc.github.io/resource/deepLearning/640-16493198686898.jpeg" alt="图片"></p><p><img src="https://jpccc.github.io/resource/deepLearning/640-16493202724694.jpeg" alt="图片"></p><h2 id="Cosine-learning-rate-decay"><a href="#Cosine-learning-rate-decay" class="headerlink" title="Cosine learning rate decay"></a>Cosine learning rate decay</h2><p>在warmup之后的训练过程中，学习率不断衰减是一个提高精度的好方法。其中有step decay和cosine decay等，前者是随着epoch增大学习率不断减去一个小的数，后者是让学习率随着训练过程曲线下降。</p><p>对于cosine decay，假设总共有T个batch（不考虑warmup阶段），在第t个batch时，学习率η_t为：</p><p><img src="https://jpccc.github.io/resource/deepLearning/640-164931987398310.jpeg" alt="图片"></p><p>这里，η代表初始设置的学习率。这种学习率递减的方式称之为cosine decay。</p><p>下面是带有warmup的学习率衰减的可视化图[4]。其中，图(a)是学习率随epoch增大而下降的图，可以看出cosine decay比step decay更加平滑一点。图(b)是准确率随epoch的变化图，两者最终的准确率没有太大差别，不过cosine decay的学习过程更加平滑。</p><p><img src="https://jpccc.github.io/resource/deepLearning/640-164931987712312.jpeg" alt="图片"></p><p>在pytorch的torch.optim.lr_scheduler中有更多的学习率衰减的方法，至于哪个效果好，可能对于不同问题答案是不一样的。</p><h2 id="Mixup-training"><a href="#Mixup-training" class="headerlink" title="Mixup training"></a>Mixup training</h2><p>Mixup[10]是一种新的数据增强的方法。Mixup training，就是每次取出2张图片，然后将它们线性组合，得到新的图片，以此来作为新的训练样本，进行网络的训练，如下公式，其中x代表图像数据，y代表标签，则得到的新的xhat, yhat。</p><p><img src="https://jpccc.github.io/resource/deepLearning/640-16493202939116.jpeg" alt="图片"></p><p>其中，λ是从Beta(α, α)随机采样的数，在[0,1]之间。在训练过程中，仅使用(xhat, yhat)。</p><p>Mixup方法主要增强了训练样本之间的线性表达，增强网络的泛化能力，不过mixup方法需要较长的时间才能收敛得比较好。</p><h2 id="AdaBound"><a href="#AdaBound" class="headerlink" title="AdaBound"></a><strong>AdaBound</strong></h2><p>AdaBound是最近一篇论文[5]中提到的，按照作者的说法，AdaBound会让你的训练过程像adam一样快，并且像SGD一样好。</p><p>如下图所示，使用AdaBound会收敛速度更快，过程更平滑，结果更好。</p><p><img src="https://jpccc.github.io/resource/deepLearning/640-164931988094314.jpeg" alt="图片"></p><p>另外，这种方法相对于SGD对超参数的变化不是那么敏感，也就是说鲁棒性更好。但是，针对不同的问题还是需要调节超参数的，只是所用的时间可能变少了。</p><p><img src="https://jpccc.github.io/resource/deepLearning/640-164931988818716.jpeg" alt="图片"></p><p>当然，AdaBound还没有经过普遍的检验，也有可能只是对于某些问题效果好。</p><p>使用方法如下：安装AdaBound</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install adabound</span><br></pre></td></tr></table></figure><p>使用AdaBound(和其他PyTorch optimizers用法一致)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=0.1)</span><br></pre></td></tr></table></figure><h2 id="AutoAugment"><a href="#AutoAugment" class="headerlink" title="AutoAugment"></a>AutoAugment</h2><p>数据增强在图像分类问题上有很重要的作用，但是增强的方法有很多，并非一股脑地用上所有的方法就是最好的。那么，如何选择最佳的数据增强方法呢？AutoAugment[11]就是一种搜索适合当前问题的数据增强方法的方法。该方法创建一个数据增强策略的搜索空间，利用搜索算法选取适合特定数据集的数据增强策略。此外，从一个数据集中学到的策略能够很好地迁移到其它相似的数据集上。</p><p>AutoAugment在cifar10上的表现如下表，达到了98.52%的准确率。</p><p><img src="https://jpccc.github.io/resource/deepLearning/640-164931989171518.jpeg" alt="图片"></p><h2 id="其他经典的tricks"><a href="#其他经典的tricks" class="headerlink" title="其他经典的tricks"></a>其他经典的tricks</h2><h3 id="常用的正则化方法为"><a href="#常用的正则化方法为" class="headerlink" title="常用的正则化方法为"></a><strong>常用的正则化方法为</strong></h3><ul><li>Dropout</li><li>L1/L2正则</li><li>Batch Normalization</li><li>Early stopping</li><li>Random cropping</li><li>Mirroring</li><li>Rotation</li><li>Color shifting</li><li>PCA color augmentation</li><li>…</li></ul><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a><strong>其他</strong></h3><ul><li>Xavier init[12]</li><li>…</li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a><strong>参考资料</strong></h2><p>[1] Deep Residual Learning for Image Recognition(<a href="https://arxiv.org/pdf/1512.03385.pdf">https://arxiv.org/pdf/1512.03385.pdf</a></p><p>[2] <a href="http://cs231n.github.io/neural-networks-2/">http://cs231n.github.io/neural-networks-2/</a></p><p>[3] Accurate, Large Minibatch SGD:Training ImageNet in 1 Hour <a href="https://arxiv.org/pdf/1706.02677v2.pdf">https://arxiv.org/pdf/1706.02677v2.pdf</a></p><p>[4] <a href="https://mp.weixin.qq.com/s/Xnlp7JGx5O_I6ZmjuQ1UGQ">深度神经网络模型训练中的 tricks（原理与代码汇总） (qq.com)</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;深度神经网络模型训练中的-tricks（原理与代码汇总）&quot;&gt;&lt;a href=&quot;#深度神经网络模型训练中的-tricks（原理与代码汇总）&quot; class=&quot;headerlink&quot; title=&quot;深度神经网络模型训练中的 tricks（原理与代码汇总）&quot;&gt;&lt;/a&gt;深度神经网络模型训练中的 tricks（原理与代码汇总）&lt;/h1&gt;&lt;p&gt;本文总结了多种&lt;strong&gt;图像分类&lt;/strong&gt;任务中的重要技巧，对于&lt;strong&gt;目标检测和图像分割&lt;/strong&gt;等任务，也起到了不错的作用。&lt;/p&gt;
&lt;p&gt;计算机视觉主要问题有图像分类、目标检测和图像分割等。针对图像分类任务，提升准确率的方法路线有两条，一个是模型的修改，另一个是各种数据处理和训练的技巧(tricks)。图像分类中的各种技巧对于目标检测、图像分割等任务也有很好的作用，因此值得好好总结。本文在精读论文的基础上，总结了图像分类任务的各种tricks如下：&lt;/p&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="https://jpccc.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Understanding Graphs, Automatic Differentiation and Autograd</title>
    <link href="https://jpccc.github.io/2022/04/06/pytorch%E8%AE%A1%E7%AE%97%E5%9B%BE/"/>
    <id>https://jpccc.github.io/2022/04/06/pytorch%E8%AE%A1%E7%AE%97%E5%9B%BE/</id>
    <published>2022-04-05T16:08:00.000Z</published>
    <updated>2022-04-06T01:32:53.828Z</updated>
    
    <content type="html"><![CDATA[<div align=center><p><img src="https://jpccc.github.io/resource/pytorch/full_graph.png" alt="img"></p></div><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>PyTorch is one of the foremost python deep learning libraries out there. It’s the go to choice for deep learning research, and as each days passes by, more and more companies and research labs are adopting this library.</p><p>In this series of tutorials, we will be introducing you to PyTorch, and how to make the best use of the libraries as well the ecosystem of tools built around it. We’ll first cover the basic building blocks, and then move onto how you can quickly prototype custom architectures. We will finally conclude with a couple of posts on how to scale your code, and how to debug your code if things go awry.</p><p>This is Part 1 of our PyTorch 101 series.</p><span id="more"></span><ol><li><a href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/">Understanding Graphs, Automatic Differentiation and Autograd</a></li><li><a href="https://blog.paperspace.com/pytorch-101-building-neural-networks/">Building Your First Neural Network</a></li><li><a href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/blog.paperspace.com/pytorch-101-advanced/">Going Deep with PyTorch</a></li><li><a href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/blog.paperspace.com/pytorch-memory-multi-gpu-debugging/">Memory Management and Using Multiple GPUs</a></li><li><a href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/">Understanding Hooks</a></li></ol><p>You can get all the code in this post, (and other posts as well) in the Github repo <a href="https://github.com/Paperspace/PyTorch-101-Tutorial-Series">here</a>.</p><hr><h2 id="Prerequisites"><a href="#Prerequisites" class="headerlink" title="Prerequisites"></a>Prerequisites</h2><ol><li>Chain rule</li><li>Basic Understanding of Deep Learning</li><li>PyTorch 1.0</li></ol><hr><p>You can get all the code in this post, (and other posts as well) in the Github repo <a href="https://github.com/Paperspace/PyTorch-101-Tutorial-Series">here</a>.</p><h2 id="Automatic-Differentiation"><a href="#Automatic-Differentiation" class="headerlink" title="Automatic Differentiation"></a><strong>Automatic</strong> Differentiation</h2><p>A lot of tutorial series on PyTorch would start begin with a rudimentary discussion of what the basic structures are. However, I’d like to instead start by discussing automatic differentiation first.</p><p>Automatic Differentiation is a building block of not only PyTorch, but every DL library out there. In my opinion, PyTorch’s automatic differentiation engine, called <em>Autograd</em> is a brilliant tool to understand how automatic differentiation works. This will not only help you understand PyTorch better, but also other DL libraries.</p><p>Modern neural network architectures can have millions of learnable parameters. From a computational point of view, training a neural network consists of two phases:</p><ol><li>A forward pass to compute the value of the loss function.</li><li>A backward pass to compute the gradients of the learnable parameters.</li></ol><p>The forward pass is pretty straight forward. The output of one layer is the input to the next and so forth.</p><p>Backward pass is a bit more complicated since it requires us to use the chain rule to compute the gradients of weights w.r.t to the loss function.</p><h2 id="A-toy-example"><a href="#A-toy-example" class="headerlink" title="A toy example"></a>A toy example</h2><p>Let us take an very simple neural network consisting of just 5 neurons. Our neural network looks like the following.</p><div align=center><p><img src="https://jpccc.github.io//resource/pytorch/full_graph-16491739555532.png" alt="img"></p></div><p>The following equations describe our neural network.</p><p>$$<br>b=w1∗a\\<br>c=w2∗a\\<br>d=w3∗b+w4∗c\\<br>L=10−d\\<br>$$</p><p>Let us compute the gradients for each of the learnable parameters ww.</p><p>$$<br>\frac{\partial{L}}{\partial{w_4}} = \frac{\partial{L}}{\partial{d}}\frac{\partial{d}}{\partial{w_4}} \\<br>\frac{\partial{L}}{\partial{w_3}} = \frac{\partial{L}}{\partial{d}}\frac{\partial{d}}{\partial{w_3}}\\<br>\frac{\partial{L}}{\partial{w_2}} = \frac{\partial{L}}{\partial{d}} * \frac{\partial{d}}{\partial{c}} * \frac{\partial{c}}{\partial{w_2}}\\<br>\frac{\partial{L}}{\partial{w_1}} = \frac{\partial{L}}{\partial{d}}* \frac{\partial{d}}{\partial{b}} * \frac{\partial{b}}{\partial{w_1}}\\<br>$$</p><p>All these gradients have been computed by applying the chain rule. Note that all the individual gradients on the right hand side of the equations mentioned above can be computed directly since the <em>numerators</em> of the gradients are explicit functions of the <em>denominators.</em></p><hr><h2 id="Computation-Graphs"><a href="#Computation-Graphs" class="headerlink" title="Computation Graphs"></a>Computation Graphs</h2><p>We could manually compute the gradients of our network as it was very simple. Imagine, what if you had a network with 152 layers. Or, if the network had multiple branches.</p><p>When we design software to implement neural networks, we want to come up with a way that can allow us to seamlessly compute the gradients, regardless of the architecture type so that the programmer doesn’t have to manually compute gradients when changes are made to the network.</p><p>We galvanize(激励,启发) this idea in form of a data structure called a <strong>Computation graph</strong>. A computation graph looks very similar to the diagram of the graph that we made in the image above. However, the nodes in a computation graph are basically <strong>operators</strong>. These operators are basically the mathematical operators except for one case, where we need to represent creation of a user-defined variable.</p><p>Notice that we have also denoted(表示…) the leaf variables <strong>a,w1,w2,w3,w4</strong> in the graph for sake of clarity. However, it should noted that they are not a part of the computation graph. What they represent in our graph is the special case for user-defined variables which we just covered as an exception.</p><div align=center><p><img src="https://jpccc.github.io/resource/pytorch/computation_graph.png" alt="img"></p></div><p>The variables, <em>b,c</em> and <em>d</em> are created as a result of mathematical operations, whereas variables <em>a, w1, w2, w3</em> and <em>w4</em> are initialised by the user itself. Since, they are not created by any mathematical operator, nodes corresponding to their creation is represented by their name itself. This is true for all the <em>leaf</em> nodes in the graph.</p><hr><h2 id="Computing-the-gradients"><a href="#Computing-the-gradients" class="headerlink" title="Computing the gradients"></a>Computing the gradients</h2><p>Now, we are ready to describe how we will compute gradients using a computation graph.</p><p>Each node of the computation graph, with the exception of(除了…外) leaf nodes, can be considered as a function which takes some inputs and produces an output. Consider the node of the graph which produces variable <em>d</em> from w4cand w3b. Therefore we can write,</p><p>$$<br>d=f(w_3b,w_4c)<br>$$</p><div align=center><p><img src="https://jpccc.github.io/resource/pytorch/d_mini.png" alt="img"></p></div><p>​                                                                                            <center>    d is output of function f(x,y) = x + y  </center></p><p>Now, we can easily compute the gradient of the ff with respect to it’s inputs, $\frac{\partial{f}}{\partial{w_3b}}$ and $\frac{\partial{f}}{\partial{w_4b}}$ (which are both 1). Now, label the edges coming into the nodes with their respective gradients like the following image.</p><div align=center><p><img src="https://jpccc.github.io/resource/pytorch/d_mini_grad.png" alt="img"></p></div><p>​                                                                                                            <center>    Local Gradients </center></p><p>We do it for the entire graph. The graph looks like this.</p><div align=center><p><img src="https://jpccc.github.io/resource/pytorch/full_graph-16491739555532.png" alt="img"></p></div><center>Back propagation in an Computational Graph</center><p>Following we describe the algorithm for computing derivative(微分) of any node in this graph with respect to the loss, LL. Let’s say we want to compute the derivative, $\frac{\partial{f}}{\partial{w_4}}$</p><ol><li>We first trace down all possible paths from <em>d</em> to $w_4$.</li><li>There is only one such path.</li><li>We multiply all the edges long this path.</li></ol><p>If you see, the product is precisely the same expression we derived using chain rule. If there is more than one path to a variable from <em>L</em> then, we multiply the edges along each path and then add them together. For example,$\frac{\partial{L}}{\partial{a}}$  is computed as</p><p>$$<br>\frac{\partial{L}}{\partial{w_4}} = \frac{\partial{L}}{\partial{d}}*\frac{\partial{d}}{\partial{b}}*\frac{\partial{b}}{\partial{a}} + \frac{\partial{L}}{\partial{d}}*\frac{\partial{d}}{\partial{c}}*\frac{\partial{c}}{\partial{a}}<br>$$</p><h2 id="PyTorch-Autograd"><a href="#PyTorch-Autograd" class="headerlink" title="PyTorch Autograd"></a>PyTorch Autograd</h2><p>Now we get what a computational graph is, let’s get back to PyTorch and understand how the above is implemented in PyTorch.</p><p><em><strong>(注意：对谁求导，对应的导数就保存在对应变量中)</strong></em></p><h3 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h3><p><code>Tensor</code> is a data structure which is a fundamental building block of PyTorch. <code>Tensor</code>s are pretty much like numpy arrays, except that unlike numpy, tensors are designed to take advantage of parallel computation capabilities of a GPU. A lot of Tensor syntax(语法) is similar to that of numpy arrays.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]:  <span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: tsr = torch.Tensor(<span class="number">3</span>,<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: tsr</span><br><span class="line">Out[<span class="number">3</span>]: </span><br><span class="line">tensor([[ <span class="number">0.0000e+00</span>,  <span class="number">0.0000e+00</span>,  <span class="number">8.4452e-29</span>, -<span class="number">1.0842e-19</span>,  <span class="number">1.2413e-35</span>],</span><br><span class="line">        [ <span class="number">1.4013e-45</span>,  <span class="number">1.2416e-35</span>,  <span class="number">1.4013e-45</span>,  <span class="number">2.3331e-35</span>,  <span class="number">1.4013e-45</span>],</span><br><span class="line">        [ <span class="number">1.0108e-36</span>,  <span class="number">1.4013e-45</span>,  <span class="number">8.3641e-37</span>,  <span class="number">1.4013e-45</span>,  <span class="number">1.0040e-36</span>]])</span><br></pre></td></tr></table></figure><p>One it’s own, <code>Tensor</code> is just like a numpy <code>ndarray</code>. A data structure that can let you do fast linear algebra options. If you want PyTorch to create a graph corresponding to these operations, you will have to set the <code>requires_grad</code> attribute of the <code>Tensor</code> to True.</p><p>The API can be a bit confusing here. There are multiple ways to initialise tensors in PyTorch. While some ways can let you explicitly define that the <code>requires_grad</code> in the constructor itself, others require you to set it manually after creation of the Tensor.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; t1 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>) </span><br><span class="line"></span><br><span class="line">&gt;&gt; t2 = torch.FloatTensor(<span class="number">3</span>,<span class="number">3</span>) <span class="comment"># No way to specify requires_grad while initiating </span></span><br><span class="line">&gt;&gt; t2.requires_grad = <span class="literal">True</span></span><br></pre></td></tr></table></figure><p><code>requires_grad</code> is contagious. It means that when a <code>Tensor</code> is created by operating on other <code>Tensor</code>s, the <code>requires_grad</code> of the resultant <code>Tensor</code> would be set <code>True</code> given at least one of the tensors used for creation has it’s <code>requires_grad</code> set to <code>True</code>.</p><p>Each <code>Tensor</code> has a something an attribute called <code>grad_fn</code><em>,</em> which refers to the <strong>mathematical operator that create the variable</strong>. If <code>requires_grad</code> is set to False, <code>grad_fn</code> would be None.</p><p>In our example where, $d=f(w_3b,w_4c)$, <em>d</em>‘s grad function would be the addition operator, since <em>f</em> adds it’s to input together. Notice, addition operator is also the node in our graph that output’s <em>d</em>. If our <code>Tensor</code> is a leaf node (initialised by the user), then the <code>grad_fn</code> is also None.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line">a = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">w1 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w2 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w3 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w4 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">b = w1*a </span><br><span class="line">c = w2*a</span><br><span class="line"></span><br><span class="line">d = w3*b + w4*c </span><br><span class="line"></span><br><span class="line">L = <span class="number">10</span> - d</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The grad fn for a is&quot;</span>, a.grad_fn)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The grad fn for d is&quot;</span>, d.grad_fn)</span><br></pre></td></tr></table></figure><p>If you run the code above, you get the following output.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">The grad fn <span class="keyword">for</span> a <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">The grad fn <span class="keyword">for</span> d <span class="keyword">is</span> &lt;AddBackward0 <span class="built_in">object</span> at <span class="number">0x1033afe48</span>&gt;</span><br></pre></td></tr></table></figure><p>One can use the member function <code>is_leaf</code> to determine whether a variable is a leaf <code>Tensor</code> or not.</p><h3 id="Function"><a href="#Function" class="headerlink" title="Function"></a>Function</h3><p>All mathematical operations in PyTorch are implemented by the <em>torch.nn.Autograd.Function</em> class. This class has two important member functions we need to look at.</p><ul><li>The first is it’s <em>forward</em>  function, which simply computes the output using it’s inputs.</li></ul><ul><li>The <code>backward</code> function takes the incoming gradient coming from the the part of the network in front of it. As you can see, the gradient to be backpropagated from a function <em>f</em> is basically the <strong>gradient that is backpropagated to f from the layers in front of it</strong> multiplied by <strong>the local gradient of the output of f with respect to it’s inputs</strong>（链式规则）. This is exactly what the <code>backward</code> function does.</li></ul><p>Let’s again understand with our example of<br>$$<br>d=f(w_3b,w_4c)<br>$$</p><ol><li><em>d</em> is our <code>Tensor</code> here. It’s <code>grad_fn</code> is <code>&lt;ThAddBackward&gt;</code><em>.</em> This is basically the addition operation since the function that creates <em>d</em> adds inputs.</li><li>The <code>forward</code> function of the it’s <code>grad_fn</code> receives the inputs $w_3b$ <em>and</em> $w_4c$ and adds them. This value is basically stored in the <em>d</em>.</li><li>The <code>backward</code> function of the <code>&lt;ThAddBackward&gt;</code> basically takes the the <strong>incoming gradient</strong> from the further layers as the input. This is basically $\frac{\partial{L}}{\partial{d}}$ coming along the edge leading from <em>L</em> to <em>d.</em> This gradient is also the gradient of <em>L</em> w.r.t to <em>d</em> and is stored in <code>grad</code> attribute of the <code>d</code>. It can be accessed by calling <code>d.grad</code><em>.</em></li><li>It then takes computes the local gradients $\frac{\partial{d}}{\partial{w_4c}}$and$\frac{\partial{d}}{\partial{w_3b}}$.</li><li>Then the backward function multiplies the incoming gradient with the <strong>locally computed gradients</strong> respectively and <em><strong>“<em><strong>sends</strong></em>“</strong></em> the gradients to it’s inputs by invoking the backward method of the <code>grad_fn</code> of their inputs.</li><li>For example, the <code>backward</code> function of <code>&lt;ThAddBackward&gt;</code> associated with <em>d</em> invokes(援引，调用) backward function of the <em>grad_fn</em> of the $w_4∗c$∗(Here, $w_4∗c$ is a intermediate Tensor, and it’s <em>grad_fn</em> is <code>&lt;ThMulBackward&gt;</code>. At time of invocation of the <code>backward</code> function, the gradient $\frac{\partial{L}}{\partial{d}}*\frac{\partial{d}}{\partial{w_4c}}$ is passed as the input.</li><li>Now, for the variable $w_4∗c$, $\frac{\partial{L}}{\partial{d}}*\frac{\partial{d}}{\partial{w_4c}}$ becomes the incoming gradient, $\frac{\partial{L}}{\partial{d}}$ was for $d$ in step 3 and the process repeats.</li></ol><p>Algorithmically, here’s how back propagation happens with a computation graph. (Not the actual implementation, only representative)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def backward (incoming_gradients):</span><br><span class="line">self.Tensor.grad = incoming_gradients</span><br><span class="line"></span><br><span class="line">for inp in self.inputs:</span><br><span class="line">if inp.grad_fn is not None:</span><br><span class="line">new_incoming_gradients = //</span><br><span class="line">  incoming_gradient * local_grad(self.Tensor, inp)</span><br><span class="line"></span><br><span class="line">inp.grad_fn.backward(new_incoming_gradients)</span><br><span class="line">else:</span><br><span class="line">pass</span><br></pre></td></tr></table></figure><p>Here, <code>self.Tensor</code> is basically the <code>Tensor</code> created by Autograd.Function, which was <em>d</em> in our example.</p><p>Incoming gradients and local gradients have been described above.</p><hr><p>In order to compute derivatives in our neural network, we generally call <code>backward</code> on the <code>Tensor</code> representing our loss. Then, we backtrack through the graph starting from node representing the <code>grad_fn</code> of our loss.</p><p>As described above, the <code>backward</code> function is recursively called through out the graph as we backtrack. Once, we reach a leaf node, since the <code>grad_fn</code> is None, but stop backtracking through that path.</p><p>One thing to note here is that PyTorch gives an error if you call <code>backward()</code> on vector-valued Tensor. This means you can only call <code>backward</code> on a scalar valued Tensor. In our example, if we assume <code>a</code> to be a vector valued Tensor, and call <code>backward</code> on L, it will throw up an error.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line">a = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">w1 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w2 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w3 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w4 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">b = w1*a </span><br><span class="line">c = w2*a</span><br><span class="line"></span><br><span class="line">d = w3*b + w4*c </span><br><span class="line"></span><br><span class="line">L = (<span class="number">10</span> - d)</span><br><span class="line"></span><br><span class="line">L.backward()</span><br></pre></td></tr></table></figure><p>Running the above snippet results in the following error.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RuntimeError: grad can be implicitly created only <span class="keyword">for</span> scalar outputs</span><br></pre></td></tr></table></figure><p>This is because gradients can be computed with respect to scalar values by definition. You can’t exactly differentiate a vector with respect to another vector. The mathematical entity used for such cases is called a <strong>Jacobian,</strong> the discussion of which is beyond the scope of this article.</p><p>There are two ways to overcome this.</p><p>If you just make a small change in the above code setting <code>L</code> to be the sum of all the errors, our problem will be solved.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line">a = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">w1 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w2 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w3 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w4 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">b = w1*a </span><br><span class="line">c = w2*a</span><br><span class="line"></span><br><span class="line">d = w3*b + w4*c </span><br><span class="line"></span><br><span class="line"><span class="comment"># Replace L = (10 - d) by </span></span><br><span class="line">L = (<span class="number">10</span> -d).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">L.backward()</span><br></pre></td></tr></table></figure><p>Once that’s done, you can access the gradients by calling the <code>grad</code> attribute of <code>Tensor</code>.</p><p><strong>Second way is</strong>, for some reason have to absolutely call <code>backward</code> on a vector function, you can pass a <code>torch.ones</code> of size of shape of the tensor you are trying to call backward with.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Replace L.backward() with </span></span><br><span class="line">L.backward(torch.ones(L.shape))</span><br></pre></td></tr></table></figure><p>Notice how <code>backward</code> used to take incoming gradients as it’s input. Doing the above makes the <code>backward</code> think that incoming gradient are just Tensor of ones of same size as L, and it’s able to back propagate.</p><p>In this way, we can have gradients for every <code>Tensor</code> , and we can update them using Optimisation algorithm of our choice.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w1 = w1 - learning_rate * w1.grad</span><br></pre></td></tr></table></figure><p>And so on.</p><h2 id="How-are-PyTorch’s-graphs-different-from-TensorFlow-graphs"><a href="#How-are-PyTorch’s-graphs-different-from-TensorFlow-graphs" class="headerlink" title="How are PyTorch’s graphs different from TensorFlow graphs"></a>How are PyTorch’s graphs different from TensorFlow graphs</h2><p>PyTorch creates something called a <strong>Dynamic Computation Graph,</strong> which means that the graph is generated on the fly.</p><p>Until the <code>forward</code> function of a Variable is called, there exists no node for the <code>Tensor</code> <em>(<em>it’s <code>grad_fn</code></em>)</em> in the graph.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)   <span class="comment">#No graph yet, as a is a leaf</span></span><br><span class="line"></span><br><span class="line">w1 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)  <span class="comment">#Same logic as above</span></span><br><span class="line"></span><br><span class="line">b = w1*a   <span class="comment">#Graph with node `mulBackward` is created.</span></span><br></pre></td></tr></table></figure><p>The graph is created as a result of <code>forward</code> function of many <em>Tensors</em> being invoked. Only then, the buffers for the non-leaf nodes allocated for the graph and intermediate values (used for computing gradients later.  When you call <code>backward</code>, as the gradients are computed, these buffers (for non-leaf variables) are essentially freed, and the graph is <em>destroyed</em> ( In a sense, you can’t backpropagate through it since the buffers holding values to compute the gradients are gone).</p><p>Next time, you will call <code>forward</code> on the same set of tensors, <strong>the leaf node buffers from the previous run will be shared, while the non-leaf nodes buffers will be created again.</strong></p><p>If you call <code>backward</code> more than once on a graph with non-leaf nodes, you’ll be met with the following error.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=<span class="literal">True</span> when calling backward the first time.</span><br></pre></td></tr></table></figure><p>This is because the non-leaf buffers gets destroyed the first time <code>backward()</code> is called and hence, there’s no path to navigate to the leaves when <code>backward</code> is invoked the second time. You can undo this non-leaf buffer destroying behaviour by adding <code>retain_graph = True</code> argument to the <code>backward</code> function.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss.backward(retain_graph = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>If you do the above, you will be able to backpropagate again through the same graph and the gradients will be accumulated, i.e. the next you backpropagate, the gradients will be added to those already stored in the previous back pass.</p><hr><p>This is in contrast to the <em><strong>Static Computation Graphs</strong></em>, used by TensorFlow where the graph is declared <strong>before</strong> running the program. Then the graph is “run” by feeding values to the predefined graph.</p><p>The dynamic graph paradigm allows you to make changes to your network architecture <em>during</em> runtime, as a graph is created only when a piece of code is run.</p><p>This means a graph may be redefined during the lifetime for a program since you don’t have to define it beforehand.</p><p>This, however, is not possible with static graphs where graphs are created before running the program, and merely executed later.</p><p>Dynamic graphs also make debugging way easier since it’s easier to locate the source of your error.</p><h2 id="Some-Tricks-of-Trade"><a href="#Some-Tricks-of-Trade" class="headerlink" title="Some Tricks of Trade"></a>Some Tricks of Trade</h2><h3 id="requires-grad"><a href="#requires-grad" class="headerlink" title="requires_grad"></a>requires_grad</h3><p>This is an attribute of the <code>Tensor</code> class. By default, it’s False. It comes handy when you have to freeze some layers, and stop them from updating parameters while training. You can simply set the <code>requires_grad</code> to False, and these <code>Tensors</code> won’t participate in the computation graph.</p><div align=center><p><img src="https://jpccc.github.io/resource/pytorch/image-4.png" alt="img"></p></div><p>Thus, no gradient would be propagated to them, or to those layers which depend upon these layers for gradient flow <code>requires_grad</code>. When set to True, <code>requires_grad</code> is contagious meaning even if one operand of an operation has <code>requires_grad</code> set to True, so will the result.</p><h3 id="torch-no-grad"><a href="#torch-no-grad" class="headerlink" title="torch.no_grad()"></a>torch.no_grad()</h3><p>When we are computing gradients, we need to cache input values, and intermediate features as they maybe required to compute the gradient later.</p><p>The gradient of $ b=w_1∗a$ w.r.t it’s inputs w1w1 and aa is aa and w1w1 respectively. We need to store these values for gradient computation during the backward pass. This affects the memory footprint of the network.</p><p>While, we are performing inference, we don’t compute gradients, and thus, don’t need to store these values. Infact, no graph needs to be create during inference as it will lead to useless consumption of memory.</p><p>PyTorch offers a context manager, called <code>torch.no_grad</code> for this purpose.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad:</span><br><span class="line">inference code goes here </span><br></pre></td></tr></table></figure><p>No graph is defined for operations executed under this context manager.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Understanding how <em>Autograd</em> and computation graphs works can make life with PyTorch a whole lot easier. With our foundations rock solid, the next posts will detail how to create custom complex architectures, how to create custom data pipelines and more interesting stuff.</p><h2 id="Further-Reading"><a href="#Further-Reading" class="headerlink" title="Further Reading"></a>Further Reading</h2><ol><li><a href="https://www.khanacademy.org/math/differential-calculus/dc-chain">Chain Rule</a></li><li><a href="http://neuralnetworksanddeeplearning.com/chap2.html">Backpropagation</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;div align=center&gt;

&lt;p&gt;&lt;img src=&quot;https://jpccc.github.io/resource/pytorch/full_graph.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;PyTorch is one of the foremost python deep learning libraries out there. It’s the go to choice for deep learning research, and as each days passes by, more and more companies and research labs are adopting this library.&lt;/p&gt;
&lt;p&gt;In this series of tutorials, we will be introducing you to PyTorch, and how to make the best use of the libraries as well the ecosystem of tools built around it. We’ll first cover the basic building blocks, and then move onto how you can quickly prototype custom architectures. We will finally conclude with a couple of posts on how to scale your code, and how to debug your code if things go awry.&lt;/p&gt;
&lt;p&gt;This is Part 1 of our PyTorch 101 series.&lt;/p&gt;</summary>
    
    
    
    
    <category term="pytorch" scheme="https://jpccc.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Jensen_inequality</title>
    <link href="https://jpccc.github.io/2021/10/28/Jensen-inequality/"/>
    <id>https://jpccc.github.io/2021/10/28/Jensen-inequality/</id>
    <published>2021-10-28T08:10:44.000Z</published>
    <updated>2022-04-05T16:03:21.412Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Jensen不等式"><a href="#Jensen不等式" class="headerlink" title="Jensen不等式"></a>Jensen不等式</h1><p>Jensen不等式（Jensen’s inequality）是以丹麦数学家Johan Jensen命名的，它在概率论、机器学习、测度论、统计物理等领域都有相关应用。 在机器学习领域，我目前接触到的是用Jensen不等式用来证明KL散度大于等于0。</p><blockquote><p>Jensen不等式是和凸函数的定义是息息相关的，首先介绍什么是凸函数(convec function)。</p></blockquote><span id="more"></span><h2 id="凸函数"><a href="#凸函数" class="headerlink" title="凸函数"></a>凸函数</h2><p>凸函数是一个定义在某个向量空间的凸子集 C（区间）上的实值函数 f，如果在其定义域 C 上的任意两点 $x_1$,$x_2$ 有:</p><p>$$tf(x_1)+(1-t)f(x_2)\geq f(tx_1+(1-t)x_2)   \tag{1}$$</p><p>也就是说凸函数任意两点的割线位于函数图形上方， 这也是Jensen不等式的两点形式。</p><h2 id="Jensen不等式2134123"><a href="#Jensen不等式2134123" class="headerlink" title="Jensen不等式2134123"></a>Jensen不等式2134123</h2><p>若对于任意点集${x_i}$，若 $\lambda_i\geq 0$ 且 $\underset {i}\sum\lambda_i=1$ ，使用数学归纳法，可以证明凸函数 f (x) 满足：<br>$$f(\sum_{i=1}^M\lambda_ix_i)\leq \sum_{i=1}^M\lambda_if(x_i) \tag{2} $$</p><p>公式(2)被称为 Jensen 不等式，它是公式(1)的泛化形式。</p><blockquote><p><strong>证明如下：</strong></p><blockquote><p>当i=1或2时，由凸函数的定义成立<br>    假设当i=M时，公式(2)成立<br>    现在证明则i=M+1时，Jensen不等式也成立：<br><a href="https://blog.csdn.net/AndrewHYang/article/details/86477162">证明</a></p></blockquote></blockquote><p>在概率论中，如果把$\lambda_i$看成取值为$x_i$的离散变量x的概率分布，那么公式(2)就可以写成：</p><p>$$f(E(X))\leq E[f(x)]$$</p><p>其中, E[·] 表示期望。</p><p>对于连续变量，Jensen不等式给出了积分的凸函数值和凸函数的积分值间的关系。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Jensen不等式&quot;&gt;&lt;a href=&quot;#Jensen不等式&quot; class=&quot;headerlink&quot; title=&quot;Jensen不等式&quot;&gt;&lt;/a&gt;Jensen不等式&lt;/h1&gt;&lt;p&gt;Jensen不等式（Jensen’s inequality）是以丹麦数学家Johan Jensen命名的，它在概率论、机器学习、测度论、统计物理等领域都有相关应用。 在机器学习领域，我目前接触到的是用Jensen不等式用来证明KL散度大于等于0。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Jensen不等式是和凸函数的定义是息息相关的，首先介绍什么是凸函数(convec function)。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    
    <category term="math" scheme="https://jpccc.github.io/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>统计学习数学基础-1</title>
    <link href="https://jpccc.github.io/2021/10/28/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0part1/"/>
    <id>https://jpccc.github.io/2021/10/28/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0part1/</id>
    <published>2021-10-28T04:44:13.000Z</published>
    <updated>2022-04-13T10:58:37.055Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>对概率的诠释有两大学派，一种是频率派另一种是贝叶斯派。后面我们对观测集采用下面记号：<br>$$<br>X_{N\times p}=(x_{1},x_{2},\cdots,x_{N})^{T},x_{i}=(x_{i1},x_{i2},\cdots,x_{ip})^{T}<br>$$<br>这个记号表示有 $N$ 个样本，每个样本都是 $p$ 维向量。其中每个观测都是由 $p(x|\theta)$ 生成的。</p><span id="more"></span><h2 id="频率派的观点"><a href="#频率派的观点" class="headerlink" title="频率派的观点"></a>频率派的观点</h2><p>$p(x|\theta)$中的 $\theta$ 是一个未知常量，数据是随机变量。对于 $N$ 个观测来说观测集的概率为</p><div align='center'><img src="https://www.zhihu.com/equation?tex=p(X|\theta)\mathop{=}\limits_{iid}\prod\limits_{i=1}^{N}{p(x_{i}|\theta)}" align="center"/></div><p>为了求 $\theta$ 的大小，我们采用最大对数似然MLE的方法：</p><img src="https://www.zhihu.com/equation?tex=\theta_{MLE}=\mathop{argmax}\limits_{\theta}\log p(X|\theta)\mathop{=}\limits_{iid}\mathop{argmax}\limits_{\theta}\sum\limits _{i=1}^{N}\log p(x_{i}|\theta)"/><p>$x_i$服从独立同分布的条件，所以$P(X|\theta)=\prod_{i=0}^n p(x_i|\theta)$,为了<b>方便计算</b>，在前面加上log,将连乘变成连加。</p><h2 id="贝叶斯派的观点"><a href="#贝叶斯派的观点" class="headerlink" title="贝叶斯派的观点"></a>贝叶斯派的观点</h2><p>贝叶斯派认为 $p(x|\theta)$ 中的 $\theta$ 不是一个常量。这个 $\theta$ 满足一个预设的<strong>先验</strong>的分布 $\theta\sim p(\theta)$ （比喻可以假设为高斯分布）,并借助贝叶斯定理，用似然将参数的先验和后验连接起来，于是根据贝叶斯定理依赖观测集参数的后验可以写成：</p><p>$$<br>p(\theta|X)=\frac{p(X|\theta)\cdot p(\theta)}{p(X)}=\frac{p(X|\theta)\cdot p(\theta)}{\int\limits _{\theta}p(X|\theta)\cdot p(\theta)d\theta}<br>$$<br>为了求 $\theta$ 的值，我们要最大化这个参数后验MAP：</p><blockquote><p>最大化后验的解释：在给定观测X的情况下，找出$\theta$的概率最大时所对应的值，即这时候的$\theta$更可能为我们要找的参数。<br>$$<br>\theta_{MAP}=\mathop{argmax}\limits _{\theta}p(\theta|X)=\mathop{argmax}\limits _{\theta}p(X|\theta)\cdot p(\theta)<br>$$<br>其中第二个等号是由于分母和 $\theta$ 没有关系。求解这个 $\theta$ 值后计算$\frac{p(X|\theta)\cdot p(\theta)}{\int\limits _{\theta}p(X|\theta)\cdot p(\theta)d\theta}$ ，就得到了参数的后验概率。其中 $p(X|\theta)$ 叫似然，是我们的模型分布。得到了参数的后验分布后，我们可以将这个分布用于<strong>贝叶斯预测</strong>：</p></blockquote><p>$$<br>p(x_{new}|X)=\int\limits <em>{\theta}p(x</em>{new},\theta|X)=\int\limits <em>{\theta}p(x</em>{new}|\theta)\cdot p(\theta|X)d\theta<br>$$</p><p> 其中积分中的被乘数是模型，乘数是后验分布。</p><blockquote><p>$p(x|\theta)$是似然，$p(\theta|x)$是后验。注意第一个等式中，$\theta$不论放分子还是分母都可以这样积分掉。</p></blockquote><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>频率派和贝叶斯派分别给出了一系列的机器学习算法。频率派的观点导出了一系列的统计机器学习算法而贝叶斯派导出了概率图理论。在应用频率派的 MLE 方法时最<strong>优化</strong>理论占有重要地位。而贝叶斯派的算法无论是后验概率的建模还是应用这个后验进行推断时<strong>积分</strong>占有重要地位。因此采样积分方法如 MCMC 有很多应用。(即频率派需要设计损失函数并进行优化，而贝叶斯派需要积分后验中分母。)</p><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><blockquote><p>频率派认为参数是客观存在不会改变的，虽然未知，但却是固定值（故可用最优化方法去找那一个唯一的值）；贝叶斯派则认为参数是随机值，因为不可能做完整的实验去确定，因此参数也可以有分布。往小处说，频率派最常关心的是似然函数，他们认为直接用样本去计算出的概率就是真实的，而贝叶斯派最常关心的是后验分布，他们认为样本只是用来修正经验观点。</p></blockquote><blockquote><p>贝叶斯派因为所有的参数都是随机变量，都有分布，因此可以使用一些基于采样的方法 （如MCMC）使得我们更容易构建复杂模型。频率派的优点则是没有假设一个先验分布，因此更加客观，也更加无偏，在一些保守的领域（比如制药业、法律）比贝叶斯方法更受到信任。</p></blockquote><h1 id="MathBasics"><a href="#MathBasics" class="headerlink" title="MathBasics"></a>MathBasics</h1><h2 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a>高斯分布</h2><h3 id="一维情况-MLE"><a href="#一维情况-MLE" class="headerlink" title="一维情况 MLE"></a>一维情况 MLE</h3><p>高斯分布在机器学习中占有举足轻重的作用。在 MLE 方法中：</p><p>$$<br>\theta=(\mu,\Sigma)=(\mu,\sigma^{2}),\theta_{MLE}=\mathop{argmax}\limits _{\theta}\log p(X|\theta)\mathop{=}\limits _{iid}\mathop{argmax}\limits _{\theta}\sum\limits <em>{i=1}^{N}\log p(x</em>{i}|\theta)<br>$$</p><p>一般地，高斯分布的概率密度函数PDF写为：</p><p>$$<br>p(x|\mu,\Sigma)=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)}<br>$$</p><p>带入 MLE 中我们考虑一维的情况</p><p>$$<br>\log p(X|\theta)=\sum\limits <em>{i=1}^{N}\log p(x</em>{i}|\theta)=\sum\limits <em>{i=1}^{N}\log\frac{1}{\sqrt{2\pi}\sigma}\exp(-(x</em>{i}-\mu)^{2}/2\sigma^{2})<br>$$</p><p>首先对 $\mu$ 的极值可以得到 ：<br>$$<br>\mu_{MLE}=\mathop{argmax}\limits _{\mu}\log p(X|\theta)=\mathop{argmax}\limits _{\mu}\sum\limits <em>{i=1}^{N}(x</em>{i}-\mu)^{2}<br>$$</p><p> 于是：<br>$$<br>\frac{\partial}{\partial\mu}\sum\limits <em>{i=1}^{N}(x</em>{i}-\mu)^{2}=0\longrightarrow\mu_{MLE}=\frac{1}{N}\sum\limits <em>{i=1}^{N}x</em>{i}<br>$$</p><p>其次对 $\theta$ 中的另一个参数 $\sigma$ ，有：</p><p>$$<br>\begin{align}<br>\sigma_{MLE}=\mathop{argmax}\limits _{\sigma}\log p(X|\theta)&amp;=\mathop{argmax}\limits _{\sigma}\sum\limits <em>{i=1}^{N}[-\log\sigma-\frac{1}{2\sigma^{2}}(x</em>{i}-\mu)^{2}]\<br>&amp;=\mathop{argmin}\limits _{\sigma}\sum\limits <em>{i=1}^{N}[\log\sigma+\frac{1}{2\sigma^{2}}(x</em>{i}-\mu)^{2}]<br>\end{align}<br>$$</p><p>于是：</p><p>$$<br>\frac{\partial}{\partial\sigma}\sum\limits <em>{i=1}^{N}[\log\sigma+\frac{1}{2\sigma^{2}}(x</em>{i}-\mu)^{2}]=0\longrightarrow\sigma_{MLE}^{2}=\frac{1}{N}\sum\limits <em>{i=1}^{N}(x</em>{i}-\mu)^{2}<br>$$</p><p>值得注意的是，上面的推导中，首先对 $\mu$ 求 MLE， 然后利用这个结果求 $\sigma_{MLE}$ ，因此可以预期的是对数据集求期望时 $\mathbb{E}<em>{\mathcal{D}}[\mu</em>{MLE}]$ 是无偏差的：</p><p>$$<br>\mathbb{E}<em>{\mathcal{D}}[\mu</em>{MLE}]=\mathbb{E}<em>{\mathcal{D}}[\frac{1}{N}\sum\limits <em>{i=1}^{N}x</em>{i}]=\frac{1}{N}\sum\limits <em>{i=1}^{N}\mathbb{E}</em>{\mathcal{D}}[x</em>{i}]=\mu<br>$$</p><p>但是当对 $\sigma_{MLE}$ 求 期望的时候由于使用了单个数据集的 $\mu_{MLE}$，因此对所有数据集求期望的时候我们会发现 $\sigma_{MLE}$ 是 有偏的：</p><p>$$<br>\begin{align}<br>\mathbb{E}<em>{\mathcal{D}}[\sigma</em>{MLE}^{2}]&amp;=\mathbb{E}<em>{\mathcal{D}}[\frac{1}{N}\sum\limits <em>{i=1}^{N}(x</em>{i}-\mu</em>{MLE})^{2}]=\mathbb{E}<em>{\mathcal{D}}[\frac{1}{N}\sum\limits <em>{i=1}^{N}(x</em>{i}^{2}-2x</em>{i}\mu_{MLE}+\mu_{MLE}^{2})<br>\&amp;=\mathbb{E}<em>{\mathcal{D}}[\frac{1}{N}\sum\limits <em>{i=1}^{N}x</em>{i}^{2}-\mu</em>{MLE}^{2}]=\mathbb{E}<em>{\mathcal{D}}[\frac{1}{N}\sum\limits <em>{i=1}^{N}x</em>{i}^{2}-\mu^{2}+\mu^{2}-\mu</em>{MLE}^{2}]\<br>&amp;= \mathbb{E}<em>{\mathcal{D}}[\frac{1}{N}\sum\limits <em>{i=1}^{N}x</em>{i}^{2}-\mu^{2}]-\mathbb{E}</em>{\mathcal{D}}[\mu_{MLE}^{2}-\mu^{2}]=\sigma^{2}-(\mathbb{E}<em>{\mathcal{D}}[\mu</em>{MLE}^{2}]-\mu^{2})\&amp;=\sigma^{2}-(\mathbb{E}<em>{\mathcal{D}}[\mu</em>{MLE}^{2}]-\mathbb{E}<em>{\mathcal{D}}^{2}[\mu</em>{MLE}])=\sigma^{2}-Var[\mu_{MLE}]\&amp;=\sigma^{2}-Var[\frac{1}{N}\sum\limits <em>{i=1}^{N}x</em>{i}]=\sigma^{2}-\frac{1}{N^{2}}\sum\limits <em>{i=1}^{N}Var[x</em>{i}]=\frac{N-1}{N}\sigma^{2}<br>\end{align}<br>$$</p><p>所以：${\sigma}^{2}$的无偏估计应该为：</p><p>$$<br>\hat{\sigma}^{2}=\frac{1}{N-1}\sum\limits <em>{i=1}^{N}(x</em>{i}-\mu)^{2}<br>$$</p><h3 id="多维情况"><a href="#多维情况" class="headerlink" title="多维情况"></a>多维情况</h3><p>多维高斯分布表达式为：</p><p>$$<br>p(x|\mu,\Sigma)=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)}<br>$$</p><p>其中 $x,\mu\in\mathbb{R}^{p},\Sigma\in\mathbb{R}^{p\times p}$ ，$\Sigma$ 为协方差矩阵，一般而言也是半正定矩阵。这里我们只考虑正定矩阵。首先我们处理指数上的数字，指数上的数字可以记为 $x$ 和 $\mu$ 之间的马氏距离。对于对称的协方差矩阵可进行特征值分解，$\Sigma=U\Lambda U^{T}=(u_{1},u_{2},\cdots,u_{p})diag(\lambda_{i})(u_{1},u_{2},\cdots,u_{p})^{T}=\sum\limits <em>{i=1}^{p}u</em>{i}\lambda_{i}u_{i}^{T}$ ，于是：</p><p>$$<br>\Sigma^{-1}=\sum\limits <em>{i=1}^{p}u</em>{i}\frac{1}{\lambda_{i}}u_{i}^{T}<br>$$</p><p>$$<br>\Delta=(x-\mu)^{T}\Sigma^{-1}(x-\mu)=\sum\limits <em>{i=1}^{p}(x-\mu)^{T}u</em>{i}\frac{1}{\lambda_{i}}u_{i}^{T}(x-\mu)=\sum\limits <em>{i=1}^{p}\frac{y</em>{i}^{2}}{\lambda_{i}}<br>$$</p><p>我们注意到 $y_{i}$ 是 $x-\mu$ 在特征向量 $u_{i}$ 上的投影长度，因此上式子就是 $\Delta$ 取不同值时的同心椭圆。</p><p>下面我们看多维高斯模型在实际应用时的两个问题</p><ol><li><p> 参数 $\Sigma,\mu$ 的自由度为 $O(p^{2})$ 对于维度很高的数据其自由度太高。解决方案：高自由度的来源是 $\Sigma$ 有 $\frac{p(p+1)}{2}$ 个自由参数，可以假设其是对角矩阵，甚至在各向同性假设中假设其对角线上的元素都相同。前一种的算法有 Factor Analysis，后一种有概率 PCA(p-PCA) 。</p></li><li><p> 第二个问题是单个高斯分布是单峰的，对有多个峰的数据分布不能得到好的结果。解决方案：高斯混合GMM 模型。</p></li></ol><p>下面对多维高斯分布的常用定理进行介绍。</p><p>我们记 $x=(x_1, x_2,\cdots,x_p)^T=(x_{a,m\times 1}, x_{b,n\times1})^T,\mu=(\mu_{a,m\times1}, \mu_{b,n\times1}),\Sigma=\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}$，已知 $x\sim\mathcal{N}(\mu,\Sigma)$。</p><p>首先是一个高斯分布的定理：</p><blockquote><p>  定理：已知 $x\sim\mathcal{N}(\mu,\Sigma), y\sim Ax+b$，那么 $y\sim\mathcal{N}(A\mu+b, A\Sigma A^T)$。</p><p>  证明：$\mathbb{E}[y]=\mathbb{E}[Ax+b]=A\mathbb{E}[x]+b=A\mu+b$，$Var[y]=Var[Ax+b]=Var[Ax]=A\cdot Var[x]\cdot A^T$。</p></blockquote><p>下面利用这个定理得到 $p(x_a),p(x_b),p(x_a|x_b),p(x_b|x_a)$ 这四个量。</p><ol><li><p>$x_a=\begin{pmatrix}\mathbb{I}<em>{m\times m}&amp;\mathbb{O}</em>{m\times n})\end{pmatrix}\begin{pmatrix}x_a\x_b\end{pmatrix}$，代入定理中得到：</p><p> $$<br> \mathbb{E}[x_a]=\begin{pmatrix}\mathbb{I}&amp;\mathbb{O}\end{pmatrix}\begin{pmatrix}\mu_a\\mu_b\end{pmatrix}=\mu_a\<br> Var[x_a]=\begin{pmatrix}\mathbb{I}&amp;\mathbb{O}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\begin{pmatrix}\mathbb{I}\\mathbb{O}\end{pmatrix}=\Sigma_{aa}<br> $$</p><p> 所以 $x_a\sim\mathcal{N}(\mu_a,\Sigma_{aa})$。</p></li><li><p> 同样的，$x_b\sim\mathcal{N}(\mu_b,\Sigma_{bb})$。</p></li><li><p>对于两个条件概率，我们引入三个量：</p><p> $$<br> x_{b\cdot a}=x_b-\Sigma_{ba}\Sigma_{aa}^{-1}x_a\<br> \mu_{b\cdot a}=\mu_b-\Sigma_{ba}\Sigma_{aa}^{-1}\mu_a\<br> \Sigma_{bb\cdot a}=\Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}<br> $$</p><p> 特别的，最后一个式子叫做 $\Sigma_{bb}$ 的 Schur Complementary。可以看到：</p><p> $$<br> x_{b\cdot a}=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbb{I}_{n\times n}\end{pmatrix}\begin{pmatrix}x_a\x_b\end{pmatrix}<br> $$</p><p> 所以：</p><p> $$<br> \mathbb{E}[x_{b\cdot a}]=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbb{I}<em>{n\times n}\end{pmatrix}\begin{pmatrix}\mu_a\\mu_b\end{pmatrix}=\mu</em>{b\cdot a}\<br> Var[x_{b\cdot a}]=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbb{I}<em>{n\times n}\end{pmatrix}\begin{pmatrix}\Sigma</em>{aa}&amp;\Sigma_{ab}\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\begin{pmatrix}-\Sigma_{aa}^{-1}\Sigma_{ba}^T\\mathbb{I}<em>{n\times n}\end{pmatrix}=\Sigma</em>{bb\cdot a}<br> $$</p><p> 利用这三个量可以得到 $x_b=x_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a$。因此：</p><p> $$<br> \mathbb{E}[x_b|x_a]=\mu_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a<br> $$</p><p> $$<br> Var[x_b|x_a]=\Sigma_{bb\cdot a}<br> $$</p><p> 这里同样用到了定理。</p></li><li><p>同样：</p><p> $$<br> x_{a\cdot b}=x_a-\Sigma_{ab}\Sigma_{bb}^{-1}x_b\<br> \mu_{a\cdot b}=\mu_a-\Sigma_{ab}\Sigma_{bb}^{-1}\mu_b\<br> \Sigma_{aa\cdot b}=\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba}<br> $$</p><p> 所以：</p><p> $$<br> \mathbb{E}[x_a|x_b]=\mu_{a\cdot b}+\Sigma_{ab}\Sigma_{bb}^{-1}x_b<br> $$</p><p> $$<br> Var[x_a|x_b]=\Sigma_{aa\cdot b}<br> $$</p></li></ol><p>下面利用上边四个量，求解线性模型：</p><blockquote><p>  已知：$p(x)=\mathcal{N}(\mu,\Lambda^{-1}),p(y|x)=\mathcal{N}(Ax+b,L^{-1})$，求解：$p(y),p(x|y)$。</p><p>  解：令 $y=Ax+b+\epsilon,\epsilon\sim\mathcal{N}(0,L^{-1})$，所以 $\mathbb{E}[y]=\mathbb{E}[Ax+b+\epsilon]=A\mu+b$，$Var[y]=A \Lambda^{-1}A^T+L^{-1}$，因此：<br>  $$<br>  p(y)=\mathcal{N}(A\mu+b,L^{-1}+A\Lambda^{-1}A^T)<br>  $$<br>  引入 $z=\begin{pmatrix}x\y\end{pmatrix}$，我们可以得到 $Cov[x,y]=\mathbb{E}[(x-\mathbb{E}[x])(y-\mathbb{E}[y])^T]$。对于这个协方差可以直接计算：<br>  $$<br>  \begin{align}<br>  Cov(x,y)&amp;=\mathbb{E}[(x-\mu)(Ax-A\mu+\epsilon)^T]=\mathbb{E}[(x-\mu)(x-\mu)^TA^T]=Var[x]A^T=\Lambda^{-1}A^T<br>  \end{align}<br>  $$<br>  注意到协方差矩阵的对称性，所以 $p(z)=\mathcal{N}\begin{pmatrix}\mu\A\mu+b\end{pmatrix},\begin{pmatrix}\Lambda^{-1}&amp;\Lambda^{-1}A^T\A\Lambda^{-1}&amp;L^{-1}+A\Lambda^{-1}A^T\end{pmatrix})$。根据之前的公式，我们可以得到：<br>  $$<br>  \mathbb{E}[x|y]=\mu+\Lambda^{-1}A^T(L^{-1}+A\Lambda^{-1}A^T)^{-1}(y-A\mu-b)<br>  $$</p><p>  $$<br>  Var[x|y]=\Lambda^{-1}-\Lambda^{-1}A^T(L^{-1}+A\Lambda^{-1}A^T)^{-1}A\Lambda^{-1}<br>  $$</p></blockquote>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;p&gt;对概率的诠释有两大学派，一种是频率派另一种是贝叶斯派。后面我们对观测集采用下面记号：&lt;br&gt;$$&lt;br&gt;X_{N\times p}=(x_{1},x_{2},\cdots,x_{N})^{T},x_{i}=(x_{i1},x_{i2},\cdots,x_{ip})^{T}&lt;br&gt;$$&lt;br&gt;这个记号表示有 $N$ 个样本，每个样本都是 $p$ 维向量。其中每个观测都是由 $p(x|\theta)$ 生成的。&lt;/p&gt;</summary>
    
    
    
    
    <category term="统计学习" scheme="https://jpccc.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>BN算法</title>
    <link href="https://jpccc.github.io/2021/10/25/BN%E7%AE%97%E6%B3%95/"/>
    <id>https://jpccc.github.io/2021/10/25/BN%E7%AE%97%E6%B3%95/</id>
    <published>2021-10-25T12:20:14.000Z</published>
    <updated>2022-04-10T04:01:33.325Z</updated>
    
    <content type="html"><![CDATA[<h1 id="BN算法概要"><a href="#BN算法概要" class="headerlink" title="BN算法概要"></a>BN算法概要</h1><p>Batch Normalization是2015年一篇论文中提出的数据归一化方法，往往用在深度神经网络中激活层之前。其作用可以加快模型训练时的收敛速度，使得模型训练过程更加稳定，避免梯度爆炸或者梯度消失。并且起到一定的正则化作用，几乎代替了Dropout。</p><span id="more"></span><h1 id="BN算法产生的背景"><a href="#BN算法产生的背景" class="headerlink" title="BN算法产生的背景"></a>BN算法产生的背景</h1><img src="https://jpccc.github.io/resource/deepLearning/001.png">&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;<img src="https://jpccc.github.io/resource/deepLearning/001.png"><blockquote><p>首先对第一张图进行分析。</p><blockquote><p>由于我们通常使用采用零均值化对网络进行参数初始化，我们初始的拟合直线也就是红色部分。另外的一条绿色直线，是我们的目标直线。从图能够直观看出，我们应该需要多次迭代才能得到我们的需要的目标直线。</p></blockquote></blockquote><blockquote><p>我们再看第二张图</p><blockquote><p>假设我们还是和第一张图有相同的分布，只是我们做了减均值，让数据均值为零。能够直观的发现可能只进行简单的微调就能够实现拟合（理想）。大大提高了我们的训练速度。因此，在训练开始前，对数据进行零均值是一个必要的操作。</p></blockquote></blockquote><blockquote><p>但是，随着网络层次加深参数对分布的影响不定(什么意思?)，导致网络每层间以及不同迭代轮次的相同层的输入分布发生改变，导致网络需要重新适应新的分布，迫使我们降低学习率降低影响。在这个背景下BN算法开始出现。       有些人首先提出在每层增加PCA白化(先对数据进行去相关然后再进行归一化)，这样基本满足了数据的0均值、单位方差、弱相关性。但是这样是不可取的，因为在白化过程中会计算协方差矩阵、求逆等操作，计算量会很大，另外，在反向传播时，白化的操作不一定可微。因此，在此背景下BN算法开始出现。</p></blockquote><h1 id="BN算法的实现和优点"><a href="#BN算法的实现和优点" class="headerlink" title="BN算法的实现和优点"></a>BN算法的实现和优点</h1><p>上面提到了PCA白化优点，能够去相关和数据均值，标准值归一化等优点。但是当数据量比较大的情况下去相关的话需要大量的计算，因此有些人提出了只对数据进行均值和标准差归一化。叫做近似白化预处理。</p><p>$$\hat{x}^k=\frac{X^k-E(X^k)}{\sqrt{Var[(x^k)}]}$$</p><p>由于训练过程采用了batch随机梯度下降，因此$E(X^k)$指的是一批训练数据时，各神经元输入值的平均值；$\sqrt{Var[(x^k)}]$指的是一批训练数据时各神经元输入值的标准差。</p><p>但是，这些应用到深度学习网络还远远不够，因为可能由于这种的强制转化导致数据的分布发生破坏。因此需要对公式的鲁棒性进行优化，就有人提出了变换重构的概念。就是在基础公式的基础之上加上了两个参数γ、β。这样在训练过程中就可以学习这两个参数，采用适合自己网络的BN公式。公式如下：<br>$$y^k=\gamma^k\hat x^k+\beta^k$$<br>每一个神经元都会有一对这样的参数γ、β。这样其实当<br>$$\beta^k=E[x^k],\gamma^k=\sqrt{var[x^k]}$$<br>时，是可以恢复出原始的某一层所学到的特征的。引入可学习重构参数γ、β，让网络可以学习恢复出原始网络所要学习的特征分布。</p><p>总结上面我们会得到BN的向前传导公式：</p><center>$\mu_\beta\leftarrow\frac{1}{m}\sum_{i=1}^nx_i$ //mnni batch mean$\delta_\beta^2\leftarrow\frac{1}{m}\sum_{i=1}^n(x_i-\mu_\beta)^2$//mnni batch variance$\hat{x}\leftarrow\frac{x_i-\mu_\beta}{\sqrt{\delta_\beta^2+\epsilon}}$//normalize$y_i\leftarrow\gamma\hat{x_i}+\beta\equiv BN_{\gamma,\beta}(x_i)$ //scale and shift</center>2. BN算法在网络中的作用<p>   BN算法像卷积层，池化层、激活层一样也输入一层。BN层添加在激活函数前，对输入激活函数的输入进行归一化。这样解决了输入数据发生偏移和增大的影响。</p><p>优点：</p><p>1、加快训练速度，能够增大学习率，即使小的学习率也能够有快速的学习速率;<br>2、不用理会拟合中的droupout、L2 正则化项的参数选择，采用BN算法可以省去这两项或者只需要小的L2正则化约束。原因，BN算法后，参数进行了归一化，原本经过激活函数没有太大影响的神经元，分布变得明显，经过一个激活函数以后，神经元会自动削弱或者去除一些神经元，就不用再对其进行dropout。另外就是L2正则化，由于每次训练都进行了归一化，就很少发生由于数据分布不同导致的参数变动过大，带来的参数不断增大。<br>3、 可以吧训练数据集打乱，防止训练发生偏移。</p><p>使用： 在卷积中，会出现每层卷积层中有（L）多个特征图。AxAxL特征矩阵。我们只需要以每个特征图为单元求取一对γ、β。</p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><div id="refer-anchor-1"></div><ul><li>[1] <a href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;BN算法概要&quot;&gt;&lt;a href=&quot;#BN算法概要&quot; class=&quot;headerlink&quot; title=&quot;BN算法概要&quot;&gt;&lt;/a&gt;BN算法概要&lt;/h1&gt;&lt;p&gt;Batch Normalization是2015年一篇论文中提出的数据归一化方法，往往用在深度神经网络中激活层之前。其作用可以加快模型训练时的收敛速度，使得模型训练过程更加稳定，避免梯度爆炸或者梯度消失。并且起到一定的正则化作用，几乎代替了Dropout。&lt;/p&gt;</summary>
    
    
    
    
    <category term="deepLearning" scheme="https://jpccc.github.io/tags/deepLearning/"/>
    
  </entry>
  
  <entry>
    <title>互信息</title>
    <link href="https://jpccc.github.io/2021/10/25/%E4%BA%92%E4%BF%A1%E6%81%AF/"/>
    <id>https://jpccc.github.io/2021/10/25/%E4%BA%92%E4%BF%A1%E6%81%AF/</id>
    <published>2021-10-25T04:44:13.000Z</published>
    <updated>2022-04-17T01:59:34.681Z</updated>
    
    <content type="html"><![CDATA[<h1 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h1><p>机器学习中很多地方都要根据目前的信息做出决策，信息熵主要是反应信息的不确定性；它的一个很重要的作用，就是做决策时提供一定的判断依据，比如决策树根据熵来往下设置分枝(branch)。</p><ol><li>计算方法 $$H(X)=-\sum_{i=1}^nP(x_i)logP(x_i)$$<br> 其中$P(x_i)$代表随机事件X为$x_i$的概率。</li></ol><span id="more"></span><ol start="2"><li>信息量</li></ol><p>信息量是对信息的度量，就跟时间的度量是秒一样；我们考虑一个离散的随机变量x，当我们观察到的这个变量的一个具体值的时候，我们接收到了多少信息呢？</p><p>多少信息用信息量来衡量，我们接受到的信息量信息的大小跟随机事件的概率分布有关，越小概率的事情发生了产生的信息量越大，如湖南产生的地震了；越大概率的事情发生了产生的信息量越小，如太阳从东边升起来了（肯定发生嘛，没什么信息量）。这很好理解！ <strong>所以描述信息量的函数应该是一个与随机变量的发生概率成负相关的函数，且不能为负数。</strong></p><p>例子:</p><blockquote><p>如果我们有俩个不相关的事件x和y，那么我们观察到的俩个事件同时发生时获得的信息应该等于观察到的事件各自发生时获得的信息之和，即：h(x,y) = h(x) + h(y)。 由于x，y是俩个不相关的事件，那么满足p(x,y) = p(x)*p(y)。<br>根据上面推导，我们很容易看出h(x)一定与p(x)的对数有关（因为只有对数形式的真数相乘之后，能够对应对数的相加形式，可以试试）。因此我们有信息量公式如下：<br>$$h(x)=-log_2p(x)$$<br>下面解决两个疑问:<br>(1). 为什么有一个负号?<br>信息量取概率的负对数，其实是因为信息量的定义是概率的倒数的对数。而用概率的倒数，是为了使概率越大，信息量越小，同时因为概率的倒数大于1，其对数自然大于0了。<br>(2). 为什么底数为2?<br>这是因为，我们只需要信息量满足低概率事件x对应于高的信息量，那么对数的选择是任意的，我们只是遵循信息论的普遍传统，使用2作为对数的底！</p></blockquote><ol start="3"><li>信息熵</li></ol><p>信息量度量的是一个具体事件发生了所带来的信息，而熵则是在结果出来之前对可能产生的信息量的期望——考虑该随机变量的所有可能取值，<strong>即所有可能发生事件所带来的信息量的期望</strong>。即<br>          $$H(X)=-\sum_{i=1}^np(x_i)logp(x_i)$$<br>信息熵还可以作为一个系统复杂程度的度量，如果系统越复杂，出现不同情况的种类越多，那么他的信息熵是比较大的。如果一个系统越简单，出现情况种类很少（极端情况为1种情况，那么对应概率为1，那么对应的信息熵为0），此时的信息熵较小。</p><h1 id="互信息"><a href="#互信息" class="headerlink" title="互信息"></a>互信息</h1><ol><li>定义<br> 互信息(Mutual Information)是衡量随机变量之间相互依赖程度的度量。</li><li>它的形象化解释是，假如明天下雨是个随机事件，假如今晚有晚霞同样是个随机事件，那么这两个随机事件互相依赖的程度是：</li></ol><p>​        当<b>不知道</b>”今晚有晚霞“情况下，”明天下雨“带来的不确定性<b>与我们已知</b>“今晚有晚霞“情况下，”明天下雨”带来的不确定性之差。</p><ol start="3"><li>解释<br> 假设存在一个随机变量$X$ ，和另外一个随机变量$Y$ ，那么它们的互信息是：<br> $$I(X;Y)=H(X)-H(X|Y)$$</li></ol><p>$H(X)$是$X$的信息熵,$H(X|Y)$是已知$Y$情况下，X带来的信息熵（条件熵）。</p><blockquote><p>直观理解是，我们知道存在两个随机事件X,Y，其中一个随机事件X 给我们带来了一些不确定性H(X)，我们想衡量Y,X 之间的关系。那么，如果X,Y 存在关联，当Y已知时，X给我们的不确定性会变化，这个变化值就是X的信息熵减去当已知 Y时，X的条件熵，就是互信息。</p></blockquote><p> 从概率角度，互信息是由随机变量 $X,Y$ 的联合概率分布 p(x,y) 和边缘概率分布 $p(x),p(y)$ 得出。</p><p> $$I(X;Y)=\sum_{y \in \cal Y}\sum_{x \in \cal X}p(x,y)log(\frac{p(x,y)}{p(x)p(y)})$$<br>  互信息和信息熵的关系是：</p> <p align="center">             <img src="https://jpccc.github.io/resource/Mutual_%20Information/001.jpg"> </p> 通常我们使用的最大化互信息条件，就是最大化两个随机事件的相关性。在数据集里，就是最大化两个数据集所拟合出的概率分布的相关性。当两个随机变量相同时,互信息最大，如下:$$I(X;Y)=H(X)-H(X|X)=H(X)$$<blockquote><p>在机器学习中，理想情况下，当互信息最大，可以认为从数据集中拟合出来的随机变量的概率分布与真实分布相同。</p></blockquote><p>到这里，应该足够大家日常理解使用了，以下是性质，应用和变形，几乎都是数学。</p><ol start="2"><li>The most common lower bound is InfoNCE [35] whose formula is given by: <p align="center">             <img src="https://jpccc.github.io/resource/Mutual_%20Information/002.PNG"> </p></li></ol>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;信息熵&quot;&gt;&lt;a href=&quot;#信息熵&quot; class=&quot;headerlink&quot; title=&quot;信息熵&quot;&gt;&lt;/a&gt;信息熵&lt;/h1&gt;&lt;p&gt;机器学习中很多地方都要根据目前的信息做出决策，信息熵主要是反应信息的不确定性；它的一个很重要的作用，就是做决策时提供一定的判断依据，比如决策树根据熵来往下设置分枝(branch)。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;计算方法 $$H(X)=-\sum_{i=1}^nP(x_i)logP(x_i)$$&lt;br&gt; 其中$P(x_i)$代表随机事件X为$x_i$的概率。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    
    <category term="math" scheme="https://jpccc.github.io/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>algebra</title>
    <link href="https://jpccc.github.io/2021/10/22/algebra/"/>
    <id>https://jpccc.github.io/2021/10/22/algebra/</id>
    <published>2021-10-22T08:26:34.000Z</published>
    <updated>2021-11-18T05:23:04.062Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一-向量"><a href="#一-向量" class="headerlink" title="一. 向量"></a>一. 向量</h1><ol><li>向量对于不同的学科有不一样的定义，我们将从三个角度对向量进行解释。<ul><li>物理学<br> 物理中的向量由长度和方向决定，长度和方向不变的情况下随意移动，表示的都是同一个向量。</li><li>计算机<br> 计算机中的向量更多的是对数据的抽象，可以是根据面积和价格定义的一个房子特征$\begin{bmatrix}100m^2\\700000￥\end{bmatrix}$或是通过神经网络得到的图象的的一个向量。</li><li>数学<br> 数学中的向量可以是任意东西，只要保证两个向量的相加$\vec v + \vec w$以及数字和向量相乘$2\vec v$是有意义的即可。<span id="more"></span></li></ul></li><li>线性代数中的向量可以理解为一个空间中的箭头，这个箭头起点落在原点。如果空间中有许多的向量，可以用点表示一个向量，即向量头的坐标。</li></ol><h1 id="二-向量的基本运算"><a href="#二-向量的基本运算" class="headerlink" title="二. 向量的基本运算"></a>二. 向量的基本运算</h1><ol><li><p>向量的加法<br> 可以理解为在坐标系中两个向量的移动。</p> <p align="center">             <img src="https://jpccc.github.io/resource/algebra/001.png"> </p></li><li><p>向量的乘法</p> <p align="center">             <img src="https://jpccc.github.io/resource/algebra/002.png"> </p></li></ol><h1 id="三-线性组合、张成空间、基"><a href="#三-线性组合、张成空间、基" class="headerlink" title="三. 线性组合、张成空间、基"></a>三. 线性组合、张成空间、基</h1><blockquote><p> 线性组合</p></blockquote><p>两个数乘向量相加称为两个向量的线性组合$a\vec v+ b\vec w$。<br>两个不共线的向量通过不同的线性组合可以得到二维平面中的所有向量。<br>两个共线的向量通过线程组合只能得到一个直线的所有向量。<br>如果两个向量都是零向量那么它只能在原点。</p><blockquote><p>张成向量</p></blockquote><p>所有可以表示给定向量线性组合的向量的集合称为给定向量的张成空间（span）。<br>一般来说两个向量张成空间可以是直线、平面。<br>三个向量张成空间可以是平面、空间。<br>如果多个向量，并且可以移除其中一个而不减小张成空间，那么它们是线性相关的，也可以说一个向量可以表示为其他向量的线性组合$\vec u = a \vec v + b\vec w$。<br>如果所有的向量都给张成的空间增加了新的维度，它们就成为线性无关的$\vec u \neq a \vec v + b\vec w$。</p><blockquote><p>基</p></blockquote><p>向量空间的一组基是张成该空间的一个线性无关向量集。</p><h1 id="四-矩阵与线性变换"><a href="#四-矩阵与线性变换" class="headerlink" title="四. 矩阵与线性变换"></a>四. 矩阵与线性变换</h1><p>(向量的基默认为(0,1)(1,0)正交基。左乘向量可以看作是向量对基向量进行操作。同一向量乘以不同的基表示对不同的基做相同的运算。)</p><p>严格意义上来说，线性变换是将向量作为输入和输出的一类函数。<br>变化可以多种多样，线性变化将变化限制在一个特殊类型的变换上，可以简单的理解为网格线保持平行且等距分布。<br>线性变化满足一下两个性质：</p><ul><li>线性变化前后直线依旧是直线不能弯曲。</li><li>原点必须保持固定</li></ul><p align="center">    <img src="https://jpccc.github.io/resource/algebra/003.png"></p><p>可以使用<strong>基向量来描述线性变化：</strong><br>通过记录两个基向量$\hat{i}$,$\hat{j}$的变换，就可以得到其他变化后的向量。<br>已知向量$\vec v = \begin{bmatrix}-1\\2\end{bmatrix}$<br>变换之前的$\hat i$和$\hat j$：</p><p>$$<br>\hat{i} = \begin{bmatrix}<br>1 \<br>   0<br>  \end{bmatrix} \<br>  \hat{j} = \begin{bmatrix}<br>0 \<br>   1<br>  \end{bmatrix} \<br>\vec{v} = -1\hat{i} + 2 \hat{j} = \begin{bmatrix}<br>-1 \<br>   2<br>  \end{bmatrix}<br>$$</p><p>变换之后的$\hat i$和$\hat j$：</p><p>$$<br>\begin{aligned}<br>\hat{i} = \begin{bmatrix}<br>1 \<br>   -2<br>  \end{bmatrix} \<br>  \hat{j} = \begin{bmatrix}<br>3 \<br>   0<br>  \end{bmatrix} \<br>\vec{v} = -1\hat{i} + 2 \hat{j} &amp;= \begin{bmatrix}<br>5 \<br>   2<br>  \end{bmatrix} \<br>  &amp;= -1\begin{bmatrix}<br>1 \<br>   -2<br>  \end{bmatrix}  + 2 \begin{bmatrix}<br>3 \<br>  0<br>  \end{bmatrix} \<br>  &amp;= \begin{bmatrix}<br>1 &amp; 3 \<br>   -2 &amp; 0<br>  \end{bmatrix}  \begin{bmatrix}<br>-1 \<br>   2<br>  \end{bmatrix} \<br>  &amp;= \begin{bmatrix}<br>5 \<br>   2<br>  \end{bmatrix} \<br>\end{aligned}<br>$$<br><strong>将对基向量的变换记录下来，对其作用于其它向量，就可以得到其它向量在变换后的空间中的值</strong></p><p>我们可以将变换后的$\hat i$和$\hat j$写成矩阵的形式：$\begin{bmatrix}<br>1 &amp; 3 \<br>   -2 &amp; 0<br>  \end{bmatrix}  \begin{bmatrix}<br>-1 \<br>   2<br>  \end{bmatrix}$，通过矩阵的乘法得到变化后的向量。(左侧矩阵是基向量的线性变换矩阵)</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/004.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/005.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/006.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/007.png"></p><p>如果变化后的$\hat{i}$和$\hat{j}$是线性相关的，变化后向量的张量就是一维空间：</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/008.png"></p><h1 id="五-矩阵乘法与线性变换复合的联系"><a href="#五-矩阵乘法与线性变换复合的联系" class="headerlink" title="五. 矩阵乘法与线性变换复合的联系"></a>五. 矩阵乘法与线性变换复合的联系</h1><blockquote><p>线性变化的复合</p></blockquote><p>如何描述先旋转再剪切的操作呢？</p><p>一个通俗的方法是首先左乘旋转矩阵然后左乘剪切矩阵。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/009.png"></p>   <p>两个矩阵的乘积需要从右向左读，类似函数的复合。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/010.png"></p><p>这样两个矩阵的乘积就对应了一个复合的线性变换，最终得到对应变换后的$\hat{i}$和$\hat{j}$</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/011.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/012.png"></p>这一过程具有普适性：<p align="center">            <img src="https://jpccc.github.io/resource/algebra/013.png"></p><blockquote><p>矩阵乘法的顺序</p></blockquote><p align="center">            <img src="https://jpccc.github.io/resource/algebra/014.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/015.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/016.png"></p><blockquote><p>如何证明矩阵乘法的结合性？</p></blockquote><p>$(AB)C = A(BC)$<br>根据线性变化我们可以得出，矩阵的乘法都是以CBA的顺序变换得到，所以他们本质上相同，通过变化的形式解释比代数计算更加容易理解。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/017.png"></p><h1 id="六-三维空间的线性变化"><a href="#六-三维空间的线性变化" class="headerlink" title="六. 三维空间的线性变化"></a>六. 三维空间的线性变化</h1><p>三维的空间变化和二维的类似。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/018.png"></p><p>同样跟踪基向量的变换，能很好的解释变换后的向量，同样两个矩阵相乘的复合变换也是。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/019.png"></p><h1 id="七-行列式"><a href="#七-行列式" class="headerlink" title="七. 行列式"></a>七. 行列式</h1><blockquote><p>行列式的本质</p></blockquote><p>行列式的本质是计算线性变化对空间的缩放比例，具体一点就是，测量一个给定区域面积增大或减小的比例。<strong>注意，面积的变化比例是对原空间中的网格来说的。也可以说成是原基向量组成的区域面积的变化。</strong><br>单位面积的变换代表任意区域的面积变换比例。<br>值得注意的是：</p><ul><li>如果一个二维线性变换的行列式为0，说明其将说明起其将整个平面压缩成一条线甚至一个点上。</li><li>所以只要检测一个矩阵的行列式是否为0，我们就可以知道矩阵所代表的变换是否将空间压缩到更小的维度上。</li></ul><p align="center">            <img src="https://jpccc.github.io/resource/algebra/020.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/021.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/022.png"></p>行列式的值表示缩放比例。<p align="center">            <img src="https://jpccc.github.io/resource/algebra/023.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/024.png"></p><p>行列式为什么有负值呢？可以从两个角度考虑：</p><ul><li>一是变换将平面进行了反转。就好像将一张纸的正面通过变换将这张纸进行了翻面。</li><li>二是考虑的$\hat i$和$\hat j$的相对位置。如$\hat i$在$\hat j$的左边，通过变换将$\hat i$变换到了$\hat j$的右边，那么这个变换所对应矩阵的行列式值为负。<p align="center">          <img src="https://jpccc.github.io/resource/algebra/001.gif"></p></li></ul><p>三维空间的行列式类似，它的单位是一个单位1的立方体。</p><p>三位空间的线性变换，可以使用右手定则判断三维空间的定向。如果变换前后都可以通过右手定则得到，那么他的行列式就是正值，否则为负值.</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/025.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/026.png"></p><h1 id="八-逆矩阵、列空间、秩、零空间"><a href="#八-逆矩阵、列空间、秩、零空间" class="headerlink" title="八. 逆矩阵、列空间、秩、零空间"></a>八. 逆矩阵、列空间、秩、零空间</h1><blockquote><p>线性方程组</p></blockquote><p align="center">            <img src="https://jpccc.github.io/resource/algebra/027.png"></p><p>从几何的角度来思考，矩阵A表示一个线性变换，我们需要找到一个$\vec x$使得它在变换后和$\vec v$重合。</p><blockquote><p>逆矩阵</p></blockquote><p>矩阵的逆运算，记为$\vec A = \begin{bmatrix}3&amp;1 \0&amp;2\end{bmatrix}^{-1}$，对于线程方程$A \vec x = \vec v $来说，找到$A^{-1}$就得到解$\vec x = A^{-1} \vec v$。<br>$A^{-1}A=\begin{bmatrix}1&amp;0 \0&amp;1\end{bmatrix}$，什么都不做称为恒等变换。</p><blockquote><p>线性方程组的解</p></blockquote><p>对于方程组$A\vec x = \vec v$，线性变换A存在两种情况：</p><ul><li>$det(A) \neq0$：这时空间的维数并没有改变，有且只有一个向量经过线性变换后和$\vec v$重合。</li><li>$det(A) =0$：空间被压缩到更低的维度，这时不存在逆变换。因为不能将一个直线解压缩为一个平面(这会要求将一个单独的向量变换为一整条线的向量，函数多对一可以，一对多不行，即存在一个矩阵A将多个向量映射到一个点，但不可能存在一个矩阵A将一个点映射成一条线的向量)。但是即使不存在逆变换，解可能仍然存在，这时候目标$\vec v$必须刚好落在压缩后的空间上。(例如一个变换将空间压缩成了一条直线，而向量$\vec v$恰好在这条直线上。共线的情况下，则有无穷解，因为有无穷都被压缩到直线上的每一点)。</li></ul><blockquote><p>秩</p></blockquote><p>秩代表变换后空间的维度。<br>如果线性变化后将空间压缩成一条直线，那么称这个变化的秩为1；<br>如果线性变化后向量落在二维平面，那么称这个变化的秩为2。</p><blockquote><p>列空间</p></blockquote><p>所有可能的输出向量$A\vec v$构成的集合(A为基向量的集合，对其进行任意的组合(组合即乘以$\vec{v}$)，所得到的所有向量)，称为列空间，即所有列向量张成的空间。</p><ul><li>更精确的秩的定义就是列空间的维数。</li></ul><blockquote><p>零空间（Null space）</p></blockquote><p>所有的线性变化中，零向量一定包含在列空间中，因为线性变换原点保持不动。对于非满秩的情况来说，会有一系列的向量在变换后仍为零向量（二维空间压缩为一条直线，一条线上的向量都会落到原点。）</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/002.gif"></p><p>三维空间压缩为二维平面，一条线上的向量都会落到原点。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/003.gif"></p><p>三维空间压缩为一条直线，整个平面上的向量都会落到原点。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/004.gif"></p><p>当$A\vec x = \vec v$中的$\vec v$是一个零向量，即$A\vec x = \begin{bmatrix}0 \\0\end{bmatrix}$时，零空间就是它所有可能的解。</p><h1 id="非方阵、不同维度空间之间的线性变换"><a href="#非方阵、不同维度空间之间的线性变换" class="headerlink" title="非方阵、不同维度空间之间的线性变换"></a>非方阵、不同维度空间之间的线性变换</h1><p>不同维度的变换也是存在的。</p><p>一个$3\times2$的矩阵：$\begin{bmatrix}2&amp;0\\-1&amp;1\\-2&amp;1 \end{bmatrix}$它的几何意义是将一个二维空间映射到三维空间上，矩阵有两列表明输入空间有两个基向量，有三行表示每个向量在变换后用三个独立的坐标描述。(A是满秩的，因为其与输入空间x的维度相等)</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/028.png"></p><p>一个$2\times 3$的矩阵：$\begin{bmatrix}3&amp;1&amp;4\\1&amp;5&amp;9 \end{bmatrix}$则表示将一个三维空间映射到二维空间上。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/029.png"></p><p>一个$1\times 2$的矩阵：$\begin{bmatrix}1&amp;2 \end{bmatrix}$表示一个二维空间映射到一维空间。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/030.png"></p>(还可以从解方程的角度，通解+特解，来理解一维变二维。)# 点积与对偶性> 点积<p>对于两个维度相同的向量，他们的点积计算为：$\begin{bmatrix}1\\2\end{bmatrix}\cdot\begin{bmatrix} 3\\4\end{bmatrix}=1\cdot3+2\cdot4=11$。<br>点积的几何解释是将一个向量向一个向量投影，然后两个长度相乘，如果为负数则表示反向。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/031.png"></p><p>为什么点积和坐标相乘联系起来了？这和对偶性有关。</p><blockquote><p>对偶性</p></blockquote><p>对偶性的思想是：<strong>每当看到一个多维空间到数轴上的线性变换时</strong>，他都与空间中的唯一一个向量对应，也就是说使用线性变换和与这个向量点乘等价。这个向量也叫做线性变换的对偶向量。<br>当二维空间向一维空间映射时，如果在二维空间中等距分布的点在变换后还是等距分布的，那么这种变换就是线性的。</p><p>假设有一个线性变换A<br>$\begin{bmatrix}1&amp;-2\end{bmatrix}$<br>和一个向量<br>$\vec v=\begin{bmatrix} 4\\3 \end{bmatrix}$。</p><p>变换后的位置为$\begin{bmatrix}1&amp;-2\end{bmatrix}\begin{bmatrix}4\3\end{bmatrix}=4\cdot1+3\cdot-2=-2$，这个变换是一个二维空间向一维空间的变化，所以变换后的结果为一个坐标值。<br>我们可以看到线性变换的计算过程和向量的点积相同$\begin{bmatrix}1\-2\end{bmatrix}\cdot\begin{bmatrix}4\3\end{bmatrix}=4\cdot1+3\cdot-2=-2$，所以向量和一个线性变化有着微妙的联系。<br>假设有一个倾斜的数轴，上面有一个单位向量$\vec v$，对于任意一个向量它在数轴上的投影都是一个数字，这表示了一个二维向量到一位空间的一种线性变换，那么如何得到这个线性变化呢？</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/032.png">        </p><p>由之前的内容来说，我们可以观察基向量$\vec i$和$\vec j$的变化，从而得到对应的线性变化。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/033.png">        </p><p>因为$\vec i$、$\vec j$、$\vec u$都是单位向量，根据对称性可以得到$\vec i$和$\vec j$在$\vec u$上的投影长度刚好是$\vec u$的坐标。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/034.png">        </p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/035.png">        </p><p>这样空间中的所有向量都可以通过线性变化<br>$\begin{bmatrix} u_x&amp;u_y \end{bmatrix}$<br>得到，而这个计算过程刚好和单位向量的点积相同。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/036.png"></p><p>也就是为什么向量投影到直线的长度，刚好等于它与直线上单位向量的点积，对于非单位向量也是类似，只是将其扩大到对应倍数。</p><h1 id="叉积"><a href="#叉积" class="headerlink" title="叉积"></a>叉积</h1><p>对于两个向量所围成的面积来说，可以使用行列式计算，将两个向量看作是变换后的基向量，这样通过行列式就可以得到变换后面积缩放的比例，因为基向量的单位为1，所以就得到了对应的面积。<br>考虑到正向，这个面积的值存在负值，这是参照基向量$\vec i$和$\vec j$的相对位置来说的。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/037.png">        </p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/038.png">        </p><p>真正的叉积是通过两个三维向量$\vec v$和$\vec w$，生成一个新的三维向量$\vec u$，这个向量垂直于向量$\vec v$和$\vec w$所在的平面，长度等于它们围成的面积。<br>叉积的反向可以通过右手定则判断：</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/039.png">        </p>叉积的计算方法：<p align="center">            <img src="https://jpccc.github.io/resource/algebra/040.png">        </p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/041.png">        </p><h1 id="线性代数看叉积"><a href="#线性代数看叉积" class="headerlink" title="线性代数看叉积"></a>线性代数看叉积</h1><p>参考二维向量的叉积计算：</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/042.png"></p><p>三维的可以写成类似的形式，但是他并是真正的叉积，不过和真正的叉积已经很接近了。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/043.png"></p><p>我可以构造一个函数，它可以把一个三维空间映射到一维空间上。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/044.png"></p><p>右侧行列式是线性的，所以我们可以找到一个线性变换代替这个函数。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/045.png"></p><p>根据对偶性的思想，从多维空间到一维空间的线性变换，等于与对应向量的点积，这个特殊的向量$\vec p$就是我们要找的向量。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/046.png"></p><blockquote><p>从数值计算上:</p></blockquote><p align="center">            <img src="https://jpccc.github.io/resource/algebra/047.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/048.png"></p><p>向量$\vec p$的计算结果刚好和叉积计算的结果相同。</p><blockquote><p>从几何意义：</p></blockquote><p align="center">            <img src="https://jpccc.github.io/resource/algebra/049.png"></p><p>当向量$\vec p$和向量$\begin{bmatrix}x\\y\\z \end{bmatrix}$点乘时，得到一个$\begin{bmatrix}x\\y\\z \end{bmatrix}$与$\vec v$与$\vec w$确定的平行六面体的有向体积，什么样的向量满足这个性质呢？<br>点积的几何解释是，其他向量在$\vec p$上的投影的长度乘以$\vec p$的长度。<br>对于平行六面体的体积来说，它等于$\vec v$和$\vec w$所确定的面积乘以$\begin{bmatrix}x\\y\\z \end{bmatrix}$在垂线上的投影。<br>那么$\vec p$要想满足这一要求，那么它就刚好符合，长度等于$\vec v,\vec w$所围成的面积，且刚好垂直这个平面。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/050.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/051.png"></p><h1 id="基变换"><a href="#基变换" class="headerlink" title="基变换"></a>基变换</h1><p>标准坐标系的基向量为$\vec {i}: \begin{bmatrix}1\0 \end{bmatrix}$和$\vec {j}: \begin{bmatrix}0\1 \end{bmatrix}$，假如詹妮弗有另一个坐标系：她的基向量为$\vec i \begin{bmatrix}2\1 \end{bmatrix}$和$\vec j \begin{bmatrix}-1\1 \end{bmatrix}$。<br>对于同一个点$\begin{bmatrix}3\2 \end{bmatrix}$来说他们所表示的形式不同，在詹妮弗的坐标系中表示为$\begin{bmatrix}\frac{5}{3}\\frac{1}{3} \end{bmatrix}$。（在不同基向量下，坐标同基相乘的结果是一样的。）<br>{詹尼弗的坐标乘以其基向量的结果(向量)是在我们坐标系中的表示。，即$\begin{bmatrix}3\2 \end{bmatrix}$}<br>从标准坐标到詹尼佛的坐标系，我能可以得到一个线性变换$A:\begin{bmatrix}2&amp;-1\1&amp;1 \end{bmatrix}$。（这个变换将詹尼佛的0，1变成我们语言表示的詹尼佛的0，1）</p><p>如果想知道詹妮弗的坐标系中点$\begin{bmatrix}3\2 \end{bmatrix}$在标准坐标系的位置，可以通过$\begin{bmatrix}2&amp;-1\1&amp;1 \end{bmatrix}\begin{bmatrix}3\2 \end{bmatrix}$得到。（基是我们的语言表示，而坐标是詹妮弗中的坐标，那么在我们的空间网格中，詹妮弗的坐标系中点$\begin{bmatrix}3\2 \end{bmatrix}$所代表的向量在我们的坐标系中的坐标为$\begin{bmatrix}4\5 \end{bmatrix}$）</p><p>如果想知道标准坐标系中点$\begin{bmatrix}3\2 \end{bmatrix}$在詹妮弗坐标系的位置，可以通过$\begin{bmatrix}2&amp;-1\1&amp;1 \end{bmatrix}^{-1}\begin{bmatrix}3\2 \end{bmatrix}$得到。<br>具体的例子，90°旋转。<br>在标准坐标系可以跟踪基向量的变化来体现：</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;一-向量&quot;&gt;&lt;a href=&quot;#一-向量&quot; class=&quot;headerlink&quot; title=&quot;一. 向量&quot;&gt;&lt;/a&gt;一. 向量&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;向量对于不同的学科有不一样的定义，我们将从三个角度对向量进行解释。&lt;ul&gt;
&lt;li&gt;物理学&lt;br&gt; 物理中的向量由长度和方向决定，长度和方向不变的情况下随意移动，表示的都是同一个向量。&lt;/li&gt;
&lt;li&gt;计算机&lt;br&gt; 计算机中的向量更多的是对数据的抽象，可以是根据面积和价格定义的一个房子特征$\begin{bmatrix}100m^2\\700000￥\end{bmatrix}$或是通过神经网络得到的图象的的一个向量。&lt;/li&gt;
&lt;li&gt;数学&lt;br&gt; 数学中的向量可以是任意东西，只要保证两个向量的相加$\vec v + \vec w$以及数字和向量相乘$2\vec v$是有意义的即可。</summary>
    
    
    
    
    <category term="math" scheme="https://jpccc.github.io/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://jpccc.github.io/2021/10/17/hello-world/"/>
    <id>https://jpccc.github.io/2021/10/17/hello-world/</id>
    <published>2021-10-17T07:02:13.391Z</published>
    <updated>2021-10-23T05:02:31.970Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
