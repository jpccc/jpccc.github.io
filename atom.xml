<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>流沙</title>
  
  <subtitle>Artificial Intelligence</subtitle>
  <link href="https://jpccc.github.io/atom.xml" rel="self"/>
  
  <link href="https://jpccc.github.io/"/>
  <updated>2022-05-09T06:00:08.883Z</updated>
  <id>https://jpccc.github.io/</id>
  
  <author>
    <name> liusha</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="https://jpccc.github.io/2022/04/11/%E5%9B%BE%E5%83%8F%E7%9A%84%E6%93%8D%E4%BD%9C/"/>
    <id>https://jpccc.github.io/2022/04/11/%E5%9B%BE%E5%83%8F%E7%9A%84%E6%93%8D%E4%BD%9C/</id>
    <published>2022-04-11T02:32:32.733Z</published>
    <updated>2022-05-09T06:00:08.883Z</updated>
    
    <content type="html"><![CDATA[<h2 id="PIL"><a href="#PIL" class="headerlink" title="PIL"></a>PIL</h2><blockquote><p>  PIL，全称 Python Imaging Library，是一个功能非常强大而且简单易用的图像处理库,已经是Python平台事实上的图像处理标准库了。PIL功能非常强大，但API却非常简单易用。但是，由于 PIL 仅支持到Python 2.7，加上年久失修，于是一群志愿者在 PIL 的基础上创建了兼容 Python 3 的版本，名字叫 Pillow ，我们可以通过安装 Pillow 来使用 PIL。</p></blockquote><h3 id="1-pip-安装-pillow"><a href="#1-pip-安装-pillow" class="headerlink" title="1. pip 安装 pillow"></a>1. pip 安装 pillow</h3><p>在 Ubuntu 下通过一个简单的命令<code>pip3 install pillow</code>即可成功安装库。</p><p>如果遇到<code>Permission denied</code>安装失败，请加上<code>sudo</code>重试。</p><h3 id="2-打开、保存、显示图片"><a href="#2-打开、保存、显示图片" class="headerlink" title="2. 打开、保存、显示图片"></a>2. 打开、保存、显示图片</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">image = Image.<span class="built_in">open</span>(<span class="string">&#x27;cat.jpg&#x27;</span>)</span><br><span class="line">image.show()</span><br><span class="line">image.save(<span class="string">&#x27;1.jpg&#x27;</span>,<span class="string">&#x27;jpeg&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(image.mode, image.size, image.<span class="built_in">format</span>)</span><br><span class="line"><span class="comment"># RGB (481, 321) JPEG</span></span><br></pre></td></tr></table></figure><ul><li>  mode 属性为图片的模式，RGB 代表彩色图像，L 代表光照图像也即灰度图像等</li><li>  size 属性为图片的大小(宽度，长度)</li><li>  format 属性为图片的格式，如常见的 PNG、JPEG 等</li></ul><h3 id="3-转换图片模式"><a href="#3-转换图片模式" class="headerlink" title="3. 转换图片模式"></a>3. 转换图片模式</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">image.show()</span><br><span class="line">grey_image = image.convert(<span class="string">&#x27;L&#x27;</span>)</span><br><span class="line">grey_image.show()</span><br></pre></td></tr></table></figure><p><img src="E:\笔记\markdown\reference\picture\v2-fa0ae3e48ef7a9645c252b49c16fce06_720w.jpg" alt="img"></p><p><img src="E:\笔记\markdown\reference\picture\v2-0e23c82abff1c3673d55358c7e6f1155_720w.jpg" alt="img"></p><ul><li>  任何支持的图片模式都可以直接转为彩色模式或者灰度模式，但是，若是想转化为其他模式，则需要借助一个中间模式（通常是彩色）来进行过转</li></ul><h3 id="4-通道分离合并"><a href="#4-通道分离合并" class="headerlink" title="4. 通道分离合并"></a>4. 通道分离合并</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">r, g, b = image.split()</span><br><span class="line">im = Image.merge(<span class="string">&#x27;RGB&#x27;</span>, (b, g, r))</span><br></pre></td></tr></table></figure><ul><li>  彩色图像可以分离出 R、G、B 通道，但若是灰度图像，则返回灰度图像本身。然后，可以将 R、G、B 通道按照一定的顺序再合并成彩色图像。</li></ul><h3 id="5-图片裁剪、旋转和改变大小"><a href="#5-图片裁剪、旋转和改变大小" class="headerlink" title="5. 图片裁剪、旋转和改变大小"></a>5. 图片裁剪、旋转和改变大小</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">box = (<span class="number">100</span>, <span class="number">100</span>, <span class="number">300</span>, <span class="number">300</span>)</span><br><span class="line">region = image.crop(box)</span><br><span class="line">region = region.transpose(Image.ROTATE_180)</span><br><span class="line">image.paste(region, box)</span><br><span class="line">image.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image, ImageFilter</span><br><span class="line"><span class="comment"># 打开一个jpg图像文件，注意是当前路径:</span></span><br><span class="line">im = Image.<span class="built_in">open</span>(<span class="string">&#x27;test.jpg&#x27;</span>)</span><br><span class="line"><span class="comment"># 应用模糊滤镜:</span></span><br><span class="line">im2 = im.<span class="built_in">filter</span>(ImageFilter.BLUR)</span><br></pre></td></tr></table></figure><p><img src="E:\笔记\markdown\reference\picture\v2-6eb679436bfcefcf51d57c95d5e1ef10_720w.jpg" alt="img"></p><ul><li>  通过定义一个 4 元组，依次为左上角 X 坐标、Y 坐标，右下角 X 坐标、Y 坐标，可以対原图片的某一区域进行裁剪，然后进行一定处理后可以在原位置粘贴回去。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">im = image.resize((300, 300))</span><br><span class="line">im = image.rotate(45)  # 逆时针旋转 45 度</span><br><span class="line">im = image.transpose(Image.FLIP_LEFT_RIGHT) # 左右翻转</span><br><span class="line">im = im.transpose(Image.FLIP_TOP_BOTTOM)# 上下翻转</span><br><span class="line"># 缩放到50%:</span><br><span class="line">im.thumbnail((w//2, h//2))</span><br></pre></td></tr></table></figure><h3 id="6-像素值操作"><a href="#6-像素值操作" class="headerlink" title="6. 像素值操作"></a>6. 像素值操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">out = image.point(<span class="keyword">lambda</span> i: i * <span class="number">1.2</span>) <span class="comment"># 对每个像素值乘以 1.2</span></span><br><span class="line">source = image.split()</span><br><span class="line">out = source[<span class="number">0</span>].point(<span class="keyword">lambda</span> i: i &gt; <span class="number">128</span> <span class="keyword">and</span> <span class="number">255</span>) <span class="comment"># 对 R 通道进行二值化</span></span><br></pre></td></tr></table></figure><p><img src="E:\笔记\markdown\reference\picture\v2-8317adc4f04515dec04330c85bd332c7_720w.jpg" alt="img"></p><ul><li>  i &gt; 128 and 255，当 i &lt;= 128 时，返回 False 也即 0,；反之返回 255 。</li></ul><h3 id="7-和-Numpy-数组之间的转化"><a href="#7-和-Numpy-数组之间的转化" class="headerlink" title="7. 和 Numpy 数组之间的转化"></a>7. 和 Numpy 数组之间的转化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">array = np.array(image)</span><br><span class="line"><span class="built_in">print</span>(array.shape) <span class="comment">#(321, 481, 3)</span></span><br><span class="line">image = Image.fromarray(array)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#tesnor 转pil</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">image = x.cpu().clone()</span><br><span class="line">image = image.squeeze(<span class="number">0</span>)  <span class="comment"># 压缩一维</span></span><br><span class="line">image = transforms.ToPILImage()(image)  <span class="comment"># 自动转换为0-255</span></span><br><span class="line"><span class="comment"># image.show()</span></span><br><span class="line">file = <span class="string">&quot;./pic/&quot;</span> + <span class="built_in">str</span>(step) + <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">Image.Image.save(image, file)</span><br></pre></td></tr></table></figure><h3 id="8-绘图"><a href="#8-绘图" class="headerlink" title="8.绘图"></a>8.绘图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image, ImageDraw, ImageFont, ImageFilter</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机字母:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rndChar</span>():</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">chr</span>(random.randint(<span class="number">65</span>, <span class="number">90</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机颜色1:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rndColor</span>():</span></span><br><span class="line">    <span class="keyword">return</span> (random.randint(<span class="number">64</span>, <span class="number">255</span>), random.randint(<span class="number">64</span>, <span class="number">255</span>), random.randint(<span class="number">64</span>, <span class="number">255</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机颜色2:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rndColor2</span>():</span></span><br><span class="line">    <span class="keyword">return</span> (random.randint(<span class="number">32</span>, <span class="number">127</span>), random.randint(<span class="number">32</span>, <span class="number">127</span>), random.randint(<span class="number">32</span>, <span class="number">127</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 240 x 60:</span></span><br><span class="line">width = <span class="number">60</span> * <span class="number">4</span></span><br><span class="line">height = <span class="number">60</span></span><br><span class="line">image = Image.new(<span class="string">&#x27;RGB&#x27;</span>, (width, height), (<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>))</span><br><span class="line"><span class="comment"># 创建Font对象:</span></span><br><span class="line">font = ImageFont.truetype(<span class="string">&#x27;Arial.ttf&#x27;</span>, <span class="number">36</span>)</span><br><span class="line"><span class="comment"># 创建Draw对象:</span></span><br><span class="line">draw = ImageDraw.Draw(image)</span><br><span class="line"><span class="comment"># 填充每个像素:</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(width):</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(height):</span><br><span class="line">        draw.point((x, y), fill=rndColor())</span><br><span class="line"><span class="comment"># 输出文字:</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">    draw.text((<span class="number">60</span> * t + <span class="number">10</span>, <span class="number">10</span>), rndChar(), font=font, fill=rndColor2())</span><br><span class="line"><span class="comment"># 模糊:</span></span><br><span class="line">image = image.<span class="built_in">filter</span>(ImageFilter.BLUR)</span><br><span class="line">image.save(<span class="string">&#x27;code.jpg&#x27;</span>, <span class="string">&#x27;jpeg&#x27;</span>)</span><br></pre></td></tr></table></figure><p><a href="https://pillow.readthedocs.org/">官方文档</a></p><h2 id="plt"><a href="#plt" class="headerlink" title="plt"></a>plt</h2><h2 id="opencv"><a href="#opencv" class="headerlink" title="opencv"></a>opencv</h2><p>openCV具体笔记见对应的ipynb文件</p><p>注意：</p><ol><li> ​    opencv的读取格式为：W,H,C，其中C是BGR的，plt等是RGB的。</li></ol><h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><h3 id="一、-概念"><a href="#一、-概念" class="headerlink" title="一、 概念"></a>一、 概念</h3><p>在图像分类任务中，图像数据的增广是一种常用的<strong>正则化方法</strong>，主要用于增加训练数据集，让数据集尽可能的多样化，使得训练的模型具有更强的<strong>泛化能力</strong>，常用于数据量不足或者模型参数较多的场景。除了 ImageNet 分类任务标准数据增广方法外，还有8种数据增广方式非常常用，这里对其进行简单的介绍和对比，大家也可以将这些增广方法应用到自己的任务中，以获得模型精度的提升。这8种数据增广方式在ImageNet上的精度指标如 <strong>图1</strong> 所示。</p><p><img src="https://paddlepedia.readthedocs.io/en/latest/_images/main_image_aug.png" alt="img"></p><center>图一 8种数据增广方法</center><h3 id="二、常用数据增广方法"><a href="#二、常用数据增广方法" class="headerlink" title="二、常用数据增广方法"></a>二、<a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#id2">常用数据增广方法</a></h3><blockquote><p>注：如果没有特殊说明，本章节中所有示例为 ImageNet 分类，并且假设最终输入网络的数据维为：[batch-size, 3, 224, 224]</p></blockquote><p>在ImageNet 分类任务中，训练阶段的标准数据增广方法为以下几步：</p><ol><li><p>图像解码：简写为 <code>ImageDecode</code></p></li><li><p>随机裁剪到长宽均为 224 的图像：简写为 <code>RandCrop</code></p></li><li><p>水平方向随机翻转：简写为 <code>RandFlip</code></p></li><li><p>图像数据的归一化：简写为 <code>Normalize</code></p></li><li><p>图像数据的重排，<code>[224, 224, 3]</code> 变为 <code>[3, 224, 224]</code>：简写为 <code>Transpose</code></p></li><li><p>多幅图像数据组成 batch 数据，如 <code>batch-size</code> 个 <code>[3, 224, 224]</code> 的图像数据拼组成 <code>[batch-size, 3, 224, 224]</code>：简写为 <code>Batch</code></p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基于pytorch框架实现以上基本的图像增广操作</span></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>相比于上述标准的图像增广方法，研究者也提出了很多改进的图像增广策略，这些策略均是在标准增广方法的不同阶段插入一定的操作，基于这些策略操作所处的不同阶段，我们将其分为了三类：</p><ol><li><p>对 <code>RandCrop</code> (上述的阶段2)后的 224 的图像进行一些变换: AutoAugment，RandAugment</p></li><li><p>对<code>Transpose</code> (上述的阶段5)后的 224 的图像进行一些裁剪: CutOut，RandErasing，HideAndSeek，GridMask</p></li><li><p>对 <code>Batch</code>(上述的阶段6) 后的数据进行混合: Mixup，Cutmix</p></li></ol><p>增广后的可视化效果如 <strong>图2</strong> 所示。</p><p><img src="E:\笔记\markdown\reference\picture\image_aug_samples_s.jpg" alt="图2 数据增广后可视化"></p><center> 图2 数据增广后可视化</center><p>下文将介绍这些策略的原理与使用方法，其中，每种数据增广策略的参考论文与参考开源代码均在下面的介绍中列出。</p><p>以 <strong>图3</strong> 为测试图像，第三节将基于测试图像进行变换，并将变换后的效果进行可视化。</p><blockquote><p>由于<code>RandCrop</code>是随机裁剪，变换前后的图像内容可能会有一定的差别，无法直观地对比变换前后的图像。因此，本节将 <code>RandCrop</code> 替换为 <code>Resize</code>。</p></blockquote><p><img src="E:\笔记\markdown\reference\picture\test_baseline.jpeg" alt="图3 测试图像"></p><center>图3 测试图像</center><h3 id="三、图像变换类"><a href="#三、图像变换类" class="headerlink" title="三、图像变换类"></a>三、<a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#id3">图像变换类</a></h3><p>图像变换类指的是对 <code>RandCrop</code> 后的224 的图像进行一些变换，主要包括：</p><ul><li>AutoAugment[1]</li><li>RandAugment[2]</li></ul><h4 id="3-1-AutoAugment"><a href="#3-1-AutoAugment" class="headerlink" title="3.1 AutoAugment"></a>3.1 <a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#autoaugment">AutoAugment</a></h4><p>论文地址：<a href="https://arxiv.org/abs/1805.09501v1">https://arxiv.org/abs/1805.09501v1</a></p><p>开源代码github地址：<a href="https://github.com/DeepVoltaire/AutoAugment">https://github.com/DeepVoltaire/AutoAugment</a></p><p>不同于常规的人工设计图像增广方式，AutoAugment 是在一系列图像增广子策略的搜索空间中通过搜索算法找到的适合特定数据集的图像增广方案。针对 ImageNet 数据集，最终搜索出来的数据增广方案包含 25 个子策略组合，每个子策略中都包含两种变换，针对每幅图像都随机的挑选一个子策略组合，然后以一定的概率来决定是否执行子策略中的每种变换。</p><p>结果如 <strong>图4</strong> 所示。</p><p><img src="E:\笔记\markdown\reference\picture\test_autoaugment.jpeg" alt="图4 AutoAugment后图像可视化"></p><center>图4 AutoAugment后图像可视化</center><h4 id="3-2-RandAugment"><a href="#3-2-RandAugment" class="headerlink" title="3.2 RandAugment"></a>3.2 <a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#randaugment">RandAugment</a></h4><p>论文地址：<a href="https://arxiv.org/pdf/1909.13719.pdf">https://arxiv.org/pdf/1909.13719.pdf</a></p><p>开源代码github地址：<a href="https://github.com/heartInsert/randaugment">https://github.com/heartInsert/randaugment</a></p><p><code>AutoAugment</code> 的搜索方法比较暴力，直接在数据集上搜索针对该数据集的最优策略，其计算量很大。在 <code>RandAugment</code> 文章中作者发现，一方面，针对越大的模型，越大的数据集，使用 <code>AutoAugment</code> 方式搜索到的增广方式产生的收益也就越小；另一方面，这种搜索出的最优策略是针对该数据集的，其迁移能力较差，并不太适合迁移到其他数据集上。</p><p>在 <code>RandAugment</code> 中，作者提出了一种随机增广的方式，不再像 <code>AutoAugment</code> 中那样使用特定的概率确定是否使用某种子策略，而是所有的子策略都会以同样的概率被选择到，论文中的实验也表明这种数据增广方式即使在大模型的训练中也具有很好的效果。</p><p>结果如 <strong>图5</strong> 所示。</p><p><img src="E:\笔记\markdown\reference\picture\test_randaugment.jpeg" alt="图5 RandAugment后图像可视化"></p><center>图5 RandAugment后图像可视化</center><h3 id="四、图像裁剪类"><a href="#四、图像裁剪类" class="headerlink" title="四、图像裁剪类"></a>四、<a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#id4">图像裁剪类</a></h3><p>图像裁剪类主要是对<code>Transpose</code> 后的 224 的图像进行一些裁剪，并将裁剪区域的像素值置为特定的常数（默认为0），主要包括：</p><ul><li>CutOut[3]</li><li>RandErasing[4]</li><li>HideAndSeek[5]</li><li>GridMask[6]</li></ul><p>图像裁剪的这些增广并非一定要放在归一化之后，也有不少实现是放在归一化之前的，也就是直接对 uint8 的图像进行操作，两种方式的差别是：如果直接对 uint8 的图像进行操作，那么再经过归一化之后被裁剪的区域将不再是纯黑或纯白（减均值除方差之后像素值不为0）。而对归一后之后的数据进行操作，裁剪的区域会是纯黑或纯白。</p><p>上述的裁剪变换思路是相同的，都是为了解决训练出的模型在有遮挡数据上泛化能力较差的问题，不同的是他们的裁剪方式、区域不太一样。</p><h4 id="4-1-Cutout"><a href="#4-1-Cutout" class="headerlink" title="4.1 Cutout"></a>4.1 <a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#cutout">Cutout</a></h4><p>论文地址：<a href="https://arxiv.org/abs/1708.04552">https://arxiv.org/abs/1708.04552</a></p><p>开源代码github地址：<a href="https://github.com/uoguelph-mlrg/Cutout">https://github.com/uoguelph-mlrg/Cutout</a></p><p>Cutout 可以理解为 Dropout 的一种扩展操作，不同的是 Dropout 是对图像经过网络后生成的特征进行遮挡，而 Cutout 是直接对输入的图像进行遮挡，相对于Dropout，Cutout 对噪声的鲁棒性更好。作者在论文中也进行了说明，这样做法有以下两点优势：(1) 通过 Cutout 可以模拟真实场景中主体被部分遮挡时的分类场景；(2) 可以促进模型充分利用图像中更多的内容来进行分类，防止网络只关注显著性的图像区域，从而发生过拟合。</p><p>结果如 <strong>图6</strong> 所示。</p><p><img src="E:\笔记\markdown\reference\picture\test_cutout.jpeg" alt="图6 Cutout后图像可视化"></p><center>图6 Cutout后图像可视化</center><h4 id="4-2-RandomErasing"><a href="#4-2-RandomErasing" class="headerlink" title="4.2 RandomErasing"></a>4.2 <a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#randomerasing">RandomErasing</a></h4><p>论文地址：<a href="https://arxiv.org/pdf/1708.04896.pdf">https://arxiv.org/pdf/1708.04896.pdf</a></p><p>开源代码github地址：<a href="https://github.com/zhunzhong07/Random-Erasing">https://github.com/zhunzhong07/Random-Erasing</a></p><p><code>RandomErasing</code> 与 <code>Cutout</code> 方法类似，同样是为了解决训练出的模型在有遮挡数据上泛化能力较差的问题，作者在论文中也指出，随机裁剪的方式与随机水平翻转具有一定的互补性。作者也在行人再识别（REID）上验证了该方法的有效性。与<code>Cutout</code>不同的是，在<code>RandomErasing</code>中，图片以一定的概率接受该种预处理方法，生成掩码的尺寸大小与长宽比也是根据预设的超参数随机生成。</p><p>结果如 <strong>图7</strong> 所示。</p><p><img src="E:\笔记\markdown\reference\picture\test_randomerassing.jpeg" alt="图7 RandomErasing后图像可视化"></p><center>图7 RandomErasing后图像可视化</center><h4 id="4-3-HideAndSeek"><a href="#4-3-HideAndSeek" class="headerlink" title="4.3 HideAndSeek"></a>4.3 <a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#hideandseek">HideAndSeek</a></h4><p>论文地址：<a href="https://arxiv.org/pdf/1811.02545.pdf">https://arxiv.org/pdf/1811.02545.pdf</a></p><p>开源代码github地址：<a href="https://github.com/kkanshul/Hide-and-Seek">https://github.com/kkanshul/Hide-and-Seek</a></p><p><code>HideAndSeek</code>论文将图像分为若干块区域(patch)，对于每块区域，都以一定的概率生成掩码，不同区域的掩码含义如 <strong>图8</strong> 所示。</p><p><img src="E:\笔记\markdown\reference\picture\hide-and-seek-visual.png" alt="图8 HideAndSeek分块掩码图"></p><center>图8 HideAndSeek分块掩码图</center><p>结果如 <strong>图9</strong> 所示。</p><p><img src="E:\笔记\markdown\reference\picture\test_hideandseek.jpeg" alt="图9 HideAndSeek后图像可视化"></p><center>图9 HideAndSeek后图像可视化</center><h4 id="4-4-GridMask"><a href="#4-4-GridMask" class="headerlink" title="4.4 GridMask"></a>4.4 <a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#gridmask">GridMask</a></h4><p>论文地址：<a href="https://arxiv.org/abs/2001.04086">https://arxiv.org/abs/2001.04086</a></p><p>开源代码github地址：<a href="https://github.com/akuxcw/GridMask">https://github.com/akuxcw/GridMask</a></p><p>作者在论文中指出，此前存在的基于对图像 crop 的方法存在两个问题，如 <strong>图10</strong> 所示：</p><ol><li><p>过度删除区域可能造成目标主体大部分甚至全部被删除，或者导致上下文信息的丢失，导致增广后的数据成为噪声数据；</p></li><li><p>保留过多的区域，对目标主体及上下文基本产生不了什么影响，失去增广的意义。</p></li></ol><p><img src="E:\笔记\markdown\reference\picture\gridmask-0.png" alt="图10 增广后的噪声数据"></p><center>图10 增广后的噪声数据</center><p>因此如果避免过度删除或过度保留成为需要解决的核心问题。</p><p><code>GridMask</code>是通过生成一个与原图分辨率相同的掩码，并将掩码进行随机翻转，与原图相乘，从而得到增广后的图像，通过超参数控制生成的掩码网格的大小。</p><p>在训练过程中，有两种以下使用方法：</p><ol><li>设置一个概率p，从训练开始就对图片以概率p使用<code>GridMask</code>进行增广。</li><li>一开始设置增广概率为0，随着迭代轮数增加，对训练图片进行<code>GridMask</code>增广的概率逐渐增大，最后变为p。</li></ol><p>论文中验证上述第二种方法的训练效果更好一些。</p><p>结果如 <strong>图11</strong> 所示。</p><p><img src="E:\笔记\markdown\reference\picture\test_gridmask.jpeg" alt="图11 GridMask后图像可视化"></p><center>图11 GridMask后图像可视化</center><h3 id="五、图像混叠"><a href="#五、图像混叠" class="headerlink" title="五、图像混叠"></a>五、<a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#id5">图像混叠</a></h3><p>图像混叠主要对 <code>Batch</code> 后的数据进行混合，包括：</p><ul><li>Mixup[7]</li><li>Cutmix[8]</li></ul><p>前文所述的图像变换与图像裁剪都是针对单幅图像进行的操作，而图像混叠是对两幅图像进行融合，生成一幅图像，两种方法的主要区别为混叠的方式不太一样。</p><h4 id="5-1-Mixup"><a href="#5-1-Mixup" class="headerlink" title="5.1 Mixup"></a>5.1 <a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#mixup">Mixup</a></h4><p>论文地址：<a href="https://arxiv.org/pdf/1710.09412.pdf">https://arxiv.org/pdf/1710.09412.pdf</a></p><p>开源代码github地址：<a href="https://github.com/facebookresearch/mixup-cifar10">https://github.com/facebookresearch/mixup-cifar10</a></p><p>Mixup 是最先提出的图像混叠增广方案，其原理简单、方便实现，不仅在图像分类上，在目标检测上也取得了不错的效果。为了便于实现，通常只对一个 batch 内的数据进行混叠，在 <code>Cutmix</code> 中也是如此。</p><p>如下是 <code>imaug</code> 中的实现，需要指出的是，下述实现会出现对同一幅进行相加的情况，也就是最终得到的图和原图一样，随着 <code>batch-size</code> 的增加这种情况出现的概率也会逐渐减小。</p><p>结果如 <strong>图12</strong> 所示。</p><p><img src="E:\笔记\markdown\reference\picture\test_mixup.png" alt="图12 Mixup后图像可视化"></p><center>图12 Mixup后图像可视化</center><h4 id="5-2-Cutmix"><a href="#5-2-Cutmix" class="headerlink" title="5.2 Cutmix"></a>5.2 <a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#cutmix">Cutmix</a></h4><p>论文地址：<a href="https://arxiv.org/pdf/1905.04899v2.pdf">https://arxiv.org/pdf/1905.04899v2.pdf</a></p><p>开源代码github地址：<a href="https://github.com/clovaai/CutMix-PyTorch">https://github.com/clovaai/CutMix-PyTorch</a></p><p>与 <code>Mixup</code> 直接对两幅图进行相加不一样，<code>Cutmix</code> 是从一幅图中随机裁剪出一个 <code>ROI</code>，然后覆盖当前图像中对应的区域。</p><p>结果如 <strong>图13</strong> 所示。</p><p><img src="E:\笔记\markdown\reference\picture\test_cutmix.png" alt="图13 Cutmix后图像可视化"></p><center>图13 Cutmix后图像可视化</center><h3 id="六、实验"><a href="#六、实验" class="headerlink" title="六、实验"></a>六、<a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#id6">实验</a></h3><p>基于PaddleClas套件，使用上述几种数据增广方法在ImageNet1k数据集上进行了实验测试，每个方法的分类精度如下。</p><table><thead><tr><th>模型</th><th>初始学习率策略</th><th>l2 decay</th><th>batch size</th><th>epoch</th><th>数据变化策略</th><th>Top1 Acc</th><th>论文中结论</th></tr></thead><tbody><tr><td>ResNet50</td><td>0.1/cosine_decay</td><td>0.0001</td><td>256</td><td>300</td><td>标准变换</td><td>0.7731</td><td>-</td></tr><tr><td>ResNet50</td><td>0.1/cosine_decay</td><td>0.0001</td><td>256</td><td>300</td><td>AutoAugment</td><td>0.7795</td><td>0.7763</td></tr><tr><td>ResNet50</td><td>0.1/cosine_decay</td><td>0.0001</td><td>256</td><td>300</td><td>mixup</td><td>0.7828</td><td>0.7790</td></tr><tr><td>ResNet50</td><td>0.1/cosine_decay</td><td>0.0001</td><td>256</td><td>300</td><td>cutmix</td><td>0.7839</td><td>0.7860</td></tr><tr><td>ResNet50</td><td>0.1/cosine_decay</td><td>0.0001</td><td>256</td><td>300</td><td>cutout</td><td>0.7801</td><td>-</td></tr><tr><td>ResNet50</td><td>0.1/cosine_decay</td><td>0.0001</td><td>256</td><td>300</td><td>gridmask</td><td>0.7785</td><td>0.7790</td></tr><tr><td>ResNet50</td><td>0.1/cosine_decay</td><td>0.0001</td><td>256</td><td>300</td><td>random-augment</td><td>0.7770</td><td>0.7760</td></tr><tr><td>ResNet50</td><td>0.1/cosine_decay</td><td>0.0001</td><td>256</td><td>300</td><td>random erasing</td><td>0.7791</td><td>-</td></tr><tr><td>ResNet50</td><td>0.1/cosine_decay</td><td>0.0001</td><td>256</td><td>300</td><td>hide and seek</td><td>0.7743</td><td>0.7720</td></tr></tbody></table><p><strong>注意</strong>：</p><ul><li>在实验中，为了便于对比，将l2 decay固定设置为1e-4，在实际使用中，推荐尝试使用更小的l2 decay。结合数据增广，发现将l2 decay由1e-4减小为7e-5均能带来至少0.3~0.5%的精度提升。</li><li>在使用数据增广后，由于训练数据更难，所以训练损失函数可能较大，训练集的准确率相对较低，但其拥有更好的泛化能力，所以验证集的准确率相对较高。</li><li>在使用数据增广后，模型可能会趋于欠拟合状态，建议可以适当的调小<code>l2_decay</code>的值来获得更高的验证集准确率。</li></ul><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a><a href="https://paddlepedia.readthedocs.io/en/latest/tutorials/computer_vision/image_augmentation/ImageAugment.html#id7">参考文献</a></h3><p>[1] <a href="https://arxiv.org/abs/1805.09501v1">Autoaugment: Learning augmentation strategies from data</a></p><p>[2] <a href="https://arxiv.org/pdf/1909.13719.pdf">Randaugment: Practical automated data augmentation with a reduced search space</a></p><p>[3] <a href="https://arxiv.org/abs/1708.04552">Improved regularization of convolutional neural networks with cutout</a></p><p>[4] <a href="https://arxiv.org/pdf/1708.04896.pdf">Random erasing data augmentation</a></p><p>[5] <a href="https://arxiv.org/pdf/1811.02545.pdf">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</a></p><p>[6] <a href="https://arxiv.org/abs/2001.04086">GridMask Data Augmentation</a></p><p>[7] <a href="https://arxiv.org/pdf/1710.09412.pdf">mixup: Beyond empirical risk minimization</a></p><p>[8] <a href="https://arxiv.org/pdf/1905.04899v2.pdf">Cutmix: Regularization strategy to train strong classifiers with localizable features</a>)</p><h3 id="实验中用到的增强方法总结"><a href="#实验中用到的增强方法总结" class="headerlink" title="实验中用到的增强方法总结"></a>实验中用到的增强方法总结</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># transforms 类</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transforms</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        transform= [</span><br><span class="line">                transforms.RandomResizedCrop(size=args.image_size,scale=(<span class="number">0.2</span>, <span class="number">1.0</span>),ratio=(<span class="number">3</span> / <span class="number">4</span>, <span class="number">4</span> / <span class="number">3</span>)),</span><br><span class="line">                transforms.ColorJitter(brightness=<span class="number">0.4</span>, contrast=<span class="number">0.4</span>, saturation=<span class="number">0.4</span>),</span><br><span class="line">                transforms.RandomGrayscale(p=<span class="number">0.2</span>),</span><br><span class="line">                transforms.ToTensor(),</span><br><span class="line">                transforms.Normalize(mean=[<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>],std=[<span class="number">0.2470</span>, <span class="number">0.2435</span>, <span class="number">0.2616</span>])</span><br><span class="line">        ]</span><br><span class="line">        self.train_transform = transforms.Compose(transform)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.train_transform(x),self.train_transform(x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 调用方法一</span></span><br><span class="line">dataset = CIFAR100(root=args.dataset_root,train=<span class="literal">False</span>,download=<span class="literal">True</span>,transform=Transforms())</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;PIL&quot;&gt;&lt;a href=&quot;#PIL&quot; class=&quot;headerlink&quot; title=&quot;PIL&quot;&gt;&lt;/a&gt;PIL&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;  PIL，全称 Python Imaging Library，是一个功能非常强大而且简单易用的图像处理库</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>特色包</title>
    <link href="https://jpccc.github.io/2022/04/07/%E7%89%B9%E8%89%B2%E5%8C%85/"/>
    <id>https://jpccc.github.io/2022/04/07/%E7%89%B9%E8%89%B2%E5%8C%85/</id>
    <published>2022-04-07T10:10:44.000Z</published>
    <updated>2022-04-10T04:05:07.816Z</updated>
    
    <content type="html"><![CDATA[<h1 id="控制台颜色"><a href="#控制台颜色" class="headerlink" title="控制台颜色"></a>控制台颜色</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> termcolor <span class="keyword">import</span> colored</span><br><span class="line"><span class="built_in">print</span>(colored(<span class="string">&#x27;Fill memory bank for mining the nearest neighbors (train) ...&#x27;</span>, <span class="string">&#x27;blue&#x27;</span>))</span><br></pre></td></tr></table></figure><h1 id="Logging"><a href="#Logging" class="headerlink" title="Logging"></a>Logging</h1><blockquote><p>  This module defines functions and classes which implement a flexible event logging system for applications and libraries.</p><p>  Python logging 模块定义了为应用程序和库实现灵活的事件日志记录的函数和类。</p></blockquote><p>程序开发过程中，很多程序都有记录日志的需求，并且日志包含的信息有正常的程序访问日志还可能有错误、警告等信息输出，Python 的 logging 模块提供了标准的日志接口，可以通过它存储各种格式的日志,日志记录提供了一组便利功能，用于简单的日志记录用法。</p><ul><li>  使用 Python Logging 模块的主要好处是所有 Python 模块都可以参与日志记录</li><li>  Logging 模块提供了大量具有灵活性的功能</li></ul><p><strong>日志记录函数以它们用来跟踪的事件的级别或严重性命名。下面描述了标准级别及其适用性（从高到低的顺序）：</strong></p><table><thead><tr><th>日志等级(level)</th><th>描述</th></tr></thead><tbody><tr><td></td><td></td></tr></tbody></table><p><strong>日志级别等级排序</strong>：critical &gt; error &gt; warning &gt; info &gt; debug</p><p><strong>级别越高打印的日志越少，反之亦然，即</strong></p><ul><li>  debug : 打印全部的日志( notset 等同于 debug )</li><li>  info : 打印 info, warning, error, critical 级别的日志</li><li>  warning : 打印 warning, error, critical 级别的日志</li><li>  error : 打印 error, critical 级别的日志</li><li>  critical : 打印 critical 级别</li></ul><h2 id="一、-Logging-模块日志记录方式"><a href="#一、-Logging-模块日志记录方式" class="headerlink" title="一、 Logging 模块日志记录方式"></a><strong>一、 Logging 模块日志记录方式</strong></h2><p>Logging 模块提供了两种日志记录方式：</p><ul><li>  一种方式是使用 Logging 提供的模块级别的函数</li><li>  另一种方式是使用 Logging 日志系统的四大组件记录</li></ul><h2 id="1、Logging-定义的模块级别函数"><a href="#1、Logging-定义的模块级别函数" class="headerlink" title="1、Logging 定义的模块级别函数"></a><strong>1、Logging 定义的模块级别函数</strong></h2><table><thead><tr><th>函数</th><th>说明</th></tr></thead></table><p>简单打印日志：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印日志级别</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_logging</span>():</span></span><br><span class="line">    logging.debug(<span class="string">&#x27;Python debug&#x27;</span>)</span><br><span class="line">    logging.info(<span class="string">&#x27;Python info&#x27;</span>)</span><br><span class="line">    logging.warning(<span class="string">&#x27;Python warning&#x27;</span>)</span><br><span class="line">    logging.error(<span class="string">&#x27;Python Error&#x27;</span>)</span><br><span class="line">    logging.critical(<span class="string">&#x27;Python critical&#x27;</span>)</span><br><span class="line"></span><br><span class="line">test_logging()</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">WARNING:root:Python warning</span><br><span class="line">ERROR:root:Python Error</span><br><span class="line">CRITICAL:root:Python critical</span><br></pre></td></tr></table></figure><p>当指定一个日志级别之后，会记录大于或等于这个日志级别的日志信息，小于的将会被丢弃， ==默认情况下日志打印只显示大于等于 WARNING 级别的日志。==</p><h3 id="1-1-设置日志显示级别"><a href="#1-1-设置日志显示级别" class="headerlink" title="1.1 设置日志显示级别"></a><strong>1.1 设置日志显示级别</strong></h3><p>通过 logging.basicConfig() 可以设置 root 的日志级别，和日志输出格式。</p><p><strong>logging.basicConfig() 关键字参数</strong>：</p><table><thead><tr><th>关键字</th><th>描述</th></tr></thead><tbody><tr><td></td><td></td></tr></tbody></table><p><strong>format 格式</strong></p><table><thead><tr><th>格式</th><th>描述</th></tr></thead><tbody><tr><td></td><td></td></tr></tbody></table><p><strong>注意</strong>：Logging.basicConfig() 需要在开头就设置，在中间设置并无作用</p><p><strong>实例</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import logging</span><br><span class="line"></span><br><span class="line"># 打印日志级别</span><br><span class="line">def test():</span><br><span class="line">    logging.basicConfig(level=logging.DEBUG)</span><br><span class="line">    logging.debug(&#x27;Python debug&#x27;)</span><br><span class="line">    logging.info(&#x27;Python info&#x27;)</span><br><span class="line">    logging.warning(&#x27;Python warning&#x27;)</span><br><span class="line">    logging.error(&#x27;Python Error&#x27;)</span><br><span class="line">    logging.critical(&#x27;Python critical&#x27;)</span><br><span class="line">    logging.log(2,&#x27;test&#x27;)</span><br><span class="line">test()</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DEBUG:root:Python debug</span><br><span class="line">INFO:root:Python info</span><br><span class="line">WARNING:root:Python warning</span><br><span class="line">ERROR:root:Python Error</span><br><span class="line">CRITICAL:root:Python critical</span><br></pre></td></tr></table></figure><h3 id="1-2-将日志信息记录到文件"><a href="#1-2-将日志信息记录到文件" class="headerlink" title="1.2 将日志信息记录到文件"></a><strong>1.2 将日志信息记录到文件</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 日志信息记录到文件</span><br><span class="line">logging.basicConfig(filename=&#x27;F:/example.log&#x27;, level=logging.DEBUG)</span><br><span class="line">logging.debug(&#x27;This message should go to the log file&#x27;)</span><br><span class="line">logging.info(&#x27;So should this&#x27;)</span><br><span class="line">logging.warning(&#x27;And this, too&#x27;)</span><br></pre></td></tr></table></figure><p>在相应的路径下会有 example.log 日志文件，内容如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DEBUG:root:This message should go to the log file</span><br><span class="line">INFO:root:So should this</span><br><span class="line">WARNING:root:And this, too</span><br></pre></td></tr></table></figure><h3 id="1-3-多个模块记录日志信息"><a href="#1-3-多个模块记录日志信息" class="headerlink" title="1.3 多个模块记录日志信息"></a><strong>1.3 多个模块记录日志信息</strong></h3><p>如果程序包含多个模块，则用以下实例来显示日志信息： 实例中有两个模块，一个模块通过导入另一个模块的方式用日志显示另一个模块的信息：</p><p><strong>myapp.py 模块</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import logging</span><br><span class="line">import mylib</span><br><span class="line">def main():</span><br><span class="line">    logging.basicConfig(filename=&#x27;myapp.log&#x27;,level=logging.DEBUG)</span><br><span class="line">    logging.info(&#x27;Started&#x27;)</span><br><span class="line">    mylib.do_something()</span><br><span class="line">    logging.info(&#x27;Finished&#x27;)</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p><strong>mylib.py 模块</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import logging</span><br><span class="line"></span><br><span class="line">def do_something():</span><br><span class="line">    logging.info(&#x27;Doing something&#x27;)</span><br></pre></td></tr></table></figure><p>执行 myapp.py 模块会打印相应日志，在文件 myapp.log 中显示信息如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">INFO:root:Started</span><br><span class="line">INFO:root:Doing something</span><br><span class="line">INFO:root:Finishe</span><br></pre></td></tr></table></figure><h3 id="1-4-显示信息的日期及更改显示消息格式"><a href="#1-4-显示信息的日期及更改显示消息格式" class="headerlink" title="1.4 显示信息的日期及更改显示消息格式"></a><strong>1.4 显示信息的日期及更改显示消息格式</strong></h3><p><strong>显示消息日期</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import logging</span><br><span class="line"># 显示消息时间</span><br><span class="line">logging.basicConfig(format=&#x27;%(asctime)s %(message)s&#x27;)</span><br><span class="line">logging.warning(&#x27;is when this event was logged.&#x27;)</span><br><span class="line"></span><br><span class="line">logging.basicConfig(format=&#x27;%(asctime)s %(message)s&#x27;, datefmt=&#x27;%m/%d/%Y %I:%M:%S %p&#x27;)</span><br><span class="line">logging.warning(&#x27;is when this event was logged.&#x27;)</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2019-10-16 18:57:45,988 is when this event was logged.</span><br><span class="line">2019-10-16 18:57:45,988 is when this event was logged.</span><br></pre></td></tr></table></figure><p><strong>更改显示消息格式</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import logging</span><br><span class="line"># 更改显示消息的格式</span><br><span class="line">logging.basicConfig(format=&#x27;%(levelname)s:%(message)s&#x27;,level=logging.DEBUG)</span><br><span class="line">logging.debug(&#x27;Python message format Debug&#x27;)</span><br><span class="line">logging.info(&#x27;Python message format Info&#x27;)</span><br><span class="line">logging.warning(&#x27;Python message format Warning&#x27;)</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DEBUG:Python message format Debug</span><br><span class="line">INFO:Python message format Info</span><br><span class="line">WARNING:Python message format Warning</span><br></pre></td></tr></table></figure><p>==注意==：显示结果只显示级别和具体信息，之前显示的 “根” 已经消失，重新定义的格式修改了默认输出方式。</p><h2 id="2、logging-模块四大组件"><a href="#2、logging-模块四大组件" class="headerlink" title="2、logging 模块四大组件"></a><strong>2、logging 模块四大组件</strong></h2><table><thead><tr><th>组件名称</th><th>对应类名</th><th>功能描述</th></tr></thead><tbody><tr><td></td><td></td><td></td></tr></tbody></table><h3 id="2-1-日志器-Logger"><a href="#2-1-日志器-Logger" class="headerlink" title="2.1 日志器- Logger"></a><strong>2.1 日志器- Logger</strong></h3><p>Logger 持有日志记录器的方法，日志记录器不直接实例化，而是通过模块级函数 logger.getlogger (name) 来实例化,使用相同的名称多次调用 getLogger() 总是会返回对相同 Logger 对象的引用。</p><ul><li>  应用程序代码能直接调用日志接口。</li><li>  Logger最常用的操作有两类：配置和发送日志消息。</li><li>  初始化 logger = logging.getLogger(“endlesscode”)，获取 logger 对象，getLogger() 方法后面最好加上所要日志记录的模块名字，配置文件和打印日志格式中的 %(name)s 对应的是这里的模块名字，如果不指定name则返回root对象。</li><li>  logger.setLevel(logging.DEBUG)，Logging 中有 NOTSET &lt; DEBUG &lt; INFO &lt; WARNING &lt; ERROR &lt; CRITICAL这几种级别，日志会记录设置级别以上的日志</li><li>  多次使用相同的name调用 getLogger 方法返回同一个 looger 对象；</li></ul><p>Logger是一个树形层级结构，在使用接口 debug，info，warn，error，critical 之前必须创建 Logger 实例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">创建方法: logger = logging.getLogger(logger_name)</span><br></pre></td></tr></table></figure><p>创建Logger实例后，可以使用以下方法进行日志级别设置，增加处理器 Handler：</p><ul><li>  logger.setLevel(logging.ERROR) # 设置日志级别为 ERROR，即只有日志级别大于等于 ERROR 的日志才会输出</li><li>  logger.addHandler(handler_name) # 为 Logger 实例增加一个处理器</li><li>  logger.removeHandler(handler_name) # 为 Logger 实例删除一个处理器</li></ul><h3 id="2-2-处理器-Handler"><a href="#2-2-处理器-Handler" class="headerlink" title="2.2 处理器- Handler"></a><strong>2.2 处理器- Handler</strong></h3><p>Handler 处理器类型有很多种，比较常用的有三个，StreamHandler，FileHandler，NullHandler</p><p><strong>StreamHandler</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">创建方法：sh = logging.StreamHandler(stream=None)</span><br></pre></td></tr></table></figure><p>创建 StreamHandler 之后，可以通过使用以下方法设置日志级别，设置格式化器 Formatter，增加或删除过滤器 Filter：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ch.setLevel(logging.WARN) # 指定日志级别，低于WARN级别的日志将被忽略</span><br><span class="line"></span><br><span class="line">ch.setFormatter(formatter_name) # 设置一个格式化器formatter</span><br><span class="line"></span><br><span class="line">ch.addFilter(filter_name) # 增加一个过滤器，可以增加多个</span><br><span class="line"> </span><br><span class="line">ch.removeFilter(filter_name) # 删除一个过滤器</span><br></pre></td></tr></table></figure><h3 id="2-3-过滤器-Filter"><a href="#2-3-过滤器-Filter" class="headerlink" title="2.3 过滤器- Filter"></a><strong>2.3 过滤器- Filter</strong></h3><p>Handlers 和 Loggers 可以使用 Filters 来完成比级别更复杂的过滤。 Filter 基类只允许特定 Logger 层次以下的事件。 例如用 ‘A.B’ 初始化的 Filter 允许Logger ‘A.B’, ‘A.B.C’, ‘A.B.C.D’, ‘A.B.D’ 等记录的事件，logger‘A.BB’, ‘B.A.B’ 等就不行。 如果用空字符串来初始化，所有的事件都接受。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">创建方法: filter = logging.Filter(name=&#x27;&#x27;)</span><br></pre></td></tr></table></figure><h3 id="2-4-格式器-Formatter"><a href="#2-4-格式器-Formatter" class="headerlink" title="2.4 格式器- Formatter"></a><strong>2.4 格式器- Formatter</strong></h3><p>使用Formatter对象设置日志信息最后的规则、结构和内容，默认的时间格式为%Y-%m-%d %H:%M:%S。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">创建方法: formatter = logging.Formatter(fmt=None, datefmt=None)</span><br></pre></td></tr></table></figure><p>其中，fmt 是消息的格式化字符串，datefmt 是日期字符串。如果不指明 fmt，将使用 ‘%(message)s’ 。如果不指明 datefmt，将使用 ISO8601 日期格式。</p><h3 id="2-5-组件之间的关联关系"><a href="#2-5-组件之间的关联关系" class="headerlink" title="2.5 组件之间的关联关系"></a><strong>2.5 组件之间的关联关系</strong></h3><ul><li>  日志器（logger）需要通过处理器（handler）将日志信息输出到目标位置，不同的处理器（handler）可以将日志输出到不同的位置；</li><li>  日志器（logger）可以设置多个处理器（handler）将同一条日志记录输出到不同的位置；</li><li>  每个处理器（handler）都可以设置自己的过滤器（filter）实现日志过滤，从而只保留感兴趣的日志；</li><li>  每个处理器（handler）都可以设置自己的格式器（formatter）实现同一条日志以不同的格式输出到不同的地方。</li></ul><p>简明了说就是：日志器（logger）是入口，真正干活儿的是处理器（handler），处理器（handler）还可以通过过滤器（filter）和格式器（formatter）对要输出的日志内容做过滤和格式化等处理操作。</p><ul><li>  Logger 可以包含一个或多个 Handler 和 Filter</li><li>  Logger 与 Handler 或 Fitler 是一对多的关系</li><li>  一个 Logger 实例可以新增多 个 Handler，一个 Handler 可以新增多个格式化器或多个过滤器，而且日志级别将会继承。</li></ul><h2 id="二、Logging-日志工作流程"><a href="#二、Logging-日志工作流程" class="headerlink" title="二、Logging 日志工作流程"></a><strong>二、Logging 日志工作流程</strong></h2><h2 id="1、Logging-模块使用过程"><a href="#1、Logging-模块使用过程" class="headerlink" title="1、Logging 模块使用过程"></a><strong>1、Logging 模块使用过程</strong></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1）第一次导入 logging 模块或使用 reload 函数重新导入 logging 模块，logging 模块中的代码将被执行，这个过程中将产生 logging 日志系统的默认配置。</span><br><span class="line"></span><br><span class="line">2）自定义配置(可选),logging标准模块支持三种配置方式: dictConfig，fileConfig，listen。其中，dictConfig 是通过一个字典进行配置 Logger，Handler，Filter，Formatter；fileConfig 则是通过一个文件进行配置；而 listen 则监听一个网络端口，通过接收网络数据来进行配置。当然，除了以上集体化配置外，也可以直接调用 Logger，Handler 等对象中的方法在代码中来显式配置。</span><br><span class="line"></span><br><span class="line">3）使用 logging 模块的全局作用域中的 getLogger 函数来得到一个 Logger 对象实例(其参数即是一个字符串，表示 Logger 对象实例的名字，即通过该名字来得到相应的 Logger 对象实例)。</span><br><span class="line"></span><br><span class="line">4）使用 Logger 对象中的 debug，info，error，warn，critical 等方法记录日志信息。</span><br></pre></td></tr></table></figure><h2 id="2、Logging-模块处理流程"><a href="#2、Logging-模块处理流程" class="headerlink" title="2、Logging 模块处理流程"></a><strong>2、Logging 模块处理流程</strong></h2><p>流程描述：</p><ol><li> 判断日志的等级是否大于 Logger 对象的等级，如果大于，则往下执行，否则，流程结束。</li><li> 产生日志：第一步，判断是否有异常，如果有，则添加异常信息。 第二步，处理日志记录方法(如 debug，info 等)中的占位符，即一般的字符串格式化处理。</li><li> 使用注册到 Logger 对象中的 Filters 进行过滤。如果有多个过滤器，则依次过滤；只要有一个过滤器返回假，则过滤结束，且该日志信息将丢弃，不再处理，而处理流程也至此结束。否则，处理流程往下执行。</li><li> 在当前 Logger 对象中查找 Handlers，如果找不到任何 Handler，则往上到该 Logger 对象的父 Logger 中查找；如果找到一个或多个 Handler，则依次用 Handler 来处理日志信息。但在每个 Handler 处理日志信息过程中，会首先判断日志信息的等级是否大于该 Handler 的等级，如果大于，则往下执行(由 Logger 对象进入 Handler 对象中)，否则，处理流程结束。</li><li> 执行 Handler 对象中的 filter 方法，该方法会依次执行注册到该 Handler 对象中的 Filter。如果有一个 Filter 判断该日志信息为假，则此后的所有 Filter 都不再执行，而直接将该日志信息丢弃，处理流程结束。</li><li> 使用 Formatter 类格式化最终的输出结果。 注：Formatter 同上述第 2 步的字符串格式化不同，它会添加额外的信息，比如日志产生的时间，产生日志的源代码所在的源文件的路径等等。</li><li> 真正地输出日志信息(到网络，文件，终端，邮件等)。至于输出到哪个目的地，由 Handler 的种类来决定。</li></ol><h2 id="三、配置日志"><a href="#三、配置日志" class="headerlink" title="三、配置日志"></a><strong>三、配置日志</strong></h2><p>程序员可以通过三种方式配置日志记录：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1、使用配置方法的 Python 代码显式创建记录器，处理程序和格式化程序。</span><br><span class="line"></span><br><span class="line">2、创建日志记录配置文件并使用该 fileConfig() 功能读取它。</span><br><span class="line"></span><br><span class="line">3、创建配置信息字典并将其传递给 dictConfig()函数。</span><br></pre></td></tr></table></figure><p>下面使用 Python 代码配置一个非常简单的记录器，一个控制台处理程序和一个简单的格式化程序：</p><p><strong>logging.conf 配置文件</strong>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[loggers]</span><br><span class="line">keys=root,simpleExample</span><br><span class="line"></span><br><span class="line">[handlers]</span><br><span class="line">keys=consoleHandler</span><br><span class="line"></span><br><span class="line">[formatters]</span><br><span class="line">keys=simpleFormatter</span><br><span class="line"></span><br><span class="line">[logger_root]</span><br><span class="line">level=DEBUG</span><br><span class="line">handlers=consoleHandler</span><br><span class="line"></span><br><span class="line">[logger_simpleExample]</span><br><span class="line">level=DEBUG</span><br><span class="line">handlers=consoleHandler</span><br><span class="line">qualname=simpleExample</span><br><span class="line">propagate=0</span><br><span class="line"></span><br><span class="line">[handler_consoleHandler]</span><br><span class="line">class=StreamHandler</span><br><span class="line">level=DEBUG</span><br><span class="line">formatter=simpleFormatter</span><br><span class="line">args=(sys.stdout,)</span><br><span class="line"></span><br><span class="line">[formatter_simpleFormatter]</span><br><span class="line">format=%(asctime)s - %(name)s - %(levelname)s - %(message)s</span><br><span class="line">datefmt=</span><br></pre></td></tr></table></figure><p><strong>config_logging.py 配置器</strong>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import logging</span><br><span class="line"># create logger</span><br><span class="line">logger = logging.getLogger(&#x27;simple_example&#x27;)</span><br><span class="line">logger.setLevel(logging.DEBUG)</span><br><span class="line"></span><br><span class="line"># create console handler and set level to debug</span><br><span class="line">ch = logging.StreamHandler()</span><br><span class="line">ch.setLevel(logging.DEBUG)</span><br><span class="line"></span><br><span class="line"># create formatter</span><br><span class="line">formatter = logging.Formatter(&#x27;%(asctime)s - %(name)s - %(levelname)s - %(message)s&#x27;)</span><br><span class="line"></span><br><span class="line"># add formatter to ch</span><br><span class="line">ch.setFormatter(formatter)</span><br><span class="line"></span><br><span class="line"># add ch to logger</span><br><span class="line">logger.addHandler(ch)</span><br><span class="line"></span><br><span class="line"># &#x27;application&#x27; code</span><br><span class="line">logger.debug(&#x27;debug message&#x27;)</span><br><span class="line">logger.info(&#x27;info message&#x27;)</span><br><span class="line">logger.warning(&#x27;warn message&#x27;)</span><br><span class="line">logger.error(&#x27;error message&#x27;)</span><br><span class="line">logger.critical(&#x27;critical message&#x27;)</span><br></pre></td></tr></table></figure><p><strong>recorder 记录器</strong>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import logging</span><br><span class="line">import logging.config</span><br><span class="line"></span><br><span class="line">logging.config.fileConfig(&#x27;logging.conf&#x27;)</span><br><span class="line"></span><br><span class="line"># create logger</span><br><span class="line">logger = logging.getLogger(&#x27;simpleExample&#x27;)</span><br><span class="line"></span><br><span class="line"># &#x27;application&#x27; code</span><br><span class="line">logger.debug(&#x27;debug message&#x27;)</span><br><span class="line">logger.info(&#x27;info message&#x27;)</span><br><span class="line">logger.warning(&#x27;warn message&#x27;)</span><br><span class="line">logger.error(&#x27;error message&#x27;)</span><br><span class="line">logger.critical(&#x27;critical message&#x27;)</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2019-10-16 19:45:34,440 - simple_example - DEBUG - debug message</span><br><span class="line">2019-10-16 19:45:34,440 - simple_example - INFO - info message</span><br><span class="line">2019-10-16 19:45:34,440 - simple_example - WARNING - warn message</span><br><span class="line">2019-10-16 19:45:34,440 - simple_example - ERROR - error message</span><br><span class="line">2019-10-16 19:45:34,441 - simple_example - CRITICAL - critical message</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><p>本章节给大家介绍了 Python 标准库中 Logging 模块的详细介绍与使用，对 Python 工程师使用该模块提供更好的支撑</p><p>参考： <a href="https://link.zhihu.com/?target=https://docs.python.org/3.6/library/logging.html?highlight=logging%23integration-with-the-warnings-module">https://docs.python.org/3.6/library/logging.html?highlight=logging#integration-with-the-warnings-module</a> <a href="https://link.zhihu.com/?target=https://www.jianshu.com/p/feb86c06c4f4">https://www.jianshu.com/p/feb86c06c4f4</a><br><a href="https://link.zhihu.com/?target=https://cloud.tencent.com/developer/article/1354396">https://cloud.tencent.com/devel</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;控制台颜色&quot;&gt;&lt;a href=&quot;#控制台颜色&quot; class=&quot;headerlink&quot; title=&quot;控制台颜色&quot;&gt;&lt;/a&gt;控制台颜色&lt;/h1&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter</summary>
      
    
    
    
    
    <category term="python" scheme="https://jpccc.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python语法</title>
    <link href="https://jpccc.github.io/2022/04/07/python%E8%AF%AD%E6%B3%95/"/>
    <id>https://jpccc.github.io/2022/04/07/python%E8%AF%AD%E6%B3%95/</id>
    <published>2022-04-07T08:10:44.000Z</published>
    <updated>2022-05-11T08:44:35.334Z</updated>
    
    <content type="html"><![CDATA[<h2 id="矩阵的操作"><a href="#矩阵的操作" class="headerlink" title="矩阵的操作"></a>矩阵的操作</h2><h3 id="view-和reshape"><a href="#view-和reshape" class="headerlink" title="view()和reshape"></a>view()和reshape</h3><p>view操作后的变量还是连续的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]],</span><br><span class="line">                  [[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],[<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]]</span><br><span class="line">                 ])</span><br><span class="line">b = a.view(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">b = tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>],</span><br><span class="line">        [<span class="number">5</span>,  <span class="number">6</span> ,<span class="number">7</span>,<span class="number">8</span>],</span><br><span class="line">        [<span class="number">9</span>,<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br></pre></td></tr></table></figure><p>张量a的size是2x2x3，使用view函数后，先将这12个元素排成一列，然后将其依次填充至新的4x3的张量中：</p><p><img src="E:\笔记\markdown\reference\picture\v2-acfb79d45817e3080ddcac7666d61f09_b.jpg" alt="img"></p><p>为了更细致，我们需要描述一下它们的具体操作流程（这并不是源码的流程，只是为了便于理解、记忆），因为二维比较直观，如果维度比较高的话，可能还是不够直观，心里老是有疑虑，还是以上面例子为例，张量a的每一个元素都有一个index，例如1的index是(0,0,0)，7的index是(1,0,0)，11的index是(1,1,1)……在拉成列向量排列时，排列规则是这样的，以(0,0,0)开始，维度从最后一维循环到第一维，在每一维内以升序将所有元素排成一列，即：</p><p><img src="E:\笔记\markdown\reference\picture\v2-0c59291b64850293305ab879087ea32d_b.jpg" alt="img"></p><p>然后依据新的size对每个元素给予新的index，仍然是维度从最后一维循环到第一维，在每一维内升序，即:</p><p><img src="E:\笔记\markdown\reference\picture\v2-ab9112d1c2f5bb21d1b3d164bffa5e95_b.jpg" alt="img"></p><blockquote><p>  torch的view()与reshape()方法都可以用来重塑tensor的shape，区别就是使用的条件不一样。view()方法只适用于满足连续性条件的tensor，并且该操作不会开辟新的内存空间，只是产生了对原存储空间的一个新别称和引用，返回值是视图。而reshape()方法的返回值既可以是视图，也可以是副本，当满足连续性条件时返回view，否则返回副本[ 此时等价于先调用contiguous()方法在使用view() ]。因此当不确能否使用view时，可以使用reshape。如果只是想简单地重塑一个tensor的shape，那么就是用reshape，但是如果需要考虑内存的开销而且要确保重塑后的tensor与之前的tensor共享存储空间，那就使用view()。</p></blockquote><h3 id="permute-和transpose"><a href="#permute-和transpose" class="headerlink" title="permute()和transpose()"></a>permute()和transpose()</h3><p>transpose()也是转置操作，与permute不同的是，transpose()每次只能进行二维转置，而permute()每次可转置多个维度().</p><p>接下来，说一下permute()，函数的参数为新的维度顺序，例如想交换第一维与第三维的index，则<a href="https://www.zhihu.com/search?q=tensor.permute&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:%2288311093%22%7D">tensor.permute</a>(2,1,0)，同样举一个简单第二维与第三维的例子，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]],[[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>],[<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]]])</span><br><span class="line">b = a.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">b = tensor([[[ <span class="number">1</span>,  <span class="number">4</span>],</span><br><span class="line">         [ <span class="number">2</span>,  <span class="number">5</span>],</span><br><span class="line">         [ <span class="number">3</span>,  <span class="number">6</span>]],</span><br><span class="line">        [[ <span class="number">7</span>, <span class="number">10</span>],</span><br><span class="line">         [ <span class="number">8</span>, <span class="number">11</span>],</span><br><span class="line">         [ <span class="number">9</span>, <span class="number">12</span>]]])</span><br></pre></td></tr></table></figure><p><img src="E:\笔记\markdown\reference\picture\v2-6bdd87e65b64534339d3efad9ae4eae5_b.jpg" alt="img"></p><p>当维度比较大，交换维度的结果并不直观，我们还是说下permute到底做了什么，示意图如下。</p><p>需要说明一下，这里面的流程其实就是更改每个元素的index而已，上述例子中只是交换坐标，</p><p><img src="E:\笔记\markdown\reference\picture\v2-ec4167d8c19713e232ab3f5ba35ed10d_b.jpg" alt="img"></p><p>然后按照新的index更改下各元素的位置并展示出即可。</p><h3 id="squeeze-和unsqueeze-函数功能"><a href="#squeeze-和unsqueeze-函数功能" class="headerlink" title="squeeze()和unsqueeze()函数功能"></a>squeeze()和unsqueeze()函数功能</h3><h4 id="1-squeeze-dim-："><a href="#1-squeeze-dim-：" class="headerlink" title="1.squeeze(dim)："></a>1.squeeze(dim)：</h4><p>给张量tensor降维，但不是啥张量都可以用这两个函数来降维。dim指定需要降维的维度，这个维度的值必须为1才能被降维。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a=torch.rand(<span class="number">2</span>,<span class="number">1</span>,<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(a.shape)</span><br><span class="line"><span class="built_in">print</span>(a.squeeze(<span class="number">1</span>).shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#torch.Size([2, 1, 4, 1])</span></span><br><span class="line"><span class="comment">#torch.Size([2, 4, 1])</span></span><br></pre></td></tr></table></figure><p>2.unsqueeze(dim)</p><p>与squeeze(dim)相反，在dim维度上添加一个维度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a=torch.rand(<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(a.shape)</span><br><span class="line"><span class="built_in">print</span>(a.unsqueeze(<span class="number">1</span>).shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#torch.Size([2, 4])</span></span><br><span class="line"><span class="comment">#torch.Size([2, 1, 4])</span></span><br></pre></td></tr></table></figure><h2 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h2><p>在 Python 中，* 和 ** 具有语法多义性，具体来说是有四类用法。</p><span id="more"></span><ol><li>算数运算</li></ol><p>​    *代表乘法</p><p>​    **代表乘方</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">2</span> * <span class="number">5</span></span><br><span class="line"><span class="number">10</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">2</span> ** <span class="number">5</span></span><br><span class="line"><span class="number">32</span></span><br></pre></td></tr></table></figure><ol start="2"><li>函数形参</li></ol><p>*args 和 **kwargs 主要用于函数定义。</p><p>你可以将不定数量的参数传递给一个函数。不定的意思是：预先并不知道, 函数使用者会传递多少个参数给你, 所以在这个场景下使用这两个关键字。其实并不是必须写成 *args 和 **kwargs。  *(星号) 才是必须的. 你也可以写成 *ar  和 **k 。而写成 *args 和**kwargs 只是一个通俗的命名约定。</p><ul><li>  python函数传递参数的方式有两种：</li></ul><p>​                位置参数（positional argument）</p><p>​                关键词参数（keyword argument）</p><ul><li>  *args 与 **kwargs 的区别，两者都是 python 中的可变参数：</li></ul><p>​                *args 表示任何多个无名参数，它本质是一个 tuple<br>​                        ** kwargs 表示关键字参数，它本质上是一个 dict<br>如果同时使用 *args 和 **kwargs 时，必须 *args 参数列要在 **kwargs 之前。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">fun</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line"><span class="meta">... </span>    <span class="built_in">print</span>(<span class="string">&#x27;args=&#x27;</span>, args)</span><br><span class="line"><span class="meta">... </span>    <span class="built_in">print</span>(<span class="string">&#x27;kwargs=&#x27;</span>, kwargs)</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fun(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, A=<span class="string">&#x27;a&#x27;</span>, B=<span class="string">&#x27;b&#x27;</span>, C=<span class="string">&#x27;c&#x27;</span>, D=<span class="string">&#x27;d&#x27;</span>)</span><br><span class="line">args= (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">kwargs= &#123;<span class="string">&#x27;A&#x27;</span>: <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;B&#x27;</span>: <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;C&#x27;</span>: <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;D&#x27;</span>: <span class="string">&#x27;d&#x27;</span>&#125;</span><br></pre></td></tr></table></figure><p>使用*args：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">fun</span>(<span class="params">name, *args</span>):</span></span><br><span class="line"><span class="meta">... </span>    <span class="built_in">print</span>(<span class="string">&#x27;你好:&#x27;</span>, name)</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">for</span> i <span class="keyword">in</span> args:</span><br><span class="line"><span class="meta">... </span>        <span class="built_in">print</span>(<span class="string">&quot;你的宠物有:&quot;</span>, i)</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fun(<span class="string">&quot;Geek&quot;</span>, <span class="string">&quot;dog&quot;</span>, <span class="string">&quot;cat&quot;</span>)</span><br><span class="line">你好: Geek</span><br><span class="line">你的宠物有: dog</span><br><span class="line">你的宠物有: cat</span><br></pre></td></tr></table></figure><p>使用 **kwargs:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">fun</span>(<span class="params">**kwargs</span>):</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">for</span> key, value <span class="keyword">in</span> kwargs.items():</span><br><span class="line"><span class="meta">... </span>        <span class="built_in">print</span>(<span class="string">&quot;&#123;0&#125; 喜欢 &#123;1&#125;&quot;</span>.<span class="built_in">format</span>(key, value))</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fun(Geek=<span class="string">&quot;cat&quot;</span>, cat=<span class="string">&quot;box&quot;</span>)</span><br><span class="line">Geek 喜欢 cat</span><br><span class="line">cat 喜欢 box</span><br></pre></td></tr></table></figure><ol start="3"><li>函数实参</li></ol><p>如果函数的形参是定长参数，也可以使用 *args 和 **kwargs 调用函数，类似对元组和字典进行解引用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">fun</span>(<span class="params">**kwargs</span>):</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">for</span> key, value <span class="keyword">in</span> kwargs.items():</span><br><span class="line"><span class="meta">... </span>        <span class="built_in">print</span>(<span class="string">&quot;&#123;0&#125; 喜欢 &#123;1&#125;&quot;</span>.<span class="built_in">format</span>(key, value))</span><br><span class="line"><span class="meta">... </span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>fun(Geek=<span class="string">&quot;cat&quot;</span>, cat=<span class="string">&quot;box&quot;</span>)</span><br><span class="line">Geek 喜欢 cat</span><br><span class="line">cat 喜欢 box</span><br></pre></td></tr></table></figure><ol start="4"><li>序列解包</li></ol><p>序列解包 往期博客有写过，这里只列出一个例子，序列解包没有 **。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a, b, *c = <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a  </span><br><span class="line"><span class="number">0</span>  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b  </span><br><span class="line"><span class="number">1</span>  </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c  </span><br><span class="line">[<span class="number">2</span>, <span class="number">3</span>]</span><br></pre></td></tr></table></figure><h2 id="TSNE可视化"><a href="#TSNE可视化" class="headerlink" title="TSNE可视化"></a>TSNE可视化</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.manifold import TSNE</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">f = &quot;predict_label.csv&quot;</span><br><span class="line">t = &quot;true_label.csv&quot;</span><br><span class="line"></span><br><span class="line">with open(f,encoding = &#x27;utf-8&#x27;) as f:</span><br><span class="line">    feature = np.loadtxt(f,delimiter = &quot;,&quot;)</span><br><span class="line">with open(t,encoding = &#x27;utf-8&#x27;) as f:</span><br><span class="line">    label = np.loadtxt(t,delimiter = &quot;,&quot;)</span><br><span class="line"></span><br><span class="line">tsne = TSNE(perplexity=30, n_components=2, init=&#x27;pca&#x27;, n_iter=1000) # TSNE降维，降到2</span><br><span class="line">Xpr = tsne.fit_transform(feature)</span><br><span class="line"></span><br><span class="line">x_min, x_max = Xpr.min(0), Xpr.max(0)</span><br><span class="line">Xpr = (Xpr - x_min) / (x_max - x_min)  # 归一化</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">for i in range(10000):</span><br><span class="line">    plt.text(Xpr[i, 0], Xpr[i, 1], str(label[i]),color=plt.cm.Set1(label[i]),fontdict=&#123;&#x27;weight&#x27;: &#x27;bold&#x27;, &#x27;size&#x27;: 9&#125;)</span><br><span class="line">plt.xticks([])</span><br><span class="line">plt.yticks([])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="copy与deepcopy-拷贝与深拷贝"><a href="#copy与deepcopy-拷贝与深拷贝" class="headerlink" title="copy与deepcopy (拷贝与深拷贝)"></a>copy与deepcopy (拷贝与深拷贝)</h2><p>python 中的copy与deepcopy是内存数据的操作，但是两个函数有一定的区别。</p><h3 id="1-copy"><a href="#1-copy" class="headerlink" title="1.copy"></a>1.copy</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="built_in">list</span> = [<span class="number">1</span>, [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line">list1 = copy.copy(<span class="built_in">list</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> <span class="built_in">id</span>(<span class="built_in">list</span>)</span><br><span class="line"><span class="built_in">print</span> <span class="built_in">id</span>(list1)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> <span class="built_in">id</span>(<span class="built_in">list</span>[<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span> <span class="built_in">id</span>(list1[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">list</span>[<span class="number">2</span>] = <span class="number">100</span></span><br><span class="line"><span class="built_in">list</span>[<span class="number">1</span>][<span class="number">0</span>] = <span class="number">44</span></span><br><span class="line"><span class="built_in">print</span> <span class="built_in">list</span></span><br><span class="line"><span class="built_in">print</span> list1</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line"><span class="number">46925320</span></span><br><span class="line"><span class="number">46967368</span></span><br><span class="line"><span class="number">46912776</span></span><br><span class="line"><span class="number">46912776</span></span><br><span class="line">[<span class="number">1</span>, [<span class="number">44</span>, <span class="number">5</span>, <span class="number">6</span>], <span class="number">100</span>, <span class="number">3</span>]</span><br><span class="line">[<span class="number">1</span>, [<span class="number">44</span>, <span class="number">5</span>, <span class="number">6</span>], <span class="number">2</span>, <span class="number">3</span>]</span><br></pre></td></tr></table></figure><p>经过copy操作的得两个list，list1拥有两个同的地址（46925320和46967368），修改list时不会影响list1的值，但是 list中间的子列表[4,5,6]在list和list1中有相同的地址46912776，所以在修改list中的子列表会影响到list1中的子列表。</p><h3 id="2-deepcopy"><a href="#2-deepcopy" class="headerlink" title="2.deepcopy"></a>2.deepcopy</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import copy</span><br><span class="line">list = [1, [4, 5, 6], 2, 3]</span><br><span class="line">list2 = copy.deepcopy(list)</span><br><span class="line"></span><br><span class="line">print id(list)</span><br><span class="line">print id(list[1])</span><br><span class="line">print id(list2)</span><br><span class="line">print id(list2[1])</span><br><span class="line"></span><br><span class="line">list[2] = 100</span><br><span class="line">list[1][0] = 44</span><br><span class="line">print list</span><br><span class="line">print list2</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">59508232</span><br><span class="line">59495688</span><br><span class="line">59509384</span><br><span class="line">59508168</span><br><span class="line">[1, [44, 5, 6], 100, 3]</span><br><span class="line">[1, [4, 5, 6], 2, 3]</span><br></pre></td></tr></table></figure><p>经过deepcopy的list与list2用有不用的地址59508232，59509384，其中的子列表页拥有不同的地址， 所以不论怎样修改list都不用影响到list2。</p><p>结论：</p><p>经过copy操作的两个数据对象拥有不同的得地址空间 ，但是这个数据对象如果是内嵌了其他的复杂数据对象，这个内嵌的数据对象在两个数据对象中拥有相同的地址空间，修改其中的值会互相印象。经过deepcopy的操作的不管是内层还是外层数据对象都拥有不同的地址空间，修改其中的值不会对两个对象都造成影响。</p><p>附：单变量用‘=’赋值不存在这个问题，因为用等号修改相当于重新赋值新的变量或常量，但并未涉及修改对应变量的值（即未对对应内存元素进行操作）。</p><h2 id="Python中os-sep的用法"><a href="#Python中os-sep的用法" class="headerlink" title="Python中os.sep的用法"></a>Python中os.sep的用法</h2><p>python是跨平台的。在Windows上，文件的路径分隔符是’&#39;，在Linux上是’/‘。</p><p>为了让代码在不同的平台上都能运行，那么路径应该写’&#39;还是’/‘呢？</p><p>使用os.sep的话，就不用考虑这个了，os.sep根据你所处的平台，自动采用相应的分隔符号。</p><p>举例</p><p>Linux系统某个路径，/usr/share/python,那么上面的os.sep就是‘/’</p><p>windows系统某个路径，C：\Users\Public\Desktop,那么上面的os.sep就是‘\’.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_dir = os.sep.join([&#x27;hello&#x27;, &#x27;world&#x27;])</span><br><span class="line">hello/world或者hello\world</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;矩阵的操作&quot;&gt;&lt;a href=&quot;#矩阵的操作&quot; class=&quot;headerlink&quot; title=&quot;矩阵的操作&quot;&gt;&lt;/a&gt;矩阵的操作&lt;/h2&gt;&lt;h3 id=&quot;view-和reshape&quot;&gt;&lt;a href=&quot;#view-和reshape&quot; class=&quot;headerlink&quot; title=&quot;view()和reshape&quot;&gt;&lt;/a&gt;view()和reshape&lt;/h3&gt;&lt;p&gt;view操作后的变量还是连续的。&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;a = torch.tensor([[[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;],[&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;]],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                  [[&lt;span class=&quot;number&quot;&gt;7&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;9&lt;/span&gt;],[&lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;11&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;12&lt;/span&gt;]]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                 ])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;b = a.view(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;b = tensor([[ &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,  &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,  &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;,  &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        [&lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;,  &lt;span class=&quot;number&quot;&gt;6&lt;/span&gt; ,&lt;span class=&quot;number&quot;&gt;7&lt;/span&gt;,	&lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        [&lt;span class=&quot;number&quot;&gt;9&lt;/span&gt;,	&lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;11&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;12&lt;/span&gt;]])&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;张量a的size是2x2x3，使用view函数后，先将这12个元素排成一列，然后将其依次填充至新的4x3的张量中：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;E:\笔记\markdown\reference\picture\v2-acfb79d45817e3080ddcac7666d61f09_b.jpg&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
&lt;p&gt;为了更细致，我们需要描述一下它们的具体操作流程（这并不是源码的流程，只是为了便于理解、记忆），因为二维比较直观，如果维度比较高的话，可能还是不够直观，心里老是有疑虑，还是以上面例子为例，张量a的每一个元素都有一个index，例如1的index是(0,0,0)，7的index是(1,0,0)，11的index是(1,1,1)……在拉成列向量排列时，排列规则是这样的，以(0,0,0)开始，维度从最后一维循环到第一维，在每一维内以升序将所有元素排成一列，即：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;E:\笔记\markdown\reference\picture\v2-0c59291b64850293305ab879087ea32d_b.jpg&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
&lt;p&gt;然后依据新的size对每个元素给予新的index，仍然是维度从最后一维循环到第一维，在每一维内升序，即:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;E:\笔记\markdown\reference\picture\v2-ab9112d1c2f5bb21d1b3d164bffa5e95_b.jpg&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;  torch的view()与reshape()方法都可以用来重塑tensor的shape，区别就是使用的条件不一样。view()方法只适用于满足连续性条件的tensor，并且该操作不会开辟新的内存空间，只是产生了对原存储空间的一个新别称和引用，返回值是视图。而reshape()方法的返回值既可以是视图，也可以是副本，当满足连续性条件时返回view，否则返回副本[ 此时等价于先调用contiguous()方法在使用view() ]。因此当不确能否使用view时，可以使用reshape。如果只是想简单地重塑一个tensor的shape，那么就是用reshape，但是如果需要考虑内存的开销而且要确保重塑后的tensor与之前的tensor共享存储空间，那就使用view()。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;permute-和transpose&quot;&gt;&lt;a href=&quot;#permute-和transpose&quot; class=&quot;headerlink&quot; title=&quot;permute()和transpose()&quot;&gt;&lt;/a&gt;permute()和transpose()&lt;/h3&gt;&lt;p&gt;transpose()也是转置操作，与permute不同的是，transpose()每次只能进行二维转置，而permute()每次可转置多个维度().&lt;/p&gt;
&lt;p&gt;接下来，说一下permute()，函数的参数为新的维度顺序，例如想交换第一维与第三维的index，则&lt;a href=&quot;https://www.zhihu.com/search?q=tensor.permute&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:%2288311093%22%7D&quot;&gt;tensor.permute&lt;/a&gt;(2,1,0)，同样举一个简单第二维与第三维的例子，&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;a = torch.tensor([[[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;],[&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;]],[[&lt;span class=&quot;number&quot;&gt;7&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;9&lt;/span&gt;],[&lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;11&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;12&lt;/span&gt;]]])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;b = a.permute(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;b = tensor([[[ &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,  &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;         [ &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,  &lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;         [ &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;,  &lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;]],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        [[ &lt;span class=&quot;number&quot;&gt;7&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;         [ &lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;11&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;         [ &lt;span class=&quot;number&quot;&gt;9&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;12&lt;/span&gt;]]])&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;E:\笔记\markdown\reference\picture\v2-6bdd87e65b64534339d3efad9ae4eae5_b.jpg&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
&lt;p&gt;当维度比较大，交换维度的结果并不直观，我们还是说下permute到底做了什么，示意图如下。&lt;/p&gt;
&lt;p&gt;需要说明一下，这里面的流程其实就是更改每个元素的index而已，上述例子中只是交换坐标，&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;E:\笔记\markdown\reference\picture\v2-ec4167d8c19713e232ab3f5ba35ed10d_b.jpg&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
&lt;p&gt;然后按照新的index更改下各元素的位置并展示出即可。&lt;/p&gt;
&lt;h3 id=&quot;squeeze-和unsqueeze-函数功能&quot;&gt;&lt;a href=&quot;#squeeze-和unsqueeze-函数功能&quot; class=&quot;headerlink&quot; title=&quot;squeeze()和unsqueeze()函数功能&quot;&gt;&lt;/a&gt;squeeze()和unsqueeze()函数功能&lt;/h3&gt;&lt;h4 id=&quot;1-squeeze-dim-：&quot;&gt;&lt;a href=&quot;#1-squeeze-dim-：&quot; class=&quot;headerlink&quot; title=&quot;1.squeeze(dim)：&quot;&gt;&lt;/a&gt;1.squeeze(dim)：&lt;/h4&gt;&lt;p&gt;给张量tensor降维，但不是啥张量都可以用这两个函数来降维。dim指定需要降维的维度，这个维度的值必须为1才能被降维。&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;a=torch.rand(&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;print&lt;/span&gt;(a.shape)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;print&lt;/span&gt;(a.squeeze(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;).shape)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#torch.Size([2, 1, 4, 1])&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#torch.Size([2, 4, 1])&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;2.unsqueeze(dim)&lt;/p&gt;
&lt;p&gt;与squeeze(dim)相反，在dim维度上添加一个维度。&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;a=torch.rand(&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;print&lt;/span&gt;(a.shape)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;print&lt;/span&gt;(a.unsqueeze(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;).shape)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#torch.Size([2, 4])&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#torch.Size([2, 1, 4])&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;





&lt;h2 id=&quot;运算符&quot;&gt;&lt;a href=&quot;#运算符&quot; class=&quot;headerlink&quot; title=&quot;运算符&quot;&gt;&lt;/a&gt;运算符&lt;/h2&gt;&lt;p&gt;在 Python 中，* 和 ** 具有语法多义性，具体来说是有四类用法。&lt;/p&gt;</summary>
    
    
    
    
    <category term="python" scheme="https://jpccc.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>聚类</title>
    <link href="https://jpccc.github.io/2022/04/07/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/"/>
    <id>https://jpccc.github.io/2022/04/07/%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/</id>
    <published>2022-04-07T08:10:44.000Z</published>
    <updated>2022-04-15T13:05:58.134Z</updated>
    
    <content type="html"><![CDATA[<p>可以使用模块 <a href="http://scikit-learn.org.cn/lists/3.html#sklearn.cluster%EF%BC%9A%E8%81%9A%E7%B1%BB"><code>sklearn.cluster</code></a> 对未标记的数据进行 <a href="https://en.wikipedia.org/wiki/Cluster_analysis">聚类(Clustering)</a> 。</p><p>每个聚类算法都有两个变体:一个是类，它实现了 <code>fit</code> 方法来学习训练数据上的簇，另一个是函数，给定训练数据，返回对应于不同簇的整数标签数组。对于类，可以在 <code>labels_</code> 属性中找到训练数据上的标签。</p><blockquote><p>  <strong>输入数据</strong></p><p>  特别需要注意的一点是，本模块实现的算法可以采用不同类型的矩阵作为输入。所有算法的调用接口都接受 shape<code>[n_samples, n_features]</code> 的标准数据矩阵。 这些矩阵可以通过使用 <a href="http://scikit-learn.org.cn/lists/3.html#sklearn.feature_extraction%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96"><code>sklearn.feature_extraction</code></a> 模块中的类获得。对于 <a href="https://scikit-learn.org.cn/view/370.html"><code>AffinityPropagation</code></a>, <a href="https://scikit-learn.org.cn/view/391.html"><code>SpectralClustering</code></a> 和 <a href="https://scikit-learn.org.cn/view/379.html"><code>DBSCAN</code></a> 也可以输入 shape <code>[n_samples, n_samples]</code> 的相似矩阵，可以通过 <a href="http://scikit-learn.org.cn/lists/3.html#sklearn.metrics%EF%BC%9A%E6%8C%87%E6%A0%87"><code>sklearn.metrics.pairwise</code></a> 模块中的函数获得。</p></blockquote><h3 id="聚类方法概述"><a href="#聚类方法概述" class="headerlink" title="聚类方法概述"></a>聚类方法概述</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;可以使用模块 &lt;a href=&quot;http://scikit-learn.org.cn/lists/3.html#sklearn.cluster%EF%BC%9A%E8%81%9A%E7%B1%BB&quot;&gt;&lt;code&gt;sklearn.cluster&lt;/code&gt;&lt;/a&gt; 对未标记</summary>
      
    
    
    
    
    <category term="算法" scheme="https://jpccc.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>pytorch使用汇总</title>
    <link href="https://jpccc.github.io/2022/04/07/pytorch%E4%BD%BF%E7%94%A8%E6%B1%87%E6%80%BB/"/>
    <id>https://jpccc.github.io/2022/04/07/pytorch%E4%BD%BF%E7%94%A8%E6%B1%87%E6%80%BB/</id>
    <published>2022-04-07T04:44:13.000Z</published>
    <updated>2022-05-09T13:45:26.263Z</updated>
    
    <content type="html"><![CDATA[<h2 id="日志输出"><a href="#日志输出" class="headerlink" title="日志输出"></a>日志输出</h2><p>利用logging模块在控制台实时打印并及时记录运行日志。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from config import  *</span><br><span class="line">import logging  # 引入logging模块</span><br><span class="line">import os.path</span><br><span class="line">class Logger:</span><br><span class="line">    def __init__(self,mode=&#x27;w&#x27;):</span><br><span class="line">        # 第一步，创建一个logger</span><br><span class="line">        self.logger = logging.getLogger()</span><br><span class="line">        self.logger.setLevel(logging.INFO)  # Log等级总开关</span><br><span class="line">        # 第二步，创建一个handler，用于写入日志文件</span><br><span class="line">        rq = time.strftime(&#x27;%Y%m%d%H%M&#x27;, time.localtime(time.time()))</span><br><span class="line">        log_path = os.getcwd() + &#x27;/Logs/&#x27;</span><br><span class="line">        log_name = log_path + rq + &#x27;.log&#x27;</span><br><span class="line">        logfile = log_name</span><br><span class="line">        fh = logging.FileHandler(logfile, mode=mode)</span><br><span class="line">        fh.setLevel(logging.DEBUG)  # 输出到file的log等级的开关</span><br><span class="line">        # 第三步，定义handler的输出格式</span><br><span class="line">        formatter = logging.Formatter(&quot;%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s&quot;)</span><br><span class="line">        fh.setFormatter(formatter)</span><br><span class="line">        # 第四步，将logger添加到handler里面</span><br><span class="line">        self.logger.addHandler(fh)</span><br><span class="line">        ch = logging.StreamHandler()</span><br><span class="line">        ch.setLevel(logging.INFO)  # 输出到console的log等级的开关</span><br><span class="line">        ch.setFormatter(formatter)</span><br><span class="line">        self.logger.addHandler(ch)</span><br></pre></td></tr></table></figure><h2 id="模型的保存和读取"><a href="#模型的保存和读取" class="headerlink" title="模型的保存和读取"></a>模型的保存和读取</h2><p>​    </p><table><thead><tr><th>保存</th><th align="left">（1）torch.save(net.state_dict(),保存路径) 。（2）多卡训练时，要用 torch.save(net.module.state_dict(),’./model/best.pth’)</th></tr></thead><tbody><tr><td>读取</td><td align="left">（1）net.load_state_dict(torch.load(‘best.pth’))。（2）并行时map_location=device.type在读取模型的时候一定要加上。即：    model.load_state_dict(torch.load(‘model/self_train_bestv2.pth’,map_location=’cuda’))</td></tr><tr><td></td><td align="left">torch.save(model.state_dict(),’checkpoint_0.tar’,_use_new_zipfile_serialization=False) #解决版本不兼容</td></tr></tbody></table><span id="more"></span><h3 id="网络参数和参数学习率的修改"><a href="#网络参数和参数学习率的修改" class="headerlink" title="网络参数和参数学习率的修改"></a>网络参数和参数学习率的修改</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">if use_resnet:</span><br><span class="line">resnet = models.resnet50(pretrained=True)</span><br><span class="line">new_state_dict = resnet.state_dict()</span><br><span class="line">dd = net.state_dict()</span><br><span class="line">for k in new_state_dict.keys():</span><br><span class="line">    print(k)</span><br><span class="line">    if k in dd.keys() and not k.startswith(&#x27;fc&#x27;):</span><br><span class="line">        print(&#x27;yes&#x27;)</span><br><span class="line">        dd[k] = new_state_dict[k]</span><br><span class="line">net.load_state_dict(dd)</span><br><span class="line"></span><br><span class="line">different learning rate</span><br><span class="line">params=[]</span><br><span class="line">params_dict = dict(net.named_parameters())</span><br><span class="line">print(learning_rate*1,learning_rate)</span><br><span class="line">for key,value in params_dict.items():</span><br><span class="line">    print(key)</span><br><span class="line">    if key.startswith(&#x27;features&#x27;):</span><br><span class="line">        params += [&#123;&#x27;params&#x27;:[value],&#x27;lr&#x27;:learning_rate*1&#125;]</span><br><span class="line">    else:</span><br><span class="line">        params += [&#123;&#x27;params&#x27;:[value],&#x27;lr&#x27;:learning_rate&#125;]</span><br><span class="line">创建优化器的时候设置了，上面的操作就没必要了</span><br></pre></td></tr></table></figure><h2 id="数据的保存和读取"><a href="#数据的保存和读取" class="headerlink" title="数据的保存和读取"></a>数据的保存和读取</h2><p>DataLoader的pin_memory和data.cuda(non_blocking=True)搭配使用</p><p><a href="https://so.csdn.net/so/search?q=PyTorch&spm=1001.2101.3001.7020">PyTorch</a>的DataLoader有一个参数pin_memory，使用固定内存，并使用non_blocking=True来并行处理数据传输。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span> x = x.cuda(non_blocking=<span class="literal">True</span>)</span><br><span class="line"><span class="number">2.</span> 进行一些和x无关的操作</span><br><span class="line"><span class="number">3.</span> 执行和x有关的操作</span><br></pre></td></tr></table></figure><p>在<code>non_blocking=true</code>下，<code>1</code>不会阻塞<code>2</code>，<code>1</code>和<code>2</code>并行。</p><p>这样将数据从CPU移动到<a href="https://so.csdn.net/so/search?q=GPU&spm=1001.2101.3001.7020">GPU</a>的时候，它是异步的。在它传输的时候，CPU还可以干其他的事情（不依赖于数据的事情）</p><h2 id="GPU的并行计算"><a href="#GPU的并行计算" class="headerlink" title="GPU的并行计算"></a>GPU的并行计算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">   device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">   device_ids = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">   model = torch.nn.DataParallel(model, device_ids=device_ids)</span><br><span class="line">   model.to(device)</span><br><span class="line">   data.to(device)</span><br><span class="line">   output=model(data)</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">   parser.add_argument(<span class="string">&quot;-g&quot;</span>, <span class="string">&quot;--gpus&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&quot;&quot;</span>)<span class="comment">#gpu的数量</span></span><br><span class="line">   <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">       net = torch.nn.DataParallel(net,device_ids=<span class="built_in">range</span>(<span class="built_in">len</span>(args.gpus.split(<span class="string">&quot;,&quot;</span>))))</span><br><span class="line">       torch.backends.cudnn.benchmark = <span class="literal">True</span></span><br></pre></td></tr></table></figure><h2 id="Wandb的使用"><a href="#Wandb的使用" class="headerlink" title="Wandb的使用"></a>Wandb的使用</h2><pre><code>import wandbwandb.init(project=&#39;yolov2&#39;)config = wandb.configconfig.learning_rate = 0.01wandb.log(&#123;&quot;loss&quot;: loss&#125;)</code></pre><h2 id="torchsummary"><a href="#torchsummary" class="headerlink" title="torchsummary"></a>torchsummary</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 安装：pip install torchsummary</span><br><span class="line"></span><br><span class="line">from torchsummary import summary</span><br><span class="line">from torchvision.models import resnet18</span><br><span class="line">model = resnet18()</span><br><span class="line">summary(model, input_size=[(3, 256, 256)], batch_size=2, device=&quot;cpu&quot;)</span><br></pre></td></tr></table></figure><h2 id="Tensor-board的使用"><a href="#Tensor-board的使用" class="headerlink" title="Tensor board的使用"></a>Tensor board的使用</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line">writer = SummaryWriter()</span><br><span class="line">writer.add_scalar(&#x27;loss&#x27;, loss, global_step=n_iter)  #loss是纵坐标，global_step是横坐标。</span><br></pre></td></tr></table></figure><h2 id="seed设置"><a href="#seed设置" class="headerlink" title="seed设置"></a>seed设置</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(args.seed)</span><br><span class="line">torch.cuda.manual_seed_all(args.seed)</span><br><span class="line">np.random.seed(args.seed)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="import-类"><a href="#import-类" class="headerlink" title="import 类"></a>import 类</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#用法1</span><br><span class="line">    import 包名</span><br><span class="line">    在包内的__init__.py文件中导入需要导入的类</span><br><span class="line">    那么就可以在import所在的文件中使用Init文件中导入的类</span><br><span class="line">#用法2</span><br><span class="line">如果在包内的某个pu文件想导入其它包的文件，路径从包名开始写</span><br><span class="line">例如： from model.resnet import Resnet18</span><br><span class="line"></span><br><span class="line">#vscode项目的文件的访问</span><br><span class="line">所有文件都能访问到最外层目录(项目所在的主目录)</span><br></pre></td></tr></table></figure><h2 id="计算过程中的梯度问题"><a href="#计算过程中的梯度问题" class="headerlink" title="计算过程中的梯度问题"></a>计算过程中的梯度问题</h2><p>requires_grad=True 要求计算梯度<br>        requires_grad=False 不要求计算梯度<br>        with torch.no_grad()或者@torch.no_grad()中的数据不需要计算梯度，也不会进行反向传播。<br>        tensor.data 近似tensor.detach()，都是让tensor不进行导数的计算，但是，tensor的值不能修改，否则求梯度时会报错。</p><p>==不计算梯度能够加快运行速度。==</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### model.eval()和with torch.no_grad()的区别</span></span><br><span class="line"></span><br><span class="line">在PyTorch中进行validation时，会使用model.<span class="built_in">eval</span>()切换到测试模式，在该模式下，</span><br><span class="line"></span><br><span class="line">主要用于通知dropout层和batchnorm层在train和<span class="built_in">eval</span>模式间切换</span><br><span class="line"></span><br><span class="line">在train模式下，dropout网络层会按照设定的参数p设置保留激活单元的概率（保留概率=p); batchnorm层会继续计算数据的mean和var等参数并更新。</span><br><span class="line"></span><br><span class="line">在<span class="built_in">eval</span>模式下，dropout层会让所有的激活单元都通过，而batchnorm层会停止计算和更新mean和var，直接使用在训练阶段已经学出的mean和var值。</span><br><span class="line"></span><br><span class="line">该模式不会影响各层的gradient计算行为，即gradient计算和存储与training模式一样，只是不进行反传（backprobagation）</span><br><span class="line"></span><br><span class="line">而<span class="keyword">with</span> torch.no_grad()则主要是用于停止autograd模块的工作，以起到加速和节省显存的作用，具体行为就是停止gradient计算，从而节省了GPU算力和显存，但是并不会影响dropout和batchnorm层的行为。</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()                                <span class="comment"># 测试模式</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval</span>():</span></span><br><span class="line">    model.<span class="built_in">eval</span>()  </span><br><span class="line">...</span><br></pre></td></tr></table></figure><h2 id="python-下划线开头和结尾-的元素"><a href="#python-下划线开头和结尾-的元素" class="headerlink" title="python(下划线开头和结尾)的元素"></a>python(下划线开头和结尾)的元素</h2><pre><code>事实上，当我们向文件导入某个模块时，导入的是该模块中那些名称不以下划线（单下划线“_”或者双下划线“__”）开头的变量、函数和类。因此，如果我们不想模块文件中的某个成员被引入到其它文件中使用，可以在其名称前添加下划线。</code></pre><h2 id="open函数"><a href="#open函数" class="headerlink" title="open函数"></a>open函数</h2><p>读取文本文件时，不要使用rb模式打开文件，而应该仅使用r模式。</p><table><thead><tr><th align="left">读取方式</th><th>解读</th></tr></thead><tbody><tr><td align="left">r</td><td>r是推荐使用的打开文本文件的模式。因为使用此模式打开文本文件时，python默认为我们做了一些处理，比如：假设在windows下，将本来应该读入的换行符\r\n处理成\n,方便我们处理。（值得一提的是，当你将\n写入文件时，python也会默认将其替换成\r\n，如果你是win系统的话）<br/>补充：其实是启用了通用换行符支持（UNS），它默认开启。</td></tr><tr><td align="left">rb</td><td>使用rb：则python不会对文本文件预处理了，你读入的\r\n依然是\r\n.</td></tr></tbody></table><h2 id="输入的图像变量需要设置梯度吗"><a href="#输入的图像变量需要设置梯度吗" class="headerlink" title="输入的图像变量需要设置梯度吗"></a>输入的图像变量需要设置梯度吗</h2><pre><code>不需要，每个网络参数都有梯度grad，因为其需要根据此来更新梯度pytorch不会记录中间变量的梯度，如果需要的话，要另外设置但是resnet等模型中的卷积层等参数，其是有梯度的，每个参数都有grad</code></pre><h2 id="F和nn"><a href="#F和nn" class="headerlink" title="F和nn"></a>F和nn</h2><pre><code>F.relu()和nn.Function()的区别：一个是函数一个是类方法。</code></pre><h2 id="读取网络参数"><a href="#读取网络参数" class="headerlink" title="读取网络参数"></a>读取网络参数</h2><p>​    </p><h2 id="contiguous"><a href="#contiguous" class="headerlink" title="contiguous"></a>contiguous</h2><pre><code>如果想要变得连续使用contiguous方法，如果Tensor不是连续的，则会重新开辟一块内存空间保证数据是在内存中是连续的，如果Tensor是连续的，则contiguous无操作。</code></pre><p>注意梯度反向传播的时候，所有的变量的应该在gpu或者同时在cpu上。</p><h2 id="提高gpu利用率"><a href="#提高gpu利用率" class="headerlink" title="提高gpu利用率"></a>提高gpu利用率</h2><pre><code>pytorch跑Unet代码，gpu利用率在0%-20%闪现，主要问题是GPU一直在等cpu处理的数据传输过去。利用top查看cup的利用率也是从0省道100%且显然cup的线程并不多，能处理出的数据也不多。在一般的程序中，除了加载从dataloader中数据和model的运行需要gpu，其余更多的dataset、dataloader、loss的计算和日志的输出很多部分都需要cup的计算。所以，可以提升的方面包括 从class dataset的优化、dataloader的优化和其他部分代码的优化。当然代码的优化是一个长期的考验代码能力的问题。那么短期的提升在于对dataloader的优化：    1.batchsize调大 提高GPU内存占用率    2.num_works 调到适当值，一般情况下为8、16是比较合适的值。太小就会出现我上述讲道的一些问题。太大的话cpu线程增加会导致gpu的利用率降低。因为模型需要将数据平均分配到几个子线程去进行预处理，分发等数据操作，设高了反而影响效率。    3.pin_memory =True 省掉了将数据从CPU传入到缓存RAM里面，再给传输到GPU上；为True时是直接映射到GPU的相关内存块上，省掉了一点数据传输时间。</code></pre><h2 id="vscode的目录访问"><a href="#vscode的目录访问" class="headerlink" title="vscode的目录访问"></a>vscode的目录访问</h2><h2 id="pytorch的GPU设置理解"><a href="#pytorch的GPU设置理解" class="headerlink" title="pytorch的GPU设置理解"></a><a href="./reference/gpu.md">pytorch的GPU设置</a>理解</h2><pre><code>cudnn.deterministic = Truecudnn.benchmark = True</code></pre><h2 id="torch-tensor与torch-Tensor的区别"><a href="#torch-tensor与torch-Tensor的区别" class="headerlink" title="torch.tensor与torch.Tensor的区别"></a><strong>torch.tensor与torch.Tensor的区别</strong></h2><p>​    细心的读者可能注意到了，通过Tensor建立数组有torch.tensor([1,2])或torch.Tensor([1,2])两种方式。那么，这两种方式有什么区别呢？</p><p>​    （1）torch.tensor是从数据中推断数据类型，而torch.Tensor是torch.empty(会随机产生垃圾数组，详见实例)和torch.tensor之间的一种混合。但是，当传入数据时，torch.Tensor使用全局默认dtype(FloatTensor)；</p><h2 id="tensor类型和形状"><a href="#tensor类型和形状" class="headerlink" title="tensor类型和形状"></a>tensor类型和形状</h2><p>​    查看tensor类型最好用tensor.type()函数，而不要用type(tensor)！</p><p>tensor.shape和tensor.size（）都可以返回tensor的形状，前一个是属性，后一个是方法。</p><h2 id="1-基本配置"><a href="#1-基本配置" class="headerlink" title="1. 基本配置"></a>1. 基本配置</h2><h3 id="导入包和版本查询"><a href="#导入包和版本查询" class="headerlink" title="导入包和版本查询"></a>导入包和版本查询</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torchvision</span><br><span class="line">print(torch.__version__)</span><br><span class="line">print(torch.version.cuda)</span><br><span class="line">print(torch.backends.cudnn.version())</span><br><span class="line">print(torch.cuda.get_device_name(0))</span><br></pre></td></tr></table></figure><h3 id="可复现性"><a href="#可复现性" class="headerlink" title="可复现性"></a>可复现性</h3><p>在硬件设备（CPU、GPU）不同时，完全的可复现性无法保证，即使随机种子相同。但是，在同一个设备上，应该保证可复现性。具体做法是，在程序开始的时候固定torch的随机种子，同时也把numpy的随机种子固定。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(0)</span><br><span class="line">torch.manual_seed(0)</span><br><span class="line">torch.cuda.manual_seed_all(0)</span><br><span class="line"></span><br><span class="line">torch.backends.cudnn.deterministic = True</span><br><span class="line">torch.backends.cudnn.benchmark = False</span><br></pre></td></tr></table></figure><h3 id="显卡设置"><a href="#显卡设置" class="headerlink" title="显卡设置"></a>显卡设置</h3><p>如果只需要一张显卡</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Device configuration</span><br><span class="line">device = torch.device(&#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)</span><br></pre></td></tr></table></figure><p>如果需要指定多张显卡，比如0，1号显卡。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">os.environ[&#x27;CUDA_VISIBLE_DEVICES&#x27;] = &#x27;0,1&#x27;</span><br></pre></td></tr></table></figure><p>也可以在命令行运行代码时设置显卡：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0,1 python train.py</span><br></pre></td></tr></table></figure><p>清除显存</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.empty_cache()</span><br></pre></td></tr></table></figure><p>也可以使用在命令行重置GPU的指令</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi --gpu-reset -i [gpu_id]</span><br></pre></td></tr></table></figure><h2 id="2-张量-Tensor-处理"><a href="#2-张量-Tensor-处理" class="headerlink" title="2. 张量(Tensor)处理"></a>2. 张量(Tensor)处理</h2><h3 id="张量的数据类型"><a href="#张量的数据类型" class="headerlink" title="张量的数据类型"></a>张量的数据类型</h3><p>PyTorch有9种CPU张量类型和9种GPU张量类型。</p><p><img src="E:\笔记\markdown\reference\picture\640.jpeg" alt="图片"></p><h3 id="张量基本信息"><a href="#张量基本信息" class="headerlink" title="张量基本信息"></a>张量基本信息</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor = torch.randn(3,4,5)</span><br><span class="line">print(tensor.type())  # 数据类型</span><br><span class="line">print(tensor.size())  # 张量的shape，是个元组</span><br><span class="line">print(tensor.dim())   # 维度的数量</span><br></pre></td></tr></table></figure><h3 id="命名张量"><a href="#命名张量" class="headerlink" title="命名张量"></a>命名张量</h3><p>张量命名是一个非常有用的方法，这样可以方便地使用维度的名字来做索引或其他操作，大大提高了可读性、易用性，防止出错。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 在PyTorch 1.3之前，需要使用注释</span><br><span class="line"># Tensor[N, C, H, W]</span><br><span class="line">images = torch.randn(32, 3, 56, 56)</span><br><span class="line">images.sum(dim=1)</span><br><span class="line">images.select(dim=1, index=0)</span><br><span class="line"></span><br><span class="line"># PyTorch 1.3之后</span><br><span class="line">NCHW = [‘N’, ‘C’, ‘H’, ‘W’]</span><br><span class="line">images = torch.randn(32, 3, 56, 56, names=NCHW)</span><br><span class="line">images.sum(&#x27;C&#x27;)</span><br><span class="line">images.select(&#x27;C&#x27;, index=0)</span><br><span class="line"># 也可以这么设置</span><br><span class="line">tensor = torch.rand(3,4,1,2,names=(&#x27;C&#x27;, &#x27;N&#x27;, &#x27;H&#x27;, &#x27;W&#x27;))</span><br><span class="line"># 使用align_to可以对维度方便地排序</span><br><span class="line">tensor = tensor.align_to(&#x27;N&#x27;, &#x27;C&#x27;, &#x27;H&#x27;, &#x27;W&#x27;)</span><br></pre></td></tr></table></figure><h3 id="数据类型转换"><a href="#数据类型转换" class="headerlink" title="数据类型转换"></a>数据类型转换</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 设置默认类型，pytorch中的FloatTensor远远快于DoubleTensor</span><br><span class="line">torch.set_default_tensor_type(torch.FloatTensor)</span><br><span class="line"></span><br><span class="line"># 类型转换</span><br><span class="line">tensor = tensor.cuda()</span><br><span class="line">tensor = tensor.cpu()</span><br><span class="line">tensor = tensor.float()</span><br><span class="line">tensor = tensor.long()</span><br></pre></td></tr></table></figure><h3 id="torch-Tensor与np-ndarray转换"><a href="#torch-Tensor与np-ndarray转换" class="headerlink" title="torch.Tensor与np.ndarray转换"></a><strong>torch.Tensor与np.ndarray转换</strong></h3><p>除了CharTensor，其他所有CPU上的张量都支持转换为numpy格式然后再转换回来。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ndarray = tensor.cpu().numpy()</span><br><span class="line">tensor = torch.from_numpy(ndarray).float()</span><br><span class="line">tensor = torch.from_numpy(ndarray.copy()).float() # If ndarray has negative stride.</span><br></pre></td></tr></table></figure><h3 id="Torch-tensor与PIL-Image转换"><a href="#Torch-tensor与PIL-Image转换" class="headerlink" title="Torch.tensor与PIL.Image转换"></a><strong>Torch.tensor与PIL.Image转换</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># pytorch中的张量默认采用[N, C, H, W]的顺序，并且数据范围在[0,1]，需要进行转置和规范化</span><br><span class="line"># torch.Tensor -&gt; PIL.Image</span><br><span class="line">image = PIL.Image.fromarray(torch.clamp(tensor*255, min=0, max=255).byte().permute(1,2,0).cpu().numpy())</span><br><span class="line">image = torchvision.transforms.functional.to_pil_image(tensor)  # Equivalently way</span><br><span class="line"></span><br><span class="line"># PIL.Image -&gt; torch.Tensor</span><br><span class="line">path = r&#x27;./figure.jpg&#x27;</span><br><span class="line">tensor = torch.from_numpy(np.asarray(PIL.Image.open(path))).permute(2,0,1).float() / 255</span><br><span class="line">tensor = torchvision.transforms.functional.to_tensor(PIL.Image.open(path)) # Equivalently way</span><br></pre></td></tr></table></figure><h3 id="np-ndarray与PIL-Image的转换"><a href="#np-ndarray与PIL-Image的转换" class="headerlink" title="np.ndarray与PIL.Image的转换"></a><strong>np.ndarray与PIL.Image的转换</strong></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">image = PIL.Image.fromarray(ndarray.astype(np.uint8))</span><br><span class="line">ndarray = np.asarray(PIL.Image.open(path))</span><br></pre></td></tr></table></figure><h3 id="从只包含一个元素的张量中提取值"><a href="#从只包含一个元素的张量中提取值" class="headerlink" title="从只包含一个元素的张量中提取值"></a>从只包含一个元素的张量中提取值</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">value = torch.rand(1).item()</span><br></pre></td></tr></table></figure><h3 id="张量形变"><a href="#张量形变" class="headerlink" title="张量形变"></a>张量形变</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 在将卷积层输入全连接层的情况下通常需要对张量做形变处理，</span><br><span class="line"># 相比torch.view，torch.reshape可以自动处理输入张量不连续的情况。</span><br><span class="line">tensor = torch.rand(2,3,4)</span><br><span class="line">shape = (6, 4)</span><br><span class="line">tensor = torch.reshape(tensor, shape)</span><br></pre></td></tr></table></figure><h3 id="打乱顺序"><a href="#打乱顺序" class="headerlink" title="打乱顺序"></a>打乱顺序</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor = tensor[torch.randperm(tensor.size(0))]  # 打乱第一个维度</span><br></pre></td></tr></table></figure><h3 id="水平翻转"><a href="#水平翻转" class="headerlink" title="水平翻转"></a>水平翻转</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># pytorch不支持tensor[::-1]这样的负步长操作，水平翻转可以通过张量索引实现</span><br><span class="line"># 假设张量的维度为[N, D, H, W].</span><br><span class="line">tensor = tensor[:,:,:,torch.arange(tensor.size(3) - 1, -1, -1).long()]</span><br></pre></td></tr></table></figure><h3 id="复制张量"><a href="#复制张量" class="headerlink" title="复制张量"></a>复制张量</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># Operation                 |  New/Shared memory | Still in computation graph |</span><br><span class="line">tensor.clone()            # |        New         |          Yes               |</span><br><span class="line">tensor.detach()           # |      Shared        |          No                |</span><br><span class="line">tensor.detach.clone()()   # |        New         |          No                |</span><br></pre></td></tr></table></figure><h3 id="张量拼接"><a href="#张量拼接" class="headerlink" title="张量拼接"></a>张量拼接</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">注意torch.cat和torch.stack的区别在于torch.cat沿着给定的维度拼接，</span><br><span class="line">而torch.stack会新增一维。例如当参数是3个10x5的张量，torch.cat的结果是30x5的张量，</span><br><span class="line">而torch.stack的结果是3x10x5的张量。</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">tensor = torch.cat(list_of_tensors, dim=0)</span><br><span class="line">tensor = torch.stack(list_of_tensors, dim=0)</span><br></pre></td></tr></table></figure><h3 id="将整数标签转为one-hot编码"><a href="#将整数标签转为one-hot编码" class="headerlink" title="将整数标签转为one-hot编码"></a>将整数标签转为one-hot编码</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># pytorch的标记默认从0开始</span><br><span class="line">tensor = torch.tensor([0, 2, 1, 3])</span><br><span class="line">N = tensor.size(0)</span><br><span class="line">num_classes = 4</span><br><span class="line">one_hot = torch.zeros(N, num_classes).long()</span><br><span class="line">one_hot.scatter_(dim=1, index=torch.unsqueeze(tensor, dim=1), src=torch.ones(N, num_classes).long())</span><br></pre></td></tr></table></figure><h3 id="得到非零元素"><a href="#得到非零元素" class="headerlink" title="得到非零元素"></a>得到非零元素</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.nonzero(tensor)               # index of non-zero elements</span><br><span class="line">torch.nonzero(tensor==0)            # index of zero elements</span><br><span class="line">torch.nonzero(tensor).size(0)       # number of non-zero elements</span><br><span class="line">torch.nonzero(tensor == 0).size(0)  # number of zero elements</span><br></pre></td></tr></table></figure><h3 id="判断两个张量相等"><a href="#判断两个张量相等" class="headerlink" title="判断两个张量相等"></a>判断两个张量相等</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.allclose(tensor1, tensor2)  # float tensor</span><br><span class="line">torch.equal(tensor1, tensor2)     # int tensor</span><br></pre></td></tr></table></figure><h3 id="张量扩展"><a href="#张量扩展" class="headerlink" title="张量扩展"></a>张量扩展</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Expand tensor of shape 64*512 to shape 64*512*7*7.</span><br><span class="line">tensor = torch.rand(64,512)</span><br><span class="line">torch.reshape(tensor, (64, 512, 1, 1)).expand(64, 512, 7, 7)</span><br></pre></td></tr></table></figure><h3 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Matrix multiplcation: (m*n) * (n*p) * -&gt; (m*p).</span><br><span class="line">result = torch.mm(tensor1, tensor2)</span><br><span class="line"></span><br><span class="line"># Batch matrix multiplication: (b*m*n) * (b*n*p) -&gt; (b*m*p)</span><br><span class="line">result = torch.bmm(tensor1, tensor2)</span><br><span class="line"></span><br><span class="line"># Element-wise multiplication.</span><br><span class="line">result = tensor1 * tensor2</span><br></pre></td></tr></table></figure><h3 id="计算两组数据之间的两两欧式距离"><a href="#计算两组数据之间的两两欧式距离" class="headerlink" title="计算两组数据之间的两两欧式距离"></a>计算两组数据之间的两两欧式距离</h3><p>利用broadcast机制</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dist = torch.sqrt(torch.sum((X1[:,None,:] - X2) ** 2, dim=2))</span><br></pre></td></tr></table></figure><h2 id="3-模型定义和操作"><a href="#3-模型定义和操作" class="headerlink" title="3. 模型定义和操作"></a>3. 模型定义和操作</h2><h3 id="一个简单两层卷积网络的示例"><a href="#一个简单两层卷积网络的示例" class="headerlink" title="一个简单两层卷积网络的示例"></a>一个简单两层卷积网络的示例</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># convolutional neural network (2 convolutional layers)</span><br><span class="line">class ConvNet(nn.Module):   </span><br><span class="line">    def __init__(self, num_classes=10):   </span><br><span class="line">        super(ConvNet, self).__init__()    </span><br><span class="line">        self.layer1 = nn.Sequential(     </span><br><span class="line">            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),           </span><br><span class="line">            nn.BatchNorm2d(16),     </span><br><span class="line">            nn.ReLU(),      </span><br><span class="line">            nn.MaxPool2d(kernel_size=2, stride=2))     </span><br><span class="line">        self.layer2 = nn.Sequential(       </span><br><span class="line">            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),         </span><br><span class="line">            nn.BatchNorm2d(32),    </span><br><span class="line">            nn.ReLU(),        </span><br><span class="line">            nn.MaxPool2d(kernel_size=2, stride=2))   </span><br><span class="line">        self.fc = nn.Linear(7*7*32, num_classes)   </span><br><span class="line">        </span><br><span class="line">    def forward(self, x):   </span><br><span class="line">        out = self.layer1(x)    </span><br><span class="line">        out = self.layer2(out)     </span><br><span class="line">        out = out.reshape(out.size(0), -1)     </span><br><span class="line">        out = self.fc(out)   </span><br><span class="line">        return out</span><br><span class="line">        </span><br><span class="line">model = ConvNet(num_classes).to(device)</span><br></pre></td></tr></table></figure><p>卷积层的计算和展示可以用这个网站辅助。</p><h3 id="双线性汇合（bilinear-pooling）"><a href="#双线性汇合（bilinear-pooling）" class="headerlink" title="双线性汇合（bilinear pooling）"></a>双线性汇合（bilinear pooling）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X = torch.reshape(N, D, H * W)       # Assume X has shape N*D*H*W</span><br><span class="line">X = torch.bmm(X, torch.transpose(X, 1, 2)) / (H * W)  # Bilinear pooling</span><br><span class="line">assert X.size() == (N, D, D)</span><br><span class="line">X = torch.reshape(X, (N, D * D))</span><br><span class="line">X = torch.sign(X) * torch.sqrt(torch.abs(X) + 1e-5)   # Signed-sqrt normalization</span><br><span class="line">X = torch.nn.functional.normalize(X)                  # L2 normalization</span><br></pre></td></tr></table></figure><h3 id="多卡同步-BN（Batch-normalization）"><a href="#多卡同步-BN（Batch-normalization）" class="headerlink" title="多卡同步 BN（Batch normalization）"></a>多卡同步 BN（Batch normalization）</h3><p>当使用 torch.nn.DataParallel 将代码运行在多张 GPU 卡上时，PyTorch 的 BN 层默认操作是各卡上数据独立地计算均值和标准差，同步 BN 使用所有卡上的数据一起计算 BN 层的均值和标准差，缓解了当批量大小（batch size）比较小时对均值和标准差估计不准的情况，是在目标检测等任务中一个有效的提升性能的技巧。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sync_bn = torch.nn.SyncBatchNorm(num_features, eps=1e-05, momentum=0.1, affine=True,             </span><br><span class="line">                                 track_running_stats=True)</span><br></pre></td></tr></table></figure><h3 id="将已有网络的所有BN层改为同步BN层"><a href="#将已有网络的所有BN层改为同步BN层" class="headerlink" title="将已有网络的所有BN层改为同步BN层"></a>将已有网络的所有BN层改为同步BN层</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def convertBNtoSyncBN(module, process_group=None):   </span><br><span class="line">    &#x27;&#x27;&#x27;Recursively replace all BN layers to SyncBN layer.  </span><br><span class="line">    </span><br><span class="line">    Args:     </span><br><span class="line">        module[torch.nn.Module]. Network  </span><br><span class="line">    &#x27;&#x27;&#x27; </span><br><span class="line">    if isinstance(module, torch.nn.modules.batchnorm._BatchNorm):      </span><br><span class="line">        sync_bn = torch.nn.SyncBatchNorm(module.num_features, module.eps, module.momentum,                                         </span><br><span class="line">                                         module.affine, module.track_running_stats, process_group)     </span><br><span class="line">        sync_bn.running_mean = module.running_mean      </span><br><span class="line">        sync_bn.running_var = module.running_var     </span><br><span class="line">        if module.affine:       </span><br><span class="line">            sync_bn.weight = module.weight.clone().detach() </span><br><span class="line">            sync_bn.bias = module.bias.clone().detach()   </span><br><span class="line">        return sync_bn  </span><br><span class="line">    else:    </span><br><span class="line">        for name, child_module in module.named_children():           </span><br><span class="line">            setattr(module, name) = convert_syncbn_model(child_module, process_group=process_group))    </span><br><span class="line">        return module</span><br></pre></td></tr></table></figure><h3 id="类似-BN-滑动平均"><a href="#类似-BN-滑动平均" class="headerlink" title="类似 BN 滑动平均"></a>类似 BN 滑动平均</h3><p>如果要实现类似 BN 滑动平均的操作，在 forward 函数中要使用原地（inplace）操作给滑动平均赋值。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">class BN(torch.nn.Module)  </span><br><span class="line">    def __init__(self):     </span><br><span class="line">        ...     </span><br><span class="line">        self.register_buffer(&#x27;running_mean&#x27;, torch.zeros(num_features))</span><br><span class="line">        </span><br><span class="line">    def forward(self, X):   </span><br><span class="line">        ...      </span><br><span class="line">        self.running_mean += momentum * (current - self.running_mean)</span><br></pre></td></tr></table></figure><h3 id="计算模型整体参数量"><a href="#计算模型整体参数量" class="headerlink" title="计算模型整体参数量"></a>计算模型整体参数量</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">num_parameters = sum(torch.numel(parameter) for parameter in model.parameters())</span><br></pre></td></tr></table></figure><h3 id="查看网络中的参数"><a href="#查看网络中的参数" class="headerlink" title="查看网络中的参数"></a>查看网络中的参数</h3><p>可以通过model.state_dict()或者model.named_parameters()函数查看现在的全部可训练参数（包括通过继承得到的父类中的参数）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">params = list(model.named_parameters())</span><br><span class="line">(name, param) = params[28]</span><br><span class="line">print(name)</span><br><span class="line">print(param.grad)</span><br><span class="line">print(&#x27;-------------------------------------------------&#x27;)</span><br><span class="line">(name2, param2) = params[29]</span><br><span class="line">print(name2)</span><br><span class="line">print(param2.grad)</span><br><span class="line">print(&#x27;----------------------------------------------------&#x27;)</span><br><span class="line">(name1, param1) = params[30]</span><br><span class="line">print(name1)</span><br><span class="line">print(param1.grad)</span><br></pre></td></tr></table></figure><h3 id="模型可视化（使用pytorchviz）"><a href="#模型可视化（使用pytorchviz）" class="headerlink" title="模型可视化（使用pytorchviz）"></a>模型可视化（使用pytorchviz）</h3><p>szagoruyko/pytorchvizgithub.com</p><h3 id="类似-Keras-的-model-summary-输出模型信息，使用pytorch-summary"><a href="#类似-Keras-的-model-summary-输出模型信息，使用pytorch-summary" class="headerlink" title="类似 Keras 的 model.summary() 输出模型信息，使用pytorch-summary"></a>类似 Keras 的 model.summary() 输出模型信息，使用pytorch-summary</h3><p>sksq96/pytorch-summarygithub.com</p><p><strong>模型权重初始化</strong></p><p>注意 model.modules() 和 model.children() 的区别：model.modules() 会迭代地遍历模型的所有子层，而 model.children() 只会遍历模型下的一层。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># Common practise for initialization.</span><br><span class="line">for layer in model.modules(): </span><br><span class="line">    if isinstance(layer, torch.nn.Conv2d):    </span><br><span class="line">        torch.nn.init.kaiming_normal_(layer.weight, mode=&#x27;fan_out&#x27;,                                </span><br><span class="line">                                       nonlinearity=&#x27;relu&#x27;) </span><br><span class="line">        if layer.bias is not None:    </span><br><span class="line">            torch.nn.init.constant_(layer.bias, val=0.0)  </span><br><span class="line">    elif isinstance(layer, torch.nn.BatchNorm2d):     </span><br><span class="line">        torch.nn.init.constant_(layer.weight, val=1.0)  </span><br><span class="line">        torch.nn.init.constant_(layer.bias, val=0.0)   </span><br><span class="line">    elif isinstance(layer, torch.nn.Linear):     </span><br><span class="line">        torch.nn.init.xavier_normal_(layer.weight)    </span><br><span class="line">        if layer.bias is not None:      </span><br><span class="line">            torch.nn.init.constant_(layer.bias, val=0.0)</span><br><span class="line"># Initialization with given tensor.</span><br><span class="line">layer.weight = torch.nn.Parameter(tensor)</span><br></pre></td></tr></table></figure><h3 id="提取模型中的某一层"><a href="#提取模型中的某一层" class="headerlink" title="提取模型中的某一层"></a>提取模型中的某一层</h3><p>modules()会返回模型中所有模块的迭代器，它能够访问到最内层，比如self.layer1.conv1这个模块，还有一个与它们相对应的是name_children()属性以及named_modules(),这两个不仅会返回模块的迭代器，还会返回网络层的名字。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 取模型中的前两层</span><br><span class="line">new_model = nn.Sequential(*list(model.children())[:2] </span><br><span class="line"># 如果希望提取出模型中的所有卷积层，可以像下面这样操作：</span><br><span class="line">for layer in model.named_modules():   </span><br><span class="line">    if isinstance(layer[1],nn.Conv2d):    </span><br><span class="line">         conv_model.add_module(layer[0],layer[1])</span><br></pre></td></tr></table></figure><h3 id="部分层使用预训练模型"><a href="#部分层使用预训练模型" class="headerlink" title="部分层使用预训练模型"></a>部分层使用预训练模型</h3><p>注意如果保存的模型是 torch.nn.DataParallel，则当前的模型也需要是</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(&#x27;model.pth&#x27;), strict=False)</span><br></pre></td></tr></table></figure><h3 id="将在-GPU-保存的模型加载到-CPU"><a href="#将在-GPU-保存的模型加载到-CPU" class="headerlink" title="将在 GPU 保存的模型加载到 CPU"></a>将在 GPU 保存的模型加载到 CPU</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(&#x27;model.pth&#x27;, map_location=&#x27;cpu&#x27;))</span><br></pre></td></tr></table></figure><h3 id="导入另一个模型的相同部分到新的模型"><a href="#导入另一个模型的相同部分到新的模型" class="headerlink" title="导入另一个模型的相同部分到新的模型"></a>导入另一个模型的相同部分到新的模型</h3><p>模型导入参数时，如果两个模型结构不一致，则直接导入参数会报错。用下面方法可以把另一个模型的相同的部分导入到新的模型中。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># model_new代表新的模型</span><br><span class="line"># model_saved代表其他模型，比如用torch.load导入的已保存的模型</span><br><span class="line">model_new_dict = model_new.state_dict()</span><br><span class="line">model_common_dict = &#123;k:v for k, v in model_saved.items() if k in model_new_dict.keys()&#125;</span><br><span class="line">model_new_dict.update(model_common_dict)</span><br><span class="line">model_new.load_state_dict(model_new_dict)</span><br></pre></td></tr></table></figure><h2 id="4-数据处理"><a href="#4-数据处理" class="headerlink" title="4. 数据处理"></a>4. 数据处理</h2><h3 id="计算数据集的均值和标准差"><a href="#计算数据集的均值和标准差" class="headerlink" title="计算数据集的均值和标准差"></a>计算数据集的均值和标准差</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import cv2</span><br><span class="line">import numpy as np</span><br><span class="line">from torch.utils.data import Dataset</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line">def compute_mean_and_std(dataset):  </span><br><span class="line">    # 输入PyTorch的dataset，输出均值和标准差   </span><br><span class="line">    mean_r = 0  </span><br><span class="line">    mean_g = 0   </span><br><span class="line">    mean_b = 0 </span><br><span class="line">    </span><br><span class="line">    for img, _ in dataset:    </span><br><span class="line">        img = np.asarray(img) # change PIL Image to numpy array     </span><br><span class="line">        mean_b += np.mean(img[:, :, 0])     </span><br><span class="line">        mean_g += np.mean(img[:, :, 1])   </span><br><span class="line">        mean_r += np.mean(img[:, :, 2])  </span><br><span class="line">        </span><br><span class="line">    mean_b /= len(dataset)  </span><br><span class="line">    mean_g /= len(dataset)   </span><br><span class="line">    mean_r /= len(dataset)  </span><br><span class="line">    </span><br><span class="line">    diff_r = 0    </span><br><span class="line">    diff_g = 0  </span><br><span class="line">    diff_b = 0  </span><br><span class="line">    </span><br><span class="line">    N = 0  </span><br><span class="line">    </span><br><span class="line">    for img, _ in dataset:   </span><br><span class="line">        img = np.asarray(img)      </span><br><span class="line">        </span><br><span class="line">        diff_b += np.sum(np.power(img[:, :, 0] - mean_b, 2))</span><br><span class="line">        diff_g += np.sum(np.power(img[:, :, 1] - mean_g, 2))</span><br><span class="line">        diff_r += np.sum(np.power(img[:, :, 2] - mean_r, 2)) </span><br><span class="line">        </span><br><span class="line">        N += np.prod(img[:, :, 0].shape)  </span><br><span class="line">    </span><br><span class="line">    std_b = np.sqrt(diff_b / N)  </span><br><span class="line">    std_g = np.sqrt(diff_g / N)  </span><br><span class="line">    std_r = np.sqrt(diff_r / N) </span><br><span class="line">    </span><br><span class="line">    mean = (mean_b.item() / 255.0, mean_g.item() / 255.0, mean_r.item() / 255.0) </span><br><span class="line">    std = (std_b.item() / 255.0, std_g.item() / 255.0, std_r.item() / 255.0) </span><br><span class="line">    return mean, std</span><br></pre></td></tr></table></figure><h3 id="得到视频数据基本信息"><a href="#得到视频数据基本信息" class="headerlink" title="得到视频数据基本信息"></a>得到视频数据基本信息</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import cv2</span><br><span class="line">video = cv2.VideoCapture(mp4_path)</span><br><span class="line">height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))</span><br><span class="line">width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))</span><br><span class="line">num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))</span><br><span class="line">fps = int(video.get(cv2.CAP_PROP_FPS))</span><br><span class="line">video.release()</span><br></pre></td></tr></table></figure><h3 id="TSN-每段（segment）采样一帧视频"><a href="#TSN-每段（segment）采样一帧视频" class="headerlink" title="TSN 每段（segment）采样一帧视频"></a>TSN 每段（segment）采样一帧视频</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">K = self._num_segments</span><br><span class="line">if is_train: </span><br><span class="line">    if num_frames &gt; K:    </span><br><span class="line">        # Random index for each segment.     </span><br><span class="line">        frame_indices = torch.randint(       </span><br><span class="line">            high=num_frames // K, size=(K,), dtype=torch.long)      </span><br><span class="line">        frame_indices += num_frames // K * torch.arange(K) </span><br><span class="line">    else:      </span><br><span class="line">        frame_indices = torch.randint(      </span><br><span class="line">            high=num_frames, size=(K - num_frames,), dtype=torch.long)     </span><br><span class="line">        frame_indices = torch.sort(torch.cat((       </span><br><span class="line">            torch.arange(num_frames), frame_indices)))[0]</span><br><span class="line">else:   </span><br><span class="line">    if num_frames &gt; K:  </span><br><span class="line">        # Middle index for each segment.    </span><br><span class="line">        frame_indices = num_frames / K // 2     </span><br><span class="line">        frame_indices += num_frames // K * torch.arange(K)  </span><br><span class="line">    else:    </span><br><span class="line">        frame_indices = torch.sort(torch.cat((      </span><br><span class="line">            torch.arange(num_frames), torch.arange(K - num_frames))))[0]</span><br><span class="line">assert frame_indices.size() == (K,)</span><br><span class="line">return [frame_indices[i] for i in range(K)]</span><br></pre></td></tr></table></figure><h3 id="常用训练和验证数据预处理"><a href="#常用训练和验证数据预处理" class="headerlink" title="常用训练和验证数据预处理"></a>常用训练和验证数据预处理</h3><p>其中 ToTensor 操作会将 PIL.Image 或形状为 H×W×D，数值范围为 [0, 255] 的 np.ndarray 转换为形状为 D×H×W，数值范围为 [0.0, 1.0] 的 torch.Tensor。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">train_transform = torchvision.transforms.Compose([ </span><br><span class="line">    torchvision.transforms.RandomResizedCrop(size=224,                                       </span><br><span class="line">                                             scale=(0.08, 1.0)),   </span><br><span class="line">    torchvision.transforms.RandomHorizontalFlip(),   </span><br><span class="line">    torchvision.transforms.ToTensor(),  </span><br><span class="line">    torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),                                </span><br><span class="line">                                     std=(0.229, 0.224, 0.225)</span><br><span class="line">]) </span><br><span class="line">val_transform = torchvision.transforms.Compose([  </span><br><span class="line">   torchvision.transforms.Resize(256),  </span><br><span class="line">   torchvision.transforms.CenterCrop(224),   </span><br><span class="line">   torchvision.transforms.ToTensor(), </span><br><span class="line">   torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),                                  </span><br><span class="line">                                    std=(0.229, 0.224, 0.225)),</span><br><span class="line">])</span><br></pre></td></tr></table></figure><h2 id="5-模型训练和测试"><a href="#5-模型训练和测试" class="headerlink" title="5. 模型训练和测试"></a>5. 模型训练和测试</h2><h3 id="分类模型训练代码"><a href="#分类模型训练代码" class="headerlink" title="分类模型训练代码"></a>分类模型训练代码</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># Loss and optimizer</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"># Train the model</span><br><span class="line">total_step = len(train_loader)</span><br><span class="line">for epoch in range(num_epochs):  </span><br><span class="line">    for i ,(images, labels) in enumerate(train_loader):    </span><br><span class="line">        images = images.to(device)    </span><br><span class="line">        labels = labels.to(device)      </span><br><span class="line">        </span><br><span class="line">        # Forward pass    </span><br><span class="line">        outputs = model(images)       </span><br><span class="line">        loss = criterion(outputs, labels)     </span><br><span class="line">        </span><br><span class="line">        # Backward and optimizer   </span><br><span class="line">        optimizer.zero_grad()     </span><br><span class="line">        loss.backward() </span><br><span class="line">        optimizer.step()      </span><br><span class="line">        </span><br><span class="line">        if (i+1) % 100 == 0:    </span><br><span class="line">            print(&#x27;Epoch: [&#123;&#125;/&#123;&#125;], Step: [&#123;&#125;/&#123;&#125;], Loss: &#123;&#125;&#x27;                 </span><br><span class="line">                  .format(epoch+1, num_epochs, i+1, total_step, loss.item()))</span><br></pre></td></tr></table></figure><h3 id="分类模型测试代码"><a href="#分类模型测试代码" class="headerlink" title="分类模型测试代码"></a>分类模型测试代码</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># Test the model</span><br><span class="line">model.eval()  # eval mode(batch norm uses moving mean/variance             </span><br><span class="line">              #instead of mini-batch mean/variance)</span><br><span class="line">with torch.no_grad(): </span><br><span class="line">    correct = 0  </span><br><span class="line">    total = 0 </span><br><span class="line">    for images, labels in test_loader:   </span><br><span class="line">        images = images.to(device)     </span><br><span class="line">        labels = labels.to(device)    </span><br><span class="line">        outputs = model(images)      </span><br><span class="line">        _, predicted = torch.max(outputs.data, 1)    </span><br><span class="line">        total += labels.size(0)      </span><br><span class="line">        correct += (predicted == labels).sum().item()   </span><br><span class="line">        </span><br><span class="line">        print(&#x27;Test accuracy of the model on the 10000 test images: &#123;&#125; %&#x27;      </span><br><span class="line">              .format(100 * correct / total))</span><br></pre></td></tr></table></figure><h3 id="自定义loss"><a href="#自定义loss" class="headerlink" title="自定义loss"></a>自定义loss</h3><p>继承torch.nn.Module类写自己的loss。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">class MyLoss(torch.nn.Moudle):  </span><br><span class="line">    def __init__(self):    </span><br><span class="line">        super(MyLoss, self).__init__()</span><br><span class="line">        </span><br><span class="line">    def forward(self, x, y):    </span><br><span class="line">        loss = torch.mean((x - y) ** 2)   </span><br><span class="line">        return loss</span><br></pre></td></tr></table></figure><h3 id="标签平滑（label-smoothing）"><a href="#标签平滑（label-smoothing）" class="headerlink" title="标签平滑（label smoothing）"></a>标签平滑（label smoothing）</h3><p>写一个label_smoothing.py的文件，然后在训练代码里引用，用LSR代替交叉熵损失即可。label_smoothing.py内容如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line">class LSR(nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self, e=0.1, reduction=&#x27;mean&#x27;):   </span><br><span class="line">        super().__init__()</span><br><span class="line">        </span><br><span class="line">        self.log_softmax = nn.LogSoftmax(dim=1)   </span><br><span class="line">        self.e = e     </span><br><span class="line">        self.reduction = reduction</span><br><span class="line">        </span><br><span class="line">    def _one_hot(self, labels, classes, value=1):   </span><br><span class="line">        &quot;&quot;&quot;        </span><br><span class="line">            Convert labels to one hot vectors</span><br><span class="line">            </span><br><span class="line">        Args:      </span><br><span class="line">            labels: torch tensor in format [label1, label2, label3, ...]       </span><br><span class="line">            classes: int, number of classes       </span><br><span class="line">            value: label value in one hot vector, default to 1</span><br><span class="line">            </span><br><span class="line">        Returns:        </span><br><span class="line">            return one hot format labels in shape [batchsize, classes]    </span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        </span><br><span class="line">        one_hot = torch.zeros(labels.size(0), classes)</span><br><span class="line">        </span><br><span class="line">        #labels and value_added  size must match     </span><br><span class="line">        labels = labels.view(labels.size(0), -1)   </span><br><span class="line">        value_added = torch.Tensor(labels.size(0), 1).fill_(value)</span><br><span class="line">        </span><br><span class="line">        value_added = value_added.to(labels.device)   </span><br><span class="line">        one_hot = one_hot.to(labels.device)</span><br><span class="line">        </span><br><span class="line">        one_hot.scatter_add_(1, labels, value_added)</span><br><span class="line">        </span><br><span class="line">        return one_hot</span><br><span class="line">        </span><br><span class="line">    def _smooth_label(self, target, length, smooth_factor): </span><br><span class="line">        &quot;&quot;&quot;convert targets to one-hot format, and smooth  </span><br><span class="line">        them.  </span><br><span class="line">        Args:      </span><br><span class="line">            target: target in form with [label1, label2, label_batchsize]        </span><br><span class="line">            length: length of one-hot format(number of classes)         </span><br><span class="line">            smooth_factor: smooth factor for label smooth</span><br><span class="line">       </span><br><span class="line">       Returns:        </span><br><span class="line">           smoothed labels in one hot format   </span><br><span class="line">       &quot;&quot;&quot;    </span><br><span class="line">       one_hot = self._one_hot(target, length, value=1 - smooth_factor)      </span><br><span class="line">       one_hot += smooth_factor / (length - 1)</span><br><span class="line">       </span><br><span class="line">       return one_hot.to(target.device)</span><br><span class="line">       </span><br><span class="line">    def forward(self, x, target):</span><br><span class="line">    </span><br><span class="line">        if x.size(0) != target.size(0):      </span><br><span class="line">            raise ValueError(&#x27;Expected input batchsize (&#123;&#125;) to match target batch_size(&#123;&#125;)&#x27;                </span><br><span class="line">                    .format(x.size(0), target.size(0)))</span><br><span class="line">                    </span><br><span class="line">        if x.dim() &lt; 2:       </span><br><span class="line">            raise ValueError(&#x27;Expected input tensor to have least 2 dimensions(got &#123;&#125;)&#x27;       </span><br><span class="line">                    .format(x.size(0)))</span><br><span class="line">                    </span><br><span class="line">        if x.dim() != 2:     </span><br><span class="line">            raise ValueError(&#x27;Only 2 dimension tensor are implemented, (got &#123;&#125;)&#x27;           </span><br><span class="line">                    .format(x.size()))</span><br><span class="line"></span><br><span class="line">        smoothed_target = self._smooth_label(target, x.size(1), self.e)     </span><br><span class="line">        x = self.log_softmax(x)   </span><br><span class="line">        loss = torch.sum(- x * smoothed_target, dim=1)</span><br><span class="line">        </span><br><span class="line">        if self.reduction == &#x27;none&#x27;:         </span><br><span class="line">            return loss</span><br><span class="line">            </span><br><span class="line">        elif self.reduction == &#x27;sum&#x27;:        </span><br><span class="line">            return torch.sum(loss)</span><br><span class="line">            </span><br><span class="line">        elif self.reduction == &#x27;mean&#x27;:      </span><br><span class="line">            return torch.mean(loss)</span><br><span class="line">            </span><br><span class="line">        else:         </span><br><span class="line">            raise ValueError(&#x27;unrecognized option, expect reduction to be one of none, mean, sum&#x27;)</span><br></pre></td></tr></table></figure><p>或者直接在训练文件里做label smoothing</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">for images, labels in train_loader:  </span><br><span class="line">    images, labels = images.cuda(), labels.cuda()  </span><br><span class="line">    N = labels.size(0) </span><br><span class="line">    # C is the number of classes.  </span><br><span class="line">    smoothed_labels = torch.full(size=(N, C), fill_value=0.1 / (C - 1)).cuda()  </span><br><span class="line">    smoothed_labels.scatter_(dim=1, index=torch.unsqueeze(labels, dim=1), value=0.9)</span><br><span class="line">    </span><br><span class="line">    score = model(images)  </span><br><span class="line">    log_prob = torch.nn.functional.log_softmax(score, dim=1)  </span><br><span class="line">    loss = -torch.sum(log_prob * smoothed_labels) / N   </span><br><span class="line">    optimizer.zero_grad()  </span><br><span class="line">    loss.backward() </span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure><h3 id="Mixup训练"><a href="#Mixup训练" class="headerlink" title="Mixup训练"></a>Mixup训练</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">beta_distribution = torch.distributions.beta.Beta(alpha, alpha)</span><br><span class="line">for images, labels in train_loader:  </span><br><span class="line">    images, labels = images.cuda(), labels.cuda()   </span><br><span class="line">    </span><br><span class="line">    # Mixup images and labels.  </span><br><span class="line">    lambda_ = beta_distribution.sample([]).item()   </span><br><span class="line">    index = torch.randperm(images.size(0)).cuda()  </span><br><span class="line">    mixed_images = lambda_ * images + (1 - lambda_) * images[index, :]  </span><br><span class="line">    label_a, label_b = labels, labels[index]  </span><br><span class="line">    </span><br><span class="line">    # Mixup loss.  </span><br><span class="line">    scores = model(mixed_images) </span><br><span class="line">    loss = (lambda_ * loss_function(scores, label_a)     </span><br><span class="line">            + (1 - lambda_) * loss_function(scores, label_b))  </span><br><span class="line">    optimizer.zero_grad() </span><br><span class="line">    loss.backward()  </span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure><h3 id="L1-正则化"><a href="#L1-正则化" class="headerlink" title="L1 正则化"></a>L1 正则化</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">l1_regularization = torch.nn.L1Loss(reduction=&#x27;sum&#x27;)</span><br><span class="line">loss = ...  # Standard cross-entropy loss</span><br><span class="line">for param in model.parameters():  </span><br><span class="line">    loss += torch.sum(torch.abs(param))</span><br><span class="line">loss.backward()</span><br></pre></td></tr></table></figure><h3 id="不对偏置项进行权重衰减（weight-decay）"><a href="#不对偏置项进行权重衰减（weight-decay）" class="headerlink" title="不对偏置项进行权重衰减（weight decay）"></a>不对偏置项进行权重衰减（weight decay）</h3><p>pytorch里的weight decay相当于l2正则</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bias_list = (param for name, param in model.named_parameters() if name[-4:] == &#x27;bias&#x27;)</span><br><span class="line">others_list = (param for name, param in model.named_parameters() if name[-4:] != &#x27;bias&#x27;)</span><br><span class="line">parameters = [&#123;&#x27;parameters&#x27;: bias_list, &#x27;weight_decay&#x27;: 0&#125;,                            </span><br><span class="line">              &#123;&#x27;parameters&#x27;: others_list&#125;]</span><br><span class="line">optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4)</span><br></pre></td></tr></table></figure><h3 id="梯度裁剪（gradient-clipping）"><a href="#梯度裁剪（gradient-clipping）" class="headerlink" title="梯度裁剪（gradient clipping）"></a>梯度裁剪（gradient clipping）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20)</span><br></pre></td></tr></table></figure><h3 id="得到当前学习率"><a href="#得到当前学习率" class="headerlink" title="得到当前学习率"></a>得到当前学习率</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># If there is one global learning rate (which is the common case).</span><br><span class="line">lr = next(iter(optimizer.param_groups))[&#x27;lr&#x27;]</span><br><span class="line"></span><br><span class="line"># If there are multiple learning rates for different layers.</span><br><span class="line">all_lr = []</span><br><span class="line">for param_group in optimizer.param_groups:  </span><br><span class="line">    all_lr.append(param_group[&#x27;lr&#x27;])</span><br></pre></td></tr></table></figure><p>另一种方法，在一个batch训练代码里，当前的lr是optimizer.param_groups[0][‘lr’]</p><h3 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># Reduce learning rate when validation accuarcy plateau.</span><br><span class="line">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=&#x27;max&#x27;, patience=5, verbose=True)</span><br><span class="line">for t in range(0, 80):  </span><br><span class="line">    train(...)   </span><br><span class="line">    val(...)  </span><br><span class="line">    scheduler.step(val_acc)</span><br><span class="line">    </span><br><span class="line"># Cosine annealing learning rate.</span><br><span class="line">scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=80)</span><br><span class="line"># Reduce learning rate by 10 at given epochs.</span><br><span class="line">scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 70], gamma=0.1)</span><br><span class="line">for t in range(0, 80):  </span><br><span class="line">    scheduler.step()    </span><br><span class="line">    train(...)  </span><br><span class="line">    val(...)</span><br><span class="line">    </span><br><span class="line"># Learning rate warmup by 10 epochs.</span><br><span class="line">scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: t / 10)</span><br><span class="line">for t in range(0, 10):  </span><br><span class="line">    scheduler.step()  </span><br><span class="line">    train(...)  </span><br><span class="line">    val(...)</span><br></pre></td></tr></table></figure><h3 id="优化器链式更新"><a href="#优化器链式更新" class="headerlink" title="优化器链式更新"></a>优化器链式更新</h3><p>从1.4版本开始，torch.optim.lr_scheduler 支持链式更新（chaining），即用户可以定义两个 schedulers，并交替在训练中使用。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch.optim import SGD</span><br><span class="line">from torch.optim.lr_scheduler import ExponentialLR, StepLR</span><br><span class="line">model = [torch.nn.Parameter(torch.randn(2, 2, requires_grad=True))]</span><br><span class="line">optimizer = SGD(model, 0.1)</span><br><span class="line">scheduler1 = ExponentialLR(optimizer, gamma=0.9)</span><br><span class="line">scheduler2 = StepLR(optimizer, step_size=3, gamma=0.1)</span><br><span class="line">for epoch in range(4):  </span><br><span class="line">    print(epoch, scheduler2.get_last_lr()[0]) </span><br><span class="line">    optimizer.step()  </span><br><span class="line">    scheduler1.step()  </span><br><span class="line">    scheduler2.step()</span><br></pre></td></tr></table></figure><h3 id="模型训练可视化"><a href="#模型训练可视化" class="headerlink" title="模型训练可视化"></a>模型训练可视化</h3><p>PyTorch可以使用tensorboard来可视化训练过程。</p><p>安装和运行TensorBoard。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install tensorboard</span><br><span class="line">tensorboard --logdir=runs</span><br></pre></td></tr></table></figure><p>使用SummaryWriter类来收集和可视化相应的数据，放了方便查看，可以使用不同的文件夹，比如’Loss/train’和’Loss/test’。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter()</span><br><span class="line"></span><br><span class="line">for n_iter in range(100):  </span><br><span class="line">    writer.add_scalar(&#x27;Loss/train&#x27;, np.random.random(), n_iter)   </span><br><span class="line">    writer.add_scalar(&#x27;Loss/test&#x27;, np.random.random(), n_iter)   </span><br><span class="line">    writer.add_scalar(&#x27;Accuracy/train&#x27;, np.random.random(), n_iter)  </span><br><span class="line">    writer.add_scalar(&#x27;Accuracy/test&#x27;, np.random.random(), n_iter)</span><br></pre></td></tr></table></figure><h3 id="保存与加载断点"><a href="#保存与加载断点" class="headerlink" title="保存与加载断点"></a>保存与加载断点</h3><p>注意为了能够恢复训练，我们需要同时保存模型和优化器的状态，以及当前的训练轮数。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">start_epoch = 0</span><br><span class="line"># Load checkpoint.</span><br><span class="line">if resume: # resume为参数，第一次训练时设为0，中断再训练时设为1  </span><br><span class="line">    model_path = os.path.join(&#x27;model&#x27;, &#x27;best_checkpoint.pth.tar&#x27;)  </span><br><span class="line">    assert os.path.isfile(model_path) </span><br><span class="line">    checkpoint = torch.load(model_path)  </span><br><span class="line">    best_acc = checkpoint[&#x27;best_acc&#x27;]  </span><br><span class="line">    start_epoch = checkpoint[&#x27;epoch&#x27;]  </span><br><span class="line">    model.load_state_dict(checkpoint[&#x27;model&#x27;])   </span><br><span class="line">    optimizer.load_state_dict(checkpoint[&#x27;optimizer&#x27;])  </span><br><span class="line">    print(&#x27;Load checkpoint at epoch &#123;&#125;.&#x27;.format(start_epoch)) </span><br><span class="line">    print(&#x27;Best accuracy so far &#123;&#125;.&#x27;.format(best_acc))</span><br><span class="line">    </span><br><span class="line"># Train the model</span><br><span class="line">for epoch in range(start_epoch, num_epochs):    </span><br><span class="line">    ... </span><br><span class="line">    </span><br><span class="line">    # Test the model  </span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">    # save checkpoint  </span><br><span class="line">    is_best = current_acc &gt; best_acc   </span><br><span class="line">    best_acc = max(current_acc, best_acc) </span><br><span class="line">    checkpoint = &#123;   </span><br><span class="line">        &#x27;best_acc&#x27;: best_acc,   </span><br><span class="line">        &#x27;epoch&#x27;: epoch + 1,   </span><br><span class="line">        &#x27;model&#x27;: model.state_dict(),   </span><br><span class="line">        &#x27;optimizer&#x27;: optimizer.state_dict(),   </span><br><span class="line">    &#125;  </span><br><span class="line">    model_path = os.path.join(&#x27;model&#x27;, &#x27;checkpoint.pth.tar&#x27;)    </span><br><span class="line">    best_model_path = os.path.join(&#x27;model&#x27;, &#x27;best_checkpoint.pth.tar&#x27;) </span><br><span class="line">    torch.save(checkpoint, model_path)  </span><br><span class="line">    if is_best:    </span><br><span class="line">        shutil.copy(model_path, best_model_path)</span><br></pre></td></tr></table></figure><h3 id="提取-ImageNet-预训练模型某层的卷积特征"><a href="#提取-ImageNet-预训练模型某层的卷积特征" class="headerlink" title="提取 ImageNet 预训练模型某层的卷积特征"></a>提取 ImageNet 预训练模型某层的卷积特征</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># VGG-16 relu5-3 feature.</span><br><span class="line">model = torchvision.models.vgg16(pretrained=True).features[:-1]</span><br><span class="line"># VGG-16 pool5 feature.</span><br><span class="line">model = torchvision.models.vgg16(pretrained=True).features</span><br><span class="line"># VGG-16 fc7 feature.</span><br><span class="line">model = torchvision.models.vgg16(pretrained=True)</span><br><span class="line">model.classifier = torch.nn.Sequential(*list(model.classifier.children())[:-3])</span><br><span class="line"># ResNet GAP feature.</span><br><span class="line">model = torchvision.models.resnet18(pretrained=True)</span><br><span class="line">model = torch.nn.Sequential(collections.OrderedDict(   </span><br><span class="line">    list(model.named_children())[:-1]))</span><br><span class="line">    </span><br><span class="line">with torch.no_grad():  </span><br><span class="line">    model.eval()  </span><br><span class="line">    conv_representation = model(image)</span><br></pre></td></tr></table></figure><h3 id="提取-ImageNet-预训练模型多层的卷积特征"><a href="#提取-ImageNet-预训练模型多层的卷积特征" class="headerlink" title="提取 ImageNet 预训练模型多层的卷积特征"></a>提取 ImageNet 预训练模型多层的卷积特征</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">class FeatureExtractor(torch.nn.Module): </span><br><span class="line">    &quot;&quot;&quot;Helper class to extract several convolution features from the given   </span><br><span class="line">    pre-trained model.</span><br><span class="line">    </span><br><span class="line">    Attributes:   </span><br><span class="line">        _model, torch.nn.Module.    </span><br><span class="line">        _layers_to_extract, list&lt;str&gt; or set&lt;str&gt;</span><br><span class="line">        </span><br><span class="line">    Example:     </span><br><span class="line">        &gt;&gt;&gt; model = torchvision.models.resnet152(pretrained=True)     </span><br><span class="line">        &gt;&gt;&gt; model = torch.nn.Sequential(collections.OrderedDict(          </span><br><span class="line">                list(model.named_children())[:-1]))    </span><br><span class="line">        &gt;&gt;&gt; conv_representation = FeatureExtractor(     </span><br><span class="line">                pretrained_model=model,           </span><br><span class="line">                layers_to_extract=&#123;&#x27;layer1&#x27;, &#x27;layer2&#x27;, &#x27;layer3&#x27;, &#x27;layer4&#x27;&#125;)(image)  </span><br><span class="line">    &quot;&quot;&quot;  </span><br><span class="line">    def __init__(self, pretrained_model, layers_to_extract):       </span><br><span class="line">        torch.nn.Module.__init__(self)   </span><br><span class="line">        self._model = pretrained_model   </span><br><span class="line">        self._model.eval()   </span><br><span class="line">        self._layers_to_extract = set(layers_to_extract)</span><br><span class="line">        </span><br><span class="line">    def forward(self, x):    </span><br><span class="line">        with torch.no_grad():     </span><br><span class="line">           conv_representation = []       </span><br><span class="line">           for name, layer in self._model.named_children():           </span><br><span class="line">               x = layer(x)       </span><br><span class="line">               if name in self._layers_to_extract:   </span><br><span class="line">                   conv_representation.append(x)    </span><br><span class="line">            return conv_representation</span><br></pre></td></tr></table></figure><h3 id="微调全连接层"><a href="#微调全连接层" class="headerlink" title="微调全连接层"></a>微调全连接层</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = torchvision.models.resnet18(pretrained=True)</span><br><span class="line">for param in model.parameters():  </span><br><span class="line">    param.requires_grad = False</span><br><span class="line">model.fc = nn.Linear(512, 100)  # Replace the last fc layer</span><br><span class="line">optimizer = torch.optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)</span><br></pre></td></tr></table></figure><h3 id="以较大学习率微调全连接层，较小学习率微调卷积层"><a href="#以较大学习率微调全连接层，较小学习率微调卷积层" class="headerlink" title="以较大学习率微调全连接层，较小学习率微调卷积层"></a>以较大学习率微调全连接层，较小学习率微调卷积层</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = torchvision.models.resnet18(pretrained=True)</span><br><span class="line">finetuned_parameters = list(map(id, model.fc.parameters()))</span><br><span class="line">conv_parameters = (p for p in model.parameters() if id(p) not in finetuned_parameters)</span><br><span class="line">parameters = [&#123;&#x27;params&#x27;: conv_parameters, &#x27;lr&#x27;: 1e-3&#125;,    </span><br><span class="line">              &#123;&#x27;params&#x27;: model.fc.parameters()&#125;]</span><br><span class="line">optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4)</span><br></pre></td></tr></table></figure><h2 id="ssh连接"><a href="#ssh连接" class="headerlink" title="ssh连接"></a>ssh连接</h2><p>ssh -L 16006:127.0.0.1:6006 <a href="mailto:&#x61;&#x63;&#x63;&#x6f;&#117;&#110;&#x74;&#64;&#115;&#101;&#x72;&#x76;&#x65;&#114;&#x2e;&#97;&#x64;&#100;&#114;&#101;&#115;&#115;">&#x61;&#x63;&#x63;&#x6f;&#117;&#110;&#x74;&#64;&#115;&#101;&#x72;&#x76;&#x65;&#114;&#x2e;&#97;&#x64;&#100;&#114;&#101;&#115;&#115;</a></p><p>服务器地址只写ip不写端口</p><p>16006是自己电脑的端口</p><p>6006是服务器上服务的端口。</p><h2 id="6-其他注意事项"><a href="#6-其他注意事项" class="headerlink" title="6. 其他注意事项"></a>6. 其他注意事项</h2><p>不要使用太大的线性层。因为nn.Linear(m,n)使用的是的内存，线性层太大很容易超出现有显存。</p><p>不要在太长的序列上使用RNN。因为RNN反向传播使用的是BPTT算法，其需要的内存和输入序列的长度呈线性关系。</p><p>model(x) 前用 model.train() 和 model.eval() 切换网络状态。</p><p>不需要计算梯度的代码块用 with torch.no_grad() 包含起来。</p><p>model.eval() 和 torch.no_grad() 的区别在于，model.eval() 是将网络切换为测试状态，例如 BN 和dropout在训练和测试阶段使用不同的计算方法。torch.no_grad() 是关闭 PyTorch 张量的自动求导机制，以减少存储使用和加速计算，得到的结果无法进行 loss.backward()。</p><p>model.zero_grad()会把整个模型的参数的梯度都归零, 而optimizer.zero_grad()只会把传入其中的参数的梯度归零.</p><p>torch.nn.CrossEntropyLoss 的输入不需要经过 Softmax。torch.nn.CrossEntropyLoss 等价于 torch.nn.functional.log_softmax + torch.nn.NLLLoss。</p><p>loss.backward() 前用 optimizer.zero_grad() 清除累积梯度。</p><p>torch.utils.data.DataLoader 中尽量设置 pin_memory=True，对特别小的数据集如 MNIST 设置 pin_memory=False 反而更快一些。num_workers 的设置需要在实验中找到最快的取值。</p><p>用 del 及时删除不用的中间变量，节约 GPU 存储。</p><p>使用 inplace 操作可节约 GPU 存储，如</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = torch.nn.functional.relu(x, inplace=True)</span><br></pre></td></tr></table></figure><p>减少 CPU 和 GPU 之间的数据传输。例如如果你想知道一个 epoch 中每个 mini-batch 的 loss 和准确率，先将它们累积在 GPU 中等一个 epoch 结束之后一起传输回 CPU 会比每个 mini-batch 都进行一次 GPU 到 CPU 的传输更快。</p><p>使用半精度浮点数 half() 会有一定的速度提升，具体效率依赖于 GPU 型号。需要小心数值精度过低带来的稳定性问题。</p><p>时常使用 assert tensor.size() == (N, D, H, W) 作为调试手段，确保张量维度和你设想中一致。</p><p>除了标记 y 外，尽量少使用一维张量，使用 n*1 的二维张量代替，可以避免一些意想不到的一维张量计算结果。</p><p>统计代码各部分耗时</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">with torch.autograd.profiler.profile(enabled=True, use_cuda=False) as profile:  </span><br><span class="line">    ...</span><br><span class="line">print(profile)</span><br><span class="line"></span><br><span class="line"># 或者在命令行运行</span><br><span class="line">python -m torch.utils.bottleneck main.py</span><br></pre></td></tr></table></figure><p>使用TorchSnooper来调试PyTorch代码，程序在执行的时候，就会自动 print 出来每一行的执行结果的 tensor 的形状、数据类型、设备、是否需要梯度的信息。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># pip install torchsnooper</span><br><span class="line">import torchsnooper</span><br><span class="line"></span><br><span class="line"># 对于函数，使用修饰器</span><br><span class="line">@torchsnooper.snoop()</span><br><span class="line"></span><br><span class="line"># 如果不是函数，使用 with 语句来激活 TorchSnooper，把训练的那个循环装进 with 语句中去。</span><br><span class="line">with torchsnooper.snoop():  </span><br><span class="line">    原本的代码</span><br></pre></td></tr></table></figure><p><a href="https://github.com/zasdfgbnm/TorchSnoopergithub.com">https://github.com/zasdfgbnm/TorchSnoopergithub.com</a></p><p>模型可解释性，使用captum库：<a href="https://captum.ai/captum.ai">https://captum.ai/captum.ai</a></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a><strong>参考资料</strong></h2><ol><li> 张皓：PyTorch Cookbook（常用代码段整理合集），<a href="https://zhuanlan.zhihu.com/p/59205847">https://zhuanlan.zhihu.com/p/59205847</a>?</li><li> PyTorch官方文档和示例</li><li> <a href="https://pytorch.org/docs/stable/notes/faq.html">https://pytorch.org/docs/stable/notes/faq.html</a></li><li> <a href="https://github.com/szagoruyko/pytorchviz">https://github.com/szagoruyko/pytorchviz</a></li><li> <a href="https://github.com/sksq96/pytorch-summary">https://github.com/sksq96/pytorch-summary</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;日志输出&quot;&gt;&lt;a href=&quot;#日志输出&quot; class=&quot;headerlink&quot; title=&quot;日志输出&quot;&gt;&lt;/a&gt;日志输出&lt;/h2&gt;&lt;p&gt;利用logging模块在控制台实时打印并及时记录运行日志。&lt;/p&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;from config import  *&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;import logging  # 引入logging模块&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;import os.path&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;class Logger:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    def __init__(self,mode=&amp;#x27;w&amp;#x27;):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        # 第一步，创建一个logger&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.logger = logging.getLogger()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.logger.setLevel(logging.INFO)  # Log等级总开关&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        # 第二步，创建一个handler，用于写入日志文件&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        rq = time.strftime(&amp;#x27;%Y%m%d%H%M&amp;#x27;, time.localtime(time.time()))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        log_path = os.getcwd() + &amp;#x27;/Logs/&amp;#x27;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        log_name = log_path + rq + &amp;#x27;.log&amp;#x27;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        logfile = log_name&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        fh = logging.FileHandler(logfile, mode=mode)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        fh.setLevel(logging.DEBUG)  # 输出到file的log等级的开关&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        # 第三步，定义handler的输出格式&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        formatter = logging.Formatter(&amp;quot;%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s&amp;quot;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        fh.setFormatter(formatter)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        # 第四步，将logger添加到handler里面&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.logger.addHandler(fh)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ch = logging.StreamHandler()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ch.setLevel(logging.INFO)  # 输出到console的log等级的开关&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ch.setFormatter(formatter)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.logger.addHandler(ch)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;



&lt;h2 id=&quot;模型的保存和读取&quot;&gt;&lt;a href=&quot;#模型的保存和读取&quot; class=&quot;headerlink&quot; title=&quot;模型的保存和读取&quot;&gt;&lt;/a&gt;模型的保存和读取&lt;/h2&gt;&lt;p&gt;​    &lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;保存&lt;/th&gt;
&lt;th align=&quot;left&quot;&gt;（1）torch.save(net.state_dict(),保存路径) 。（2）多卡训练时，要用 torch.save(net.module.state_dict(),’./model/best.pth’)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;&lt;tr&gt;
&lt;td&gt;读取&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;（1）net.load_state_dict(torch.load(‘best.pth’))。（2）并行时map_location=device.type在读取模型的时候一定要加上。即：    model.load_state_dict(torch.load(‘model/self_train_bestv2.pth’,map_location=’cuda’))&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td align=&quot;left&quot;&gt;torch.save(model.state_dict(),’checkpoint_0.tar’,_use_new_zipfile_serialization=False) #解决版本不兼容&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;</summary>
    
    
    
    
    <category term="pytorch" scheme="https://jpccc.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>深度学习训练tricks</title>
    <link href="https://jpccc.github.io/2022/04/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20tricks/"/>
    <id>https://jpccc.github.io/2022/04/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20tricks/</id>
    <published>2022-04-06T15:06:13.000Z</published>
    <updated>2022-04-07T08:44:25.458Z</updated>
    
    <content type="html"><![CDATA[<h1 id="深度神经网络模型训练中的-tricks（原理与代码汇总）"><a href="#深度神经网络模型训练中的-tricks（原理与代码汇总）" class="headerlink" title="深度神经网络模型训练中的 tricks（原理与代码汇总）"></a>深度神经网络模型训练中的 tricks（原理与代码汇总）</h1><p>本文总结了多种<strong>图像分类</strong>任务中的重要技巧，对于<strong>目标检测和图像分割</strong>等任务，也起到了不错的作用。</p><p>计算机视觉主要问题有图像分类、目标检测和图像分割等。针对图像分类任务，提升准确率的方法路线有两条，一个是模型的修改，另一个是各种数据处理和训练的技巧(tricks)。图像分类中的各种技巧对于目标检测、图像分割等任务也有很好的作用，因此值得好好总结。本文在精读论文的基础上，总结了图像分类任务的各种tricks如下：</p><span id="more"></span><h3 id="tricks合集"><a href="#tricks合集" class="headerlink" title="tricks合集"></a>tricks合集</h3><ul><li>Warmup</li><li>Linear scaling learning rate</li><li>Label-smoothing</li><li>Random image cropping and patching</li><li>Knowledge Distillation</li><li>Cutout</li><li>Random erasing</li><li>Cosine learning rate decay</li><li>Mixup training</li><li>AdaBoud</li><li>AutoAugment</li><li>其他经典的tricks</li></ul><h2 id="Warmup"><a href="#Warmup" class="headerlink" title="Warmup"></a>Warmup</h2><p>学习率是神经网络训练中最重要的超参数之一，针对学习率的技巧有很多。Warm up是在ResNet论文[1]中提到的一种学习率预热的方法。由于刚开始训练时模型的权重(weights)是随机初始化的(全部置为0是一个坑，原因见[2])，此时选择一个较大的学习率，可能会带来模型的不稳定。学习率预热就是在刚开始训练的时候先使用一个较小的学习率，训练一些epoches或iterations，等模型稳定时再修改为预先设置的学习率进行训练。论文[1]中使用一个110层的ResNet在cifar10上训练时，先用0.01的学习率训练直到训练误差低于80%(大概训练了400个iterations)，然后使用0.1的学习率进行训练。</p><p>上述的方法是constant warmup，18年Facebook又针对上面的warmup进行了改进[3]，因为从一个很小的学习率一下变为比较大的学习率可能会导致训练误差突然增大。论文[3]提出了gradual warmup来解决这个问题，即从最开始的小学习率开始，每个iteration增大一点，直到最初设置的比较大的学习率。</p><h2 id="Linear-scaling-learning-rate"><a href="#Linear-scaling-learning-rate" class="headerlink" title="Linear scaling learning rate"></a>Linear scaling learning rate</h2><p>Linear scaling learning rate是在论文[3]中针对比较大的batch size而提出的一种方法。</p><p>在凸优化问题中，随着批量的增加，收敛速度会降低，神经网络也有类似的实证结果。随着batch size的增大，处理相同数据量的速度会越来越快，但是达到相同精度所需要的epoch数量越来越多。也就是说，使用相同的epoch时，大batch size训练的模型与小batch size训练的模型相比，验证准确率会减小。</p><p>上面提到的gradual warmup是解决此问题的方法之一。另外，linear scaling learning rate也是一种有效的方法。在mini-batch SGD训练时，梯度下降的值是随机的，因为每一个batch的数据是随机选择的。增大batch size不会改变梯度的期望，但是会降低它的方差。也就是说，大batch size会降低梯度中的噪声，所以我们可以增大学习率来加快收敛。</p><p>具体做法很简单，比如ResNet原论文[1]中，batch size为256时选择的学习率是0.1，当我们把batch size变为一个较大的数b时，学习率应该变为 0.1 × b/256。</p><h2 id="Label-smoothing"><a href="#Label-smoothing" class="headerlink" title="Label-smoothing"></a>Label-smoothing</h2><p>在分类问题中，我们的最后一层一般是全连接层，然后对应标签的one-hot编码，即把对应类别的值编码为1，其他为0。这种编码方式和通过降低交叉熵损失来调整参数的方式结合起来，会有一些问题。这种方式会鼓励模型对不同类别的输出分数差异非常大，或者说，模型过分相信它的判断。但是，对于一个由多人标注的数据集，不同人标注的准则可能不同，每个人的标注也可能会有一些错误。<strong>模型对标签的过分相信会导致过拟合。</strong></p><p>标签平滑(Label-smoothing regularization,LSR)是应对该问题的有效方法之一，它的具体思想是降低我们对于标签的信任，例如我们可以将损失的目标值从1稍微降到0.9，或者将从0稍微升到0.1。标签平滑最早在inception-v2[4]中被提出，它将真实的概率改造为：</p><p><img src="https://jpccc.github.io/resource/deepLearning/640.jpeg" alt="图片"></p><p>其中，ε是一个小的常数，K是类别的数目，y是图片的真正的标签，i代表第i个类别，q_i是图片为第i类的概率。</p><p>总的来说，LSR是一种通过在标签y中加入噪声，实现对模型约束，降低模型过拟合程度的一种正则化方法。</p><p>LSR代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchimport torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSR</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, e=<span class="number">0.1</span>, reduction=<span class="string">&#x27;mean&#x27;</span></span>):</span>        </span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.log_softmax = nn.LogSoftmax(dim=<span class="number">1</span>)        </span><br><span class="line">        self.e = e        </span><br><span class="line">        self.reduction = reduction</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one_hot</span>(<span class="params">self, labels, classes, value=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;            </span></span><br><span class="line"><span class="string">        Convert labels to one hot vectors</span></span><br><span class="line"><span class="string">        Args:            </span></span><br><span class="line"><span class="string">        labels: torch tensor in format [label1, label2, label3, ...]            </span></span><br><span class="line"><span class="string">        classes: int, number of classes            </span></span><br><span class="line"><span class="string">        value: label value in one hot vector, default to 1</span></span><br><span class="line"><span class="string">        Returns: return one hot format labels in shape [batchsize, classes]        </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        one_hot = torch.zeros(labels.size(<span class="number">0</span>), classes)</span><br><span class="line">        <span class="comment">#labels and value_added  size must match        </span></span><br><span class="line">        labels = labels.view(labels.size(<span class="number">0</span>), -<span class="number">1</span>)        </span><br><span class="line">        value_added = torch.Tensor(labels.size(<span class="number">0</span>), <span class="number">1</span>).fill_(value)</span><br><span class="line">        value_added = value_added.to(labels.device)        </span><br><span class="line">        one_hot = one_hot.to(labels.device)</span><br><span class="line">        one_hot.scatter_add_(<span class="number">1</span>, labels, value_added)</span><br><span class="line"><span class="keyword">return</span> one_hot</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_smooth_label</span>(<span class="params">self, target, length, smooth_factor</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        convert targets to one-hot format, and smooth them.       </span></span><br><span class="line"><span class="string">        Args:            </span></span><br><span class="line"><span class="string">        target: target in form with [label1, label2, label_batchsize]            </span></span><br><span class="line"><span class="string">        length: length of one-hot format(number of classes)            </span></span><br><span class="line"><span class="string">        smooth_factor: smooth factor for label smooth</span></span><br><span class="line"><span class="string">        Returns:smoothed labels in one hot format        </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span>        </span><br><span class="line">        one_hot = self._one_hot(target, length, value=<span class="number">1</span>- smooth_factor)        </span><br><span class="line">        one_hot += smooth_factor / length</span><br><span class="line">        <span class="keyword">return</span> one_hot.to(target.device)</span><br></pre></td></tr></table></figure><h2 id="Random-image-cropping-and-patching"><a href="#Random-image-cropping-and-patching" class="headerlink" title="Random image cropping and patching"></a>Random image cropping and patching</h2><p>Random image cropping and patching (RICAP)[7]方法随机裁剪四个图片中的部分，然后把它们拼接为一个图片，同时混合这四个图片的标签。</p><p>RICAP在caifar10上达到了2.19%的错误率。</p><p><img src="https://jpccc.github.io/resource/deepLearning/640.jpeg" alt="图片"></p><p>如下图所示，Ix, Iy是原始图片的宽和高。w和h称为boundary position，它决定了四个裁剪得到的小图片的尺寸。w和h从beta分布Beta(β, β)中随机生成，β也是RICAP的超参数。最终拼接的图片尺寸和原图片尺寸保持一致。</p><p><img src="https://jpccc.github.io/resource/deepLearning/640-16493198579212.jpeg" alt="图片"></p><p><img src="https://jpccc.github.io/resource/deepLearning/640-16493198617344.jpeg" alt="图片"></p><h2 id="Knowledge-Distillation"><a href="#Knowledge-Distillation" class="headerlink" title="Knowledge Distillation"></a>Knowledge Distillation</h2><p>提高几乎所有机器学习算法性能的一种非常简单的方法是在相同的数据上训练许多不同的模型，然后对它们的预测进行平均。但是使用所有的模型集成进行预测是比较麻烦的，并且可能计算量太大而无法部署到大量用户。Knowledge Distillation(知识蒸馏)[8]方法就是应对这种问题的有效方法之一。</p><p>在知识蒸馏方法中，我们使用一个教师模型来帮助当前的模型（学生模型）训练。教师模型是一个较高准确率的预训练模型，因此学生模型可以在保持模型复杂度不变的情况下提升准确率。比如，可以使用ResNet-152作为教师模型来帮助学生模型ResNet-50训练。在训练过程中，我们会加一个蒸馏损失来惩罚学生模型和教师模型的输出之间的差异。</p><p>给定输入，假定p是真正的概率分布，z和r分别是学生模型和教师模型最后一个全连接层的输出。之前我们会用交叉熵损失l(p,softmax(z))来度量p和z之间的差异，这里的蒸馏损失同样用交叉熵。所以，使用知识蒸馏方法总的损失函数是</p><p><img src="https://jpccc.github.io/resource/deepLearning/640-16493202572062.jpeg" alt="图片"></p><p>上式中，第一项还是原来的损失函数，第二项是添加的用来惩罚学生模型和教师模型输出差异的蒸馏损失。其中，T是一个温度超参数，用来使softmax的输出更加平滑的。实验证明，用ResNet-152作为教师模型来训练ResNet-50，可以提高后者的准确率。</p><p>##Cutout</p><p>Cutout[9]是一种新的正则化方法。原理是在训练时随机把图片的一部分减掉，这样能提高模型的鲁棒性。它的来源是计算机视觉任务中经常遇到的物体遮挡问题。通过cutout生成一些类似被遮挡的物体，不仅可以让模型在遇到遮挡问题时表现更好，还能让模型在做决定时更多地考虑环境(context)。</p><p>效果如下图，每个图片的一小部分被cutout了。</p><p><img src="https://jpccc.github.io/resource/deepLearning/640-16493198658876.jpeg" alt="图片"></p><h2 id="Random-erasing"><a href="#Random-erasing" class="headerlink" title="Random erasing"></a>Random erasing</h2><p>Random erasing[6]其实和cutout非常类似，也是一种模拟物体遮挡情况的数据增强方法。区别在于，cutout是把图片中随机抽中的矩形区域的像素值置为0，相当于裁剪掉，random erasing是用随机数或者数据集中像素的平均值替换原来的像素值。而且，cutout每次裁剪掉的区域大小是固定的，Random erasing替换掉的区域大小是随机的。</p><p><img src="https://jpccc.github.io/resource/deepLearning/640-16493198686898.jpeg" alt="图片"></p><p><img src="https://jpccc.github.io/resource/deepLearning/640-16493202724694.jpeg" alt="图片"></p><h2 id="Cosine-learning-rate-decay"><a href="#Cosine-learning-rate-decay" class="headerlink" title="Cosine learning rate decay"></a>Cosine learning rate decay</h2><p>在warmup之后的训练过程中，学习率不断衰减是一个提高精度的好方法。其中有step decay和cosine decay等，前者是随着epoch增大学习率不断减去一个小的数，后者是让学习率随着训练过程曲线下降。</p><p>对于cosine decay，假设总共有T个batch（不考虑warmup阶段），在第t个batch时，学习率η_t为：</p><p><img src="https://jpccc.github.io/resource/deepLearning/640-164931987398310.jpeg" alt="图片"></p><p>这里，η代表初始设置的学习率。这种学习率递减的方式称之为cosine decay。</p><p>下面是带有warmup的学习率衰减的可视化图[4]。其中，图(a)是学习率随epoch增大而下降的图，可以看出cosine decay比step decay更加平滑一点。图(b)是准确率随epoch的变化图，两者最终的准确率没有太大差别，不过cosine decay的学习过程更加平滑。</p><p><img src="https://jpccc.github.io/resource/deepLearning/640-164931987712312.jpeg" alt="图片"></p><p>在pytorch的torch.optim.lr_scheduler中有更多的学习率衰减的方法，至于哪个效果好，可能对于不同问题答案是不一样的。</p><h2 id="Mixup-training"><a href="#Mixup-training" class="headerlink" title="Mixup training"></a>Mixup training</h2><p>Mixup[10]是一种新的数据增强的方法。Mixup training，就是每次取出2张图片，然后将它们线性组合，得到新的图片，以此来作为新的训练样本，进行网络的训练，如下公式，其中x代表图像数据，y代表标签，则得到的新的xhat, yhat。</p><p><img src="https://jpccc.github.io/resource/deepLearning/640-16493202939116.jpeg" alt="图片"></p><p>其中，λ是从Beta(α, α)随机采样的数，在[0,1]之间。在训练过程中，仅使用(xhat, yhat)。</p><p>Mixup方法主要增强了训练样本之间的线性表达，增强网络的泛化能力，不过mixup方法需要较长的时间才能收敛得比较好。</p><h2 id="AdaBound"><a href="#AdaBound" class="headerlink" title="AdaBound"></a><strong>AdaBound</strong></h2><p>AdaBound是最近一篇论文[5]中提到的，按照作者的说法，AdaBound会让你的训练过程像adam一样快，并且像SGD一样好。</p><p>如下图所示，使用AdaBound会收敛速度更快，过程更平滑，结果更好。</p><p><img src="https://jpccc.github.io/resource/deepLearning/640-164931988094314.jpeg" alt="图片"></p><p>另外，这种方法相对于SGD对超参数的变化不是那么敏感，也就是说鲁棒性更好。但是，针对不同的问题还是需要调节超参数的，只是所用的时间可能变少了。</p><p><img src="https://jpccc.github.io/resource/deepLearning/640-164931988818716.jpeg" alt="图片"></p><p>当然，AdaBound还没有经过普遍的检验，也有可能只是对于某些问题效果好。</p><p>使用方法如下：安装AdaBound</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install adabound</span><br></pre></td></tr></table></figure><p>使用AdaBound(和其他PyTorch optimizers用法一致)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=0.1)</span><br></pre></td></tr></table></figure><h2 id="AutoAugment"><a href="#AutoAugment" class="headerlink" title="AutoAugment"></a>AutoAugment</h2><p>数据增强在图像分类问题上有很重要的作用，但是增强的方法有很多，并非一股脑地用上所有的方法就是最好的。那么，如何选择最佳的数据增强方法呢？AutoAugment[11]就是一种搜索适合当前问题的数据增强方法的方法。该方法创建一个数据增强策略的搜索空间，利用搜索算法选取适合特定数据集的数据增强策略。此外，从一个数据集中学到的策略能够很好地迁移到其它相似的数据集上。</p><p>AutoAugment在cifar10上的表现如下表，达到了98.52%的准确率。</p><p><img src="https://jpccc.github.io/resource/deepLearning/640-164931989171518.jpeg" alt="图片"></p><h2 id="其他经典的tricks"><a href="#其他经典的tricks" class="headerlink" title="其他经典的tricks"></a>其他经典的tricks</h2><h3 id="常用的正则化方法为"><a href="#常用的正则化方法为" class="headerlink" title="常用的正则化方法为"></a><strong>常用的正则化方法为</strong></h3><ul><li>Dropout</li><li>L1/L2正则</li><li>Batch Normalization</li><li>Early stopping</li><li>Random cropping</li><li>Mirroring</li><li>Rotation</li><li>Color shifting</li><li>PCA color augmentation</li><li>…</li></ul><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a><strong>其他</strong></h3><ul><li>Xavier init[12]</li><li>…</li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a><strong>参考资料</strong></h2><p>[1] Deep Residual Learning for Image Recognition(<a href="https://arxiv.org/pdf/1512.03385.pdf">https://arxiv.org/pdf/1512.03385.pdf</a></p><p>[2] <a href="http://cs231n.github.io/neural-networks-2/">http://cs231n.github.io/neural-networks-2/</a></p><p>[3] Accurate, Large Minibatch SGD:Training ImageNet in 1 Hour <a href="https://arxiv.org/pdf/1706.02677v2.pdf">https://arxiv.org/pdf/1706.02677v2.pdf</a></p><p>[4] <a href="https://mp.weixin.qq.com/s/Xnlp7JGx5O_I6ZmjuQ1UGQ">深度神经网络模型训练中的 tricks（原理与代码汇总） (qq.com)</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;深度神经网络模型训练中的-tricks（原理与代码汇总）&quot;&gt;&lt;a href=&quot;#深度神经网络模型训练中的-tricks（原理与代码汇总）&quot; class=&quot;headerlink&quot; title=&quot;深度神经网络模型训练中的 tricks（原理与代码汇总）&quot;&gt;&lt;/a&gt;深度神经网络模型训练中的 tricks（原理与代码汇总）&lt;/h1&gt;&lt;p&gt;本文总结了多种&lt;strong&gt;图像分类&lt;/strong&gt;任务中的重要技巧，对于&lt;strong&gt;目标检测和图像分割&lt;/strong&gt;等任务，也起到了不错的作用。&lt;/p&gt;
&lt;p&gt;计算机视觉主要问题有图像分类、目标检测和图像分割等。针对图像分类任务，提升准确率的方法路线有两条，一个是模型的修改，另一个是各种数据处理和训练的技巧(tricks)。图像分类中的各种技巧对于目标检测、图像分割等任务也有很好的作用，因此值得好好总结。本文在精读论文的基础上，总结了图像分类任务的各种tricks如下：&lt;/p&gt;</summary>
    
    
    
    
    <category term="深度学习" scheme="https://jpccc.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Understanding Graphs, Automatic Differentiation and Autograd</title>
    <link href="https://jpccc.github.io/2022/04/06/pytorch%E8%AE%A1%E7%AE%97%E5%9B%BE/"/>
    <id>https://jpccc.github.io/2022/04/06/pytorch%E8%AE%A1%E7%AE%97%E5%9B%BE/</id>
    <published>2022-04-05T16:08:00.000Z</published>
    <updated>2022-04-06T01:32:53.828Z</updated>
    
    <content type="html"><![CDATA[<div align=center><p><img src="https://jpccc.github.io/resource/pytorch/full_graph.png" alt="img"></p></div><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>PyTorch is one of the foremost python deep learning libraries out there. It’s the go to choice for deep learning research, and as each days passes by, more and more companies and research labs are adopting this library.</p><p>In this series of tutorials, we will be introducing you to PyTorch, and how to make the best use of the libraries as well the ecosystem of tools built around it. We’ll first cover the basic building blocks, and then move onto how you can quickly prototype custom architectures. We will finally conclude with a couple of posts on how to scale your code, and how to debug your code if things go awry.</p><p>This is Part 1 of our PyTorch 101 series.</p><span id="more"></span><ol><li><a href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/">Understanding Graphs, Automatic Differentiation and Autograd</a></li><li><a href="https://blog.paperspace.com/pytorch-101-building-neural-networks/">Building Your First Neural Network</a></li><li><a href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/blog.paperspace.com/pytorch-101-advanced/">Going Deep with PyTorch</a></li><li><a href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/blog.paperspace.com/pytorch-memory-multi-gpu-debugging/">Memory Management and Using Multiple GPUs</a></li><li><a href="https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/">Understanding Hooks</a></li></ol><p>You can get all the code in this post, (and other posts as well) in the Github repo <a href="https://github.com/Paperspace/PyTorch-101-Tutorial-Series">here</a>.</p><hr><h2 id="Prerequisites"><a href="#Prerequisites" class="headerlink" title="Prerequisites"></a>Prerequisites</h2><ol><li>Chain rule</li><li>Basic Understanding of Deep Learning</li><li>PyTorch 1.0</li></ol><hr><p>You can get all the code in this post, (and other posts as well) in the Github repo <a href="https://github.com/Paperspace/PyTorch-101-Tutorial-Series">here</a>.</p><h2 id="Automatic-Differentiation"><a href="#Automatic-Differentiation" class="headerlink" title="Automatic Differentiation"></a><strong>Automatic</strong> Differentiation</h2><p>A lot of tutorial series on PyTorch would start begin with a rudimentary discussion of what the basic structures are. However, I’d like to instead start by discussing automatic differentiation first.</p><p>Automatic Differentiation is a building block of not only PyTorch, but every DL library out there. In my opinion, PyTorch’s automatic differentiation engine, called <em>Autograd</em> is a brilliant tool to understand how automatic differentiation works. This will not only help you understand PyTorch better, but also other DL libraries.</p><p>Modern neural network architectures can have millions of learnable parameters. From a computational point of view, training a neural network consists of two phases:</p><ol><li>A forward pass to compute the value of the loss function.</li><li>A backward pass to compute the gradients of the learnable parameters.</li></ol><p>The forward pass is pretty straight forward. The output of one layer is the input to the next and so forth.</p><p>Backward pass is a bit more complicated since it requires us to use the chain rule to compute the gradients of weights w.r.t to the loss function.</p><h2 id="A-toy-example"><a href="#A-toy-example" class="headerlink" title="A toy example"></a>A toy example</h2><p>Let us take an very simple neural network consisting of just 5 neurons. Our neural network looks like the following.</p><div align=center><p><img src="https://jpccc.github.io//resource/pytorch/full_graph-16491739555532.png" alt="img"></p></div><p>The following equations describe our neural network.</p><p>$$<br>b=w1∗a\\<br>c=w2∗a\\<br>d=w3∗b+w4∗c\\<br>L=10−d\\<br>$$</p><p>Let us compute the gradients for each of the learnable parameters ww.</p><p>$$<br>\frac{\partial{L}}{\partial{w_4}} = \frac{\partial{L}}{\partial{d}}\frac{\partial{d}}{\partial{w_4}} \\<br>\frac{\partial{L}}{\partial{w_3}} = \frac{\partial{L}}{\partial{d}}\frac{\partial{d}}{\partial{w_3}}\\<br>\frac{\partial{L}}{\partial{w_2}} = \frac{\partial{L}}{\partial{d}} * \frac{\partial{d}}{\partial{c}} * \frac{\partial{c}}{\partial{w_2}}\\<br>\frac{\partial{L}}{\partial{w_1}} = \frac{\partial{L}}{\partial{d}}* \frac{\partial{d}}{\partial{b}} * \frac{\partial{b}}{\partial{w_1}}\\<br>$$</p><p>All these gradients have been computed by applying the chain rule. Note that all the individual gradients on the right hand side of the equations mentioned above can be computed directly since the <em>numerators</em> of the gradients are explicit functions of the <em>denominators.</em></p><hr><h2 id="Computation-Graphs"><a href="#Computation-Graphs" class="headerlink" title="Computation Graphs"></a>Computation Graphs</h2><p>We could manually compute the gradients of our network as it was very simple. Imagine, what if you had a network with 152 layers. Or, if the network had multiple branches.</p><p>When we design software to implement neural networks, we want to come up with a way that can allow us to seamlessly compute the gradients, regardless of the architecture type so that the programmer doesn’t have to manually compute gradients when changes are made to the network.</p><p>We galvanize(激励,启发) this idea in form of a data structure called a <strong>Computation graph</strong>. A computation graph looks very similar to the diagram of the graph that we made in the image above. However, the nodes in a computation graph are basically <strong>operators</strong>. These operators are basically the mathematical operators except for one case, where we need to represent creation of a user-defined variable.</p><p>Notice that we have also denoted(表示…) the leaf variables <strong>a,w1,w2,w3,w4</strong> in the graph for sake of clarity. However, it should noted that they are not a part of the computation graph. What they represent in our graph is the special case for user-defined variables which we just covered as an exception.</p><div align=center><p><img src="https://jpccc.github.io/resource/pytorch/computation_graph.png" alt="img"></p></div><p>The variables, <em>b,c</em> and <em>d</em> are created as a result of mathematical operations, whereas variables <em>a, w1, w2, w3</em> and <em>w4</em> are initialised by the user itself. Since, they are not created by any mathematical operator, nodes corresponding to their creation is represented by their name itself. This is true for all the <em>leaf</em> nodes in the graph.</p><hr><h2 id="Computing-the-gradients"><a href="#Computing-the-gradients" class="headerlink" title="Computing the gradients"></a>Computing the gradients</h2><p>Now, we are ready to describe how we will compute gradients using a computation graph.</p><p>Each node of the computation graph, with the exception of(除了…外) leaf nodes, can be considered as a function which takes some inputs and produces an output. Consider the node of the graph which produces variable <em>d</em> from w4cand w3b. Therefore we can write,</p><p>$$<br>d=f(w_3b,w_4c)<br>$$</p><div align=center><p><img src="https://jpccc.github.io/resource/pytorch/d_mini.png" alt="img"></p></div><p>​                                                                                            <center>    d is output of function f(x,y) = x + y  </center></p><p>Now, we can easily compute the gradient of the ff with respect to it’s inputs, $\frac{\partial{f}}{\partial{w_3b}}$ and $\frac{\partial{f}}{\partial{w_4b}}$ (which are both 1). Now, label the edges coming into the nodes with their respective gradients like the following image.</p><div align=center><p><img src="https://jpccc.github.io/resource/pytorch/d_mini_grad.png" alt="img"></p></div><p>​                                                                                                            <center>    Local Gradients </center></p><p>We do it for the entire graph. The graph looks like this.</p><div align=center><p><img src="https://jpccc.github.io/resource/pytorch/full_graph-16491739555532.png" alt="img"></p></div><center>Back propagation in an Computational Graph</center><p>Following we describe the algorithm for computing derivative(微分) of any node in this graph with respect to the loss, LL. Let’s say we want to compute the derivative, $\frac{\partial{f}}{\partial{w_4}}$</p><ol><li>We first trace down all possible paths from <em>d</em> to $w_4$.</li><li>There is only one such path.</li><li>We multiply all the edges long this path.</li></ol><p>If you see, the product is precisely the same expression we derived using chain rule. If there is more than one path to a variable from <em>L</em> then, we multiply the edges along each path and then add them together. For example,$\frac{\partial{L}}{\partial{a}}$  is computed as</p><p>$$<br>\frac{\partial{L}}{\partial{w_4}} = \frac{\partial{L}}{\partial{d}}*\frac{\partial{d}}{\partial{b}}*\frac{\partial{b}}{\partial{a}} + \frac{\partial{L}}{\partial{d}}*\frac{\partial{d}}{\partial{c}}*\frac{\partial{c}}{\partial{a}}<br>$$</p><h2 id="PyTorch-Autograd"><a href="#PyTorch-Autograd" class="headerlink" title="PyTorch Autograd"></a>PyTorch Autograd</h2><p>Now we get what a computational graph is, let’s get back to PyTorch and understand how the above is implemented in PyTorch.</p><p><em><strong>(注意：对谁求导，对应的导数就保存在对应变量中)</strong></em></p><h3 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h3><p><code>Tensor</code> is a data structure which is a fundamental building block of PyTorch. <code>Tensor</code>s are pretty much like numpy arrays, except that unlike numpy, tensors are designed to take advantage of parallel computation capabilities of a GPU. A lot of Tensor syntax(语法) is similar to that of numpy arrays.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]:  <span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: tsr = torch.Tensor(<span class="number">3</span>,<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: tsr</span><br><span class="line">Out[<span class="number">3</span>]: </span><br><span class="line">tensor([[ <span class="number">0.0000e+00</span>,  <span class="number">0.0000e+00</span>,  <span class="number">8.4452e-29</span>, -<span class="number">1.0842e-19</span>,  <span class="number">1.2413e-35</span>],</span><br><span class="line">        [ <span class="number">1.4013e-45</span>,  <span class="number">1.2416e-35</span>,  <span class="number">1.4013e-45</span>,  <span class="number">2.3331e-35</span>,  <span class="number">1.4013e-45</span>],</span><br><span class="line">        [ <span class="number">1.0108e-36</span>,  <span class="number">1.4013e-45</span>,  <span class="number">8.3641e-37</span>,  <span class="number">1.4013e-45</span>,  <span class="number">1.0040e-36</span>]])</span><br></pre></td></tr></table></figure><p>One it’s own, <code>Tensor</code> is just like a numpy <code>ndarray</code>. A data structure that can let you do fast linear algebra options. If you want PyTorch to create a graph corresponding to these operations, you will have to set the <code>requires_grad</code> attribute of the <code>Tensor</code> to True.</p><p>The API can be a bit confusing here. There are multiple ways to initialise tensors in PyTorch. While some ways can let you explicitly define that the <code>requires_grad</code> in the constructor itself, others require you to set it manually after creation of the Tensor.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; t1 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>) </span><br><span class="line"></span><br><span class="line">&gt;&gt; t2 = torch.FloatTensor(<span class="number">3</span>,<span class="number">3</span>) <span class="comment"># No way to specify requires_grad while initiating </span></span><br><span class="line">&gt;&gt; t2.requires_grad = <span class="literal">True</span></span><br></pre></td></tr></table></figure><p><code>requires_grad</code> is contagious. It means that when a <code>Tensor</code> is created by operating on other <code>Tensor</code>s, the <code>requires_grad</code> of the resultant <code>Tensor</code> would be set <code>True</code> given at least one of the tensors used for creation has it’s <code>requires_grad</code> set to <code>True</code>.</p><p>Each <code>Tensor</code> has a something an attribute called <code>grad_fn</code><em>,</em> which refers to the <strong>mathematical operator that create the variable</strong>. If <code>requires_grad</code> is set to False, <code>grad_fn</code> would be None.</p><p>In our example where, $d=f(w_3b,w_4c)$, <em>d</em>‘s grad function would be the addition operator, since <em>f</em> adds it’s to input together. Notice, addition operator is also the node in our graph that output’s <em>d</em>. If our <code>Tensor</code> is a leaf node (initialised by the user), then the <code>grad_fn</code> is also None.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line">a = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">w1 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w2 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w3 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w4 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">b = w1*a </span><br><span class="line">c = w2*a</span><br><span class="line"></span><br><span class="line">d = w3*b + w4*c </span><br><span class="line"></span><br><span class="line">L = <span class="number">10</span> - d</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The grad fn for a is&quot;</span>, a.grad_fn)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The grad fn for d is&quot;</span>, d.grad_fn)</span><br></pre></td></tr></table></figure><p>If you run the code above, you get the following output.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">The grad fn <span class="keyword">for</span> a <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line">The grad fn <span class="keyword">for</span> d <span class="keyword">is</span> &lt;AddBackward0 <span class="built_in">object</span> at <span class="number">0x1033afe48</span>&gt;</span><br></pre></td></tr></table></figure><p>One can use the member function <code>is_leaf</code> to determine whether a variable is a leaf <code>Tensor</code> or not.</p><h3 id="Function"><a href="#Function" class="headerlink" title="Function"></a>Function</h3><p>All mathematical operations in PyTorch are implemented by the <em>torch.nn.Autograd.Function</em> class. This class has two important member functions we need to look at.</p><ul><li>The first is it’s <em>forward</em>  function, which simply computes the output using it’s inputs.</li></ul><ul><li>The <code>backward</code> function takes the incoming gradient coming from the the part of the network in front of it. As you can see, the gradient to be backpropagated from a function <em>f</em> is basically the <strong>gradient that is backpropagated to f from the layers in front of it</strong> multiplied by <strong>the local gradient of the output of f with respect to it’s inputs</strong>（链式规则）. This is exactly what the <code>backward</code> function does.</li></ul><p>Let’s again understand with our example of<br>$$<br>d=f(w_3b,w_4c)<br>$$</p><ol><li><em>d</em> is our <code>Tensor</code> here. It’s <code>grad_fn</code> is <code>&lt;ThAddBackward&gt;</code><em>.</em> This is basically the addition operation since the function that creates <em>d</em> adds inputs.</li><li>The <code>forward</code> function of the it’s <code>grad_fn</code> receives the inputs $w_3b$ <em>and</em> $w_4c$ and adds them. This value is basically stored in the <em>d</em>.</li><li>The <code>backward</code> function of the <code>&lt;ThAddBackward&gt;</code> basically takes the the <strong>incoming gradient</strong> from the further layers as the input. This is basically $\frac{\partial{L}}{\partial{d}}$ coming along the edge leading from <em>L</em> to <em>d.</em> This gradient is also the gradient of <em>L</em> w.r.t to <em>d</em> and is stored in <code>grad</code> attribute of the <code>d</code>. It can be accessed by calling <code>d.grad</code><em>.</em></li><li>It then takes computes the local gradients $\frac{\partial{d}}{\partial{w_4c}}$and$\frac{\partial{d}}{\partial{w_3b}}$.</li><li>Then the backward function multiplies the incoming gradient with the <strong>locally computed gradients</strong> respectively and <em><strong>“<em><strong>sends</strong></em>“</strong></em> the gradients to it’s inputs by invoking the backward method of the <code>grad_fn</code> of their inputs.</li><li>For example, the <code>backward</code> function of <code>&lt;ThAddBackward&gt;</code> associated with <em>d</em> invokes(援引，调用) backward function of the <em>grad_fn</em> of the $w_4∗c$∗(Here, $w_4∗c$ is a intermediate Tensor, and it’s <em>grad_fn</em> is <code>&lt;ThMulBackward&gt;</code>. At time of invocation of the <code>backward</code> function, the gradient $\frac{\partial{L}}{\partial{d}}*\frac{\partial{d}}{\partial{w_4c}}$ is passed as the input.</li><li>Now, for the variable $w_4∗c$, $\frac{\partial{L}}{\partial{d}}*\frac{\partial{d}}{\partial{w_4c}}$ becomes the incoming gradient, $\frac{\partial{L}}{\partial{d}}$ was for $d$ in step 3 and the process repeats.</li></ol><p>Algorithmically, here’s how back propagation happens with a computation graph. (Not the actual implementation, only representative)</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def backward (incoming_gradients):</span><br><span class="line">self.Tensor.grad = incoming_gradients</span><br><span class="line"></span><br><span class="line">for inp in self.inputs:</span><br><span class="line">if inp.grad_fn is not None:</span><br><span class="line">new_incoming_gradients = //</span><br><span class="line">  incoming_gradient * local_grad(self.Tensor, inp)</span><br><span class="line"></span><br><span class="line">inp.grad_fn.backward(new_incoming_gradients)</span><br><span class="line">else:</span><br><span class="line">pass</span><br></pre></td></tr></table></figure><p>Here, <code>self.Tensor</code> is basically the <code>Tensor</code> created by Autograd.Function, which was <em>d</em> in our example.</p><p>Incoming gradients and local gradients have been described above.</p><hr><p>In order to compute derivatives in our neural network, we generally call <code>backward</code> on the <code>Tensor</code> representing our loss. Then, we backtrack through the graph starting from node representing the <code>grad_fn</code> of our loss.</p><p>As described above, the <code>backward</code> function is recursively called through out the graph as we backtrack. Once, we reach a leaf node, since the <code>grad_fn</code> is None, but stop backtracking through that path.</p><p>One thing to note here is that PyTorch gives an error if you call <code>backward()</code> on vector-valued Tensor. This means you can only call <code>backward</code> on a scalar valued Tensor. In our example, if we assume <code>a</code> to be a vector valued Tensor, and call <code>backward</code> on L, it will throw up an error.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line">a = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">w1 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w2 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w3 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w4 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">b = w1*a </span><br><span class="line">c = w2*a</span><br><span class="line"></span><br><span class="line">d = w3*b + w4*c </span><br><span class="line"></span><br><span class="line">L = (<span class="number">10</span> - d)</span><br><span class="line"></span><br><span class="line">L.backward()</span><br></pre></td></tr></table></figure><p>Running the above snippet results in the following error.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RuntimeError: grad can be implicitly created only <span class="keyword">for</span> scalar outputs</span><br></pre></td></tr></table></figure><p>This is because gradients can be computed with respect to scalar values by definition. You can’t exactly differentiate a vector with respect to another vector. The mathematical entity used for such cases is called a <strong>Jacobian,</strong> the discussion of which is beyond the scope of this article.</p><p>There are two ways to overcome this.</p><p>If you just make a small change in the above code setting <code>L</code> to be the sum of all the errors, our problem will be solved.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"></span><br><span class="line">a = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">w1 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w2 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w3 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">w4 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">b = w1*a </span><br><span class="line">c = w2*a</span><br><span class="line"></span><br><span class="line">d = w3*b + w4*c </span><br><span class="line"></span><br><span class="line"><span class="comment"># Replace L = (10 - d) by </span></span><br><span class="line">L = (<span class="number">10</span> -d).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">L.backward()</span><br></pre></td></tr></table></figure><p>Once that’s done, you can access the gradients by calling the <code>grad</code> attribute of <code>Tensor</code>.</p><p><strong>Second way is</strong>, for some reason have to absolutely call <code>backward</code> on a vector function, you can pass a <code>torch.ones</code> of size of shape of the tensor you are trying to call backward with.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Replace L.backward() with </span></span><br><span class="line">L.backward(torch.ones(L.shape))</span><br></pre></td></tr></table></figure><p>Notice how <code>backward</code> used to take incoming gradients as it’s input. Doing the above makes the <code>backward</code> think that incoming gradient are just Tensor of ones of same size as L, and it’s able to back propagate.</p><p>In this way, we can have gradients for every <code>Tensor</code> , and we can update them using Optimisation algorithm of our choice.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w1 = w1 - learning_rate * w1.grad</span><br></pre></td></tr></table></figure><p>And so on.</p><h2 id="How-are-PyTorch’s-graphs-different-from-TensorFlow-graphs"><a href="#How-are-PyTorch’s-graphs-different-from-TensorFlow-graphs" class="headerlink" title="How are PyTorch’s graphs different from TensorFlow graphs"></a>How are PyTorch’s graphs different from TensorFlow graphs</h2><p>PyTorch creates something called a <strong>Dynamic Computation Graph,</strong> which means that the graph is generated on the fly.</p><p>Until the <code>forward</code> function of a Variable is called, there exists no node for the <code>Tensor</code> <em>(<em>it’s <code>grad_fn</code></em>)</em> in the graph.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)   <span class="comment">#No graph yet, as a is a leaf</span></span><br><span class="line"></span><br><span class="line">w1 = torch.randn((<span class="number">3</span>,<span class="number">3</span>), requires_grad = <span class="literal">True</span>)  <span class="comment">#Same logic as above</span></span><br><span class="line"></span><br><span class="line">b = w1*a   <span class="comment">#Graph with node `mulBackward` is created.</span></span><br></pre></td></tr></table></figure><p>The graph is created as a result of <code>forward</code> function of many <em>Tensors</em> being invoked. Only then, the buffers for the non-leaf nodes allocated for the graph and intermediate values (used for computing gradients later.  When you call <code>backward</code>, as the gradients are computed, these buffers (for non-leaf variables) are essentially freed, and the graph is <em>destroyed</em> ( In a sense, you can’t backpropagate through it since the buffers holding values to compute the gradients are gone).</p><p>Next time, you will call <code>forward</code> on the same set of tensors, <strong>the leaf node buffers from the previous run will be shared, while the non-leaf nodes buffers will be created again.</strong></p><p>If you call <code>backward</code> more than once on a graph with non-leaf nodes, you’ll be met with the following error.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=<span class="literal">True</span> when calling backward the first time.</span><br></pre></td></tr></table></figure><p>This is because the non-leaf buffers gets destroyed the first time <code>backward()</code> is called and hence, there’s no path to navigate to the leaves when <code>backward</code> is invoked the second time. You can undo this non-leaf buffer destroying behaviour by adding <code>retain_graph = True</code> argument to the <code>backward</code> function.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss.backward(retain_graph = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>If you do the above, you will be able to backpropagate again through the same graph and the gradients will be accumulated, i.e. the next you backpropagate, the gradients will be added to those already stored in the previous back pass.</p><hr><p>This is in contrast to the <em><strong>Static Computation Graphs</strong></em>, used by TensorFlow where the graph is declared <strong>before</strong> running the program. Then the graph is “run” by feeding values to the predefined graph.</p><p>The dynamic graph paradigm allows you to make changes to your network architecture <em>during</em> runtime, as a graph is created only when a piece of code is run.</p><p>This means a graph may be redefined during the lifetime for a program since you don’t have to define it beforehand.</p><p>This, however, is not possible with static graphs where graphs are created before running the program, and merely executed later.</p><p>Dynamic graphs also make debugging way easier since it’s easier to locate the source of your error.</p><h2 id="Some-Tricks-of-Trade"><a href="#Some-Tricks-of-Trade" class="headerlink" title="Some Tricks of Trade"></a>Some Tricks of Trade</h2><h3 id="requires-grad"><a href="#requires-grad" class="headerlink" title="requires_grad"></a>requires_grad</h3><p>This is an attribute of the <code>Tensor</code> class. By default, it’s False. It comes handy when you have to freeze some layers, and stop them from updating parameters while training. You can simply set the <code>requires_grad</code> to False, and these <code>Tensors</code> won’t participate in the computation graph.</p><div align=center><p><img src="https://jpccc.github.io/resource/pytorch/image-4.png" alt="img"></p></div><p>Thus, no gradient would be propagated to them, or to those layers which depend upon these layers for gradient flow <code>requires_grad</code>. When set to True, <code>requires_grad</code> is contagious meaning even if one operand of an operation has <code>requires_grad</code> set to True, so will the result.</p><h3 id="torch-no-grad"><a href="#torch-no-grad" class="headerlink" title="torch.no_grad()"></a>torch.no_grad()</h3><p>When we are computing gradients, we need to cache input values, and intermediate features as they maybe required to compute the gradient later.</p><p>The gradient of $ b=w_1∗a$ w.r.t it’s inputs w1w1 and aa is aa and w1w1 respectively. We need to store these values for gradient computation during the backward pass. This affects the memory footprint of the network.</p><p>While, we are performing inference, we don’t compute gradients, and thus, don’t need to store these values. Infact, no graph needs to be create during inference as it will lead to useless consumption of memory.</p><p>PyTorch offers a context manager, called <code>torch.no_grad</code> for this purpose.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad:</span><br><span class="line">inference code goes here </span><br></pre></td></tr></table></figure><p>No graph is defined for operations executed under this context manager.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Understanding how <em>Autograd</em> and computation graphs works can make life with PyTorch a whole lot easier. With our foundations rock solid, the next posts will detail how to create custom complex architectures, how to create custom data pipelines and more interesting stuff.</p><h2 id="Further-Reading"><a href="#Further-Reading" class="headerlink" title="Further Reading"></a>Further Reading</h2><ol><li><a href="https://www.khanacademy.org/math/differential-calculus/dc-chain">Chain Rule</a></li><li><a href="http://neuralnetworksanddeeplearning.com/chap2.html">Backpropagation</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;div align=center&gt;

&lt;p&gt;&lt;img src=&quot;https://jpccc.github.io/resource/pytorch/full_graph.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
&lt;/div&gt;

&lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;PyTorch is one of the foremost python deep learning libraries out there. It’s the go to choice for deep learning research, and as each days passes by, more and more companies and research labs are adopting this library.&lt;/p&gt;
&lt;p&gt;In this series of tutorials, we will be introducing you to PyTorch, and how to make the best use of the libraries as well the ecosystem of tools built around it. We’ll first cover the basic building blocks, and then move onto how you can quickly prototype custom architectures. We will finally conclude with a couple of posts on how to scale your code, and how to debug your code if things go awry.&lt;/p&gt;
&lt;p&gt;This is Part 1 of our PyTorch 101 series.&lt;/p&gt;</summary>
    
    
    
    
    <category term="pytorch" scheme="https://jpccc.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Jensen_inequality</title>
    <link href="https://jpccc.github.io/2021/10/28/Jensen-inequality/"/>
    <id>https://jpccc.github.io/2021/10/28/Jensen-inequality/</id>
    <published>2021-10-28T08:10:44.000Z</published>
    <updated>2022-04-05T16:03:21.412Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Jensen不等式"><a href="#Jensen不等式" class="headerlink" title="Jensen不等式"></a>Jensen不等式</h1><p>Jensen不等式（Jensen’s inequality）是以丹麦数学家Johan Jensen命名的，它在概率论、机器学习、测度论、统计物理等领域都有相关应用。 在机器学习领域，我目前接触到的是用Jensen不等式用来证明KL散度大于等于0。</p><blockquote><p>Jensen不等式是和凸函数的定义是息息相关的，首先介绍什么是凸函数(convec function)。</p></blockquote><span id="more"></span><h2 id="凸函数"><a href="#凸函数" class="headerlink" title="凸函数"></a>凸函数</h2><p>凸函数是一个定义在某个向量空间的凸子集 C（区间）上的实值函数 f，如果在其定义域 C 上的任意两点 $x_1$,$x_2$ 有:</p><p>$$tf(x_1)+(1-t)f(x_2)\geq f(tx_1+(1-t)x_2)   \tag{1}$$</p><p>也就是说凸函数任意两点的割线位于函数图形上方， 这也是Jensen不等式的两点形式。</p><h2 id="Jensen不等式2134123"><a href="#Jensen不等式2134123" class="headerlink" title="Jensen不等式2134123"></a>Jensen不等式2134123</h2><p>若对于任意点集${x_i}$，若 $\lambda_i\geq 0$ 且 $\underset {i}\sum\lambda_i=1$ ，使用数学归纳法，可以证明凸函数 f (x) 满足：<br>$$f(\sum_{i=1}^M\lambda_ix_i)\leq \sum_{i=1}^M\lambda_if(x_i) \tag{2} $$</p><p>公式(2)被称为 Jensen 不等式，它是公式(1)的泛化形式。</p><blockquote><p><strong>证明如下：</strong></p><blockquote><p>当i=1或2时，由凸函数的定义成立<br>    假设当i=M时，公式(2)成立<br>    现在证明则i=M+1时，Jensen不等式也成立：<br><a href="https://blog.csdn.net/AndrewHYang/article/details/86477162">证明</a></p></blockquote></blockquote><p>在概率论中，如果把$\lambda_i$看成取值为$x_i$的离散变量x的概率分布，那么公式(2)就可以写成：</p><p>$$f(E(X))\leq E[f(x)]$$</p><p>其中, E[·] 表示期望。</p><p>对于连续变量，Jensen不等式给出了积分的凸函数值和凸函数的积分值间的关系。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Jensen不等式&quot;&gt;&lt;a href=&quot;#Jensen不等式&quot; class=&quot;headerlink&quot; title=&quot;Jensen不等式&quot;&gt;&lt;/a&gt;Jensen不等式&lt;/h1&gt;&lt;p&gt;Jensen不等式（Jensen’s inequality）是以丹麦数学家Johan Jensen命名的，它在概率论、机器学习、测度论、统计物理等领域都有相关应用。 在机器学习领域，我目前接触到的是用Jensen不等式用来证明KL散度大于等于0。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Jensen不等式是和凸函数的定义是息息相关的，首先介绍什么是凸函数(convec function)。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    
    <category term="math" scheme="https://jpccc.github.io/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>统计学习数学基础-1</title>
    <link href="https://jpccc.github.io/2021/10/28/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0part1/"/>
    <id>https://jpccc.github.io/2021/10/28/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0part1/</id>
    <published>2021-10-28T04:44:13.000Z</published>
    <updated>2022-05-12T12:25:10.839Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>对概率的诠释有两大学派，一种是频率派另一种是贝叶斯派。后面我们对观测集采用下面记号：<br>$$<br>X_{N\times p}=(x_{1},x_{2},\cdots,x_{N})^{T},x_{i}=(x_{i1},x_{i2},\cdots,x_{ip})^{T}<br>$$<br>这个记号表示有 $N$ 个样本，每个样本都是 $p$ 维向量。其中每个观测都是由 $p(x|\theta)$ 生成的。</p><span id="more"></span><h2 id="频率派的观点"><a href="#频率派的观点" class="headerlink" title="频率派的观点"></a>频率派的观点</h2><p>$p(x|\theta)$中的 $\theta$ 是一个未知常量，数据是随机变量。对于 $N$ 个观测来说观测集的概率为</p><div align='center'><img src="https://www.zhihu.com/equation?tex=p(X|\theta)\mathop{=}\limits_{iid}\prod\limits_{i=1}^{N}{p(x_{i}|\theta)}" align="center"/></div><p>为了求 $\theta$ 的大小，我们采用最大对数似然MLE的方法：</p><img src="https://www.zhihu.com/equation?tex=\theta_{MLE}=\mathop{argmax}\limits_{\theta}\log p(X|\theta)\mathop{=}\limits_{iid}\mathop{argmax}\limits_{\theta}\sum\limits _{i=1}^{N}\log p(x_{i}|\theta)"/><p>$x_i$服从独立同分布的条件，所以$P(X|\theta)=\prod_{i=0}^n p(x_i|\theta)$,为了<b>方便计算</b>，在前面加上log,将连乘变成连加。</p><h2 id="贝叶斯派的观点"><a href="#贝叶斯派的观点" class="headerlink" title="贝叶斯派的观点"></a>贝叶斯派的观点</h2><p>贝叶斯派认为 $p(x|\theta)$ 中的 $\theta$ 不是一个常量。这个 $\theta$ 满足一个预设的<strong>先验</strong>的分布 $\theta\sim p(\theta)$ （比喻可以假设为高斯分布）,并借助贝叶斯定理，用似然将参数的先验和后验连接起来，于是根据贝叶斯定理依赖观测集参数的后验可以写成：</p><p>$$<br>p(\theta|X)=\frac{p(X|\theta)\cdot p(\theta)}{p(X)}=\frac{p(X|\theta)\cdot p(\theta)}{\int\limits _{\theta}p(X|\theta)\cdot p(\theta)d\theta}<br>$$<br>为了求 $\theta$ 的值，我们要最大化这个参数后验MAP：</p><blockquote><p>最大化后验的解释：在给定观测X的情况下，找出$\theta$的概率最大时所对应的值，即这时候的$\theta$更可能为我们要找的参数。<br>$$<br>\theta_{MAP}=\mathop{argmax}\limits _{\theta}p(\theta|X)=\mathop{argmax}\limits _{\theta}p(X|\theta)\cdot p(\theta)<br>$$<br>其中第二个等号是由于分母和 $\theta$ 没有关系。求解这个 $\theta$ 值后计算$\frac{p(X|\theta)\cdot p(\theta)}{\int\limits _{\theta}p(X|\theta)\cdot p(\theta)d\theta}$ ，就得到了参数的后验概率。其中 $p(X|\theta)$ 叫似然，是我们的模型分布。得到了参数的后验分布后，我们可以将这个分布用于<strong>贝叶斯预测</strong>：</p></blockquote><p>$$<br>p(x_{new}|X)=\int\limits <em>{\theta}p(x</em>{new},\theta|X)=\int\limits <em>{\theta}p(x</em>{new}|\theta)\cdot p(\theta|X)d\theta<br>$$</p><p> 其中积分中的被乘数是模型，乘数是后验分布。</p><blockquote><p>$p(x|\theta)$是似然，$p(\theta|x)$是后验。注意第一个等式中，$\theta$不论放分子还是分母都可以这样积分掉。</p></blockquote><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>频率派和贝叶斯派分别给出了一系列的机器学习算法。频率派的观点导出了一系列的统计机器学习算法而贝叶斯派导出了概率图理论。在应用频率派的 MLE 方法时最<strong>优化</strong>理论占有重要地位。而贝叶斯派的算法无论是后验概率的建模还是应用这个后验进行推断时<strong>积分</strong>占有重要地位。因此采样积分方法如 MCMC 有很多应用。(即频率派需要设计损失函数并进行优化，而贝叶斯派需要积分后验中分母。)</p><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><blockquote><p>频率派认为参数是客观存在不会改变的，虽然未知，但却是固定值（故可用最优化方法去找那一个唯一的值）；贝叶斯派则认为参数是随机值，因为不可能做完整的实验去确定，因此参数也可以有分布。往小处说，频率派最常关心的是似然函数，他们认为直接用样本去计算出的概率就是真实的，而贝叶斯派最常关心的是后验分布，他们认为样本只是用来修正经验观点。</p></blockquote><blockquote><p>贝叶斯派因为所有的参数都是随机变量，都有分布，因此可以使用一些基于采样的方法 （如MCMC）使得我们更容易构建复杂模型。频率派的优点则是没有假设一个先验分布，因此更加客观，也更加无偏，在一些保守的领域（比如制药业、法律）比贝叶斯方法更受到信任。</p></blockquote><h1 id="MathBasics"><a href="#MathBasics" class="headerlink" title="MathBasics"></a>MathBasics</h1><h2 id="高斯分布"><a href="#高斯分布" class="headerlink" title="高斯分布"></a>高斯分布</h2><h3 id="一维情况-MLE"><a href="#一维情况-MLE" class="headerlink" title="一维情况 MLE"></a>一维情况 MLE</h3><p>高斯分布在机器学习中占有举足轻重的作用。在 MLE 方法中：</p><p>$$<br>\theta=(\mu,\Sigma)=(\mu,\sigma^{2}),\theta_{MLE}=\mathop{argmax}\limits _{\theta}\log p(X|\theta)\mathop{=}\limits _{iid}\mathop{argmax}\limits _{\theta}\sum\limits <em>{i=1}^{N}\log p(x</em>{i}|\theta)<br>$$</p><p>一般地，高斯分布的概率密度函数PDF写为：</p><p>$$<br>p(x|\mu,\Sigma)=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)}<br>$$</p><p>带入 MLE 中我们考虑一维的情况</p><p>$$<br>\log p(X|\theta)=\sum\limits <em>{i=1}^{N}\log p(x</em>{i}|\theta)=\sum\limits <em>{i=1}^{N}\log\frac{1}{\sqrt{2\pi}\sigma}\exp(-(x</em>{i}-\mu)^{2}/2\sigma^{2})<br>$$</p><p>首先对 $\mu$ 的极值可以得到 ：<br>$$<br>\mu_{MLE}=\mathop{argmax}\limits _{\mu}\log p(X|\theta)=\mathop{argmax}\limits _{\mu}\sum\limits <em>{i=1}^{N}(x</em>{i}-\mu)^{2}<br>$$</p><p> 于是：<br>$$<br>\frac{\partial}{\partial\mu}\sum\limits <em>{i=1}^{N}(x</em>{i}-\mu)^{2}=0\longrightarrow\mu_{MLE}=\frac{1}{N}\sum\limits <em>{i=1}^{N}x</em>{i}<br>$$</p><p>其次对 $\theta$ 中的另一个参数 $\sigma$ ，有：</p><p>$$<br>\begin{align}<br>\sigma_{MLE}=\mathop{argmax}\limits _{\sigma}\log p(X|\theta)&amp;=\mathop{argmax}\limits _{\sigma}\sum\limits <em>{i=1}^{N}[-\log\sigma-\frac{1}{2\sigma^{2}}(x</em>{i}-\mu)^{2}]\<br>&amp;=\mathop{argmin}\limits _{\sigma}\sum\limits <em>{i=1}^{N}[\log\sigma+\frac{1}{2\sigma^{2}}(x</em>{i}-\mu)^{2}]<br>\end{align}<br>$$</p><p>于是：</p><p>$$<br>\frac{\partial}{\partial\sigma}\sum\limits <em>{i=1}^{N}[\log\sigma+\frac{1}{2\sigma^{2}}(x</em>{i}-\mu)^{2}]=0\longrightarrow\sigma_{MLE}^{2}=\frac{1}{N}\sum\limits <em>{i=1}^{N}(x</em>{i}-\mu)^{2}<br>$$</p><p>值得注意的是，上面的推导中，首先对 $\mu$ 求 MLE， 然后利用这个结果求 $\sigma_{MLE}$ ，因此可以预期的是对数据集求期望时 $\mathbb{E}<em>{\mathcal{D}}[\mu</em>{MLE}]$ 是无偏差的：</p><p>$$<br>\mathbb{E}<em>{\mathcal{D}}[\mu</em>{MLE}]=\mathbb{E}<em>{\mathcal{D}}[\frac{1}{N}\sum\limits <em>{i=1}^{N}x</em>{i}]=\frac{1}{N}\sum\limits <em>{i=1}^{N}\mathbb{E}</em>{\mathcal{D}}[x</em>{i}]=\mu<br>$$</p><p>但是当对 $\sigma_{MLE}$ 求 期望的时候由于使用了单个数据集的 $\mu_{MLE}$，因此对所有数据集求期望的时候我们会发现 $\sigma_{MLE}$ 是 有偏的：</p><p>$$<br>\begin{align}<br>\mathbb{E}<em>{\mathcal{D}}[\sigma</em>{MLE}^{2}]&amp;=\mathbb{E}<em>{\mathcal{D}}[\frac{1}{N}\sum\limits <em>{i=1}^{N}(x</em>{i}-\mu</em>{MLE})^{2}]=\mathbb{E}<em>{\mathcal{D}}[\frac{1}{N}\sum\limits <em>{i=1}^{N}(x</em>{i}^{2}-2x</em>{i}\mu_{MLE}+\mu_{MLE}^{2})<br>\&amp;=\mathbb{E}<em>{\mathcal{D}}[\frac{1}{N}\sum\limits <em>{i=1}^{N}x</em>{i}^{2}-\mu</em>{MLE}^{2}]=\mathbb{E}<em>{\mathcal{D}}[\frac{1}{N}\sum\limits <em>{i=1}^{N}x</em>{i}^{2}-\mu^{2}+\mu^{2}-\mu</em>{MLE}^{2}]\<br>&amp;= \mathbb{E}<em>{\mathcal{D}}[\frac{1}{N}\sum\limits <em>{i=1}^{N}x</em>{i}^{2}-\mu^{2}]-\mathbb{E}</em>{\mathcal{D}}[\mu_{MLE}^{2}-\mu^{2}]=\sigma^{2}-(\mathbb{E}<em>{\mathcal{D}}[\mu</em>{MLE}^{2}]-\mu^{2})\&amp;=\sigma^{2}-(\mathbb{E}<em>{\mathcal{D}}[\mu</em>{MLE}^{2}]-\mathbb{E}<em>{\mathcal{D}}^{2}[\mu</em>{MLE}])=\sigma^{2}-Var[\mu_{MLE}]\&amp;=\sigma^{2}-Var[\frac{1}{N}\sum\limits <em>{i=1}^{N}x</em>{i}]=\sigma^{2}-\frac{1}{N^{2}}\sum\limits <em>{i=1}^{N}Var[x</em>{i}]=\frac{N-1}{N}\sigma^{2}<br>\end{align}<br>$$</p><p>所以：${\sigma}^{2}$的无偏估计应该为：</p><p>$$<br>\hat{\sigma}^{2}=\frac{1}{N-1}\sum\limits <em>{i=1}^{N}(x</em>{i}-\mu)^{2}<br>$$</p><h3 id="多维情况"><a href="#多维情况" class="headerlink" title="多维情况"></a>多维情况</h3><p>多维高斯分布表达式为：</p><p>$$<br>p(x|\mu,\Sigma)=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}e^{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)}<br>$$</p><p>其中 $x,\mu\in\mathbb{R}^{p},\Sigma\in\mathbb{R}^{p\times p}$ ，$\Sigma$ 为协方差矩阵，一般而言也是半正定矩阵。这里我们只考虑正定矩阵。首先我们处理指数上的数字，指数上的数字可以记为 $x$ 和 $\mu$ 之间的马氏距离。对于对称的协方差矩阵可进行特征值分解，$\Sigma=U\Lambda U^{T}=(u_{1},u_{2},\cdots,u_{p})diag(\lambda_{i})(u_{1},u_{2},\cdots,u_{p})^{T}=\sum\limits <em>{i=1}^{p}u</em>{i}\lambda_{i}u_{i}^{T}$ ，于是：</p><p>$$<br>\Sigma^{-1}=\sum\limits <em>{i=1}^{p}u</em>{i}\frac{1}{\lambda_{i}}u_{i}^{T}<br>$$</p><p>$$<br>\Delta=(x-\mu)^{T}\Sigma^{-1}(x-\mu)=\sum\limits <em>{i=1}^{p}(x-\mu)^{T}u</em>{i}\frac{1}{\lambda_{i}}u_{i}^{T}(x-\mu)=\sum\limits <em>{i=1}^{p}\frac{y</em>{i}^{2}}{\lambda_{i}}<br>$$</p><p>我们注意到 $y_{i}$ 是 $x-\mu$ 在特征向量 $u_{i}$ 上的投影长度，因此上式子就是 $\Delta$ 取不同值时的同心椭圆。</p><p>下面我们看多维高斯模型在实际应用时的两个问题</p><ol><li><p> 参数 $\Sigma,\mu$ 的自由度为 $O(p^{2})$ 对于维度很高的数据其自由度太高。解决方案：高自由度的来源是 $\Sigma$ 有 $\frac{p(p+1)}{2}$ 个自由参数，可以假设其是对角矩阵，甚至在各向同性假设中假设其对角线上的元素都相同。前一种的算法有 Factor Analysis，后一种有概率 PCA(p-PCA) 。</p></li><li><p> 第二个问题是单个高斯分布是单峰的，对有多个峰的数据分布不能得到好的结果。解决方案：高斯混合GMM 模型。</p></li></ol><p>下面对多维高斯分布的常用定理进行介绍。</p><p>我们记 $x=(x_1, x_2,\cdots,x_p)^T=(x_{a,m\times 1}, x_{b,n\times1})^T,\mu=(\mu_{a,m\times1}, \mu_{b,n\times1}),\Sigma=\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}$，已知 $x\sim\mathcal{N}(\mu,\Sigma)$。</p><p>首先是一个高斯分布的定理：</p><blockquote><p>  定理：已知 $x\sim\mathcal{N}(\mu,\Sigma), y\sim Ax+b$，那么 $y\sim\mathcal{N}(A\mu+b, A\Sigma A^T)$。</p><p>  证明：$\mathbb{E}[y]=\mathbb{E}[Ax+b]=A\mathbb{E}[x]+b=A\mu+b$，$Var[y]=Var[Ax+b]=Var[Ax]=A\cdot Var[x]\cdot A^T$。</p></blockquote><p>下面利用这个定理得到 $p(x_a),p(x_b),p(x_a|x_b),p(x_b|x_a)$ 这四个量。</p><ol><li><p>$x_a=\begin{pmatrix}\mathbb{I}<em>{m\times m}&amp;\mathbb{O}</em>{m\times n})\end{pmatrix}\begin{pmatrix}x_a\x_b\end{pmatrix}$，代入定理中得到：</p><p> $$<br> \mathbb{E}[x_a]=\begin{pmatrix}\mathbb{I}&amp;\mathbb{O}\end{pmatrix}\begin{pmatrix}\mu_a\\mu_b\end{pmatrix}=\mu_a\<br> Var[x_a]=\begin{pmatrix}\mathbb{I}&amp;\mathbb{O}\end{pmatrix}\begin{pmatrix}\Sigma_{aa}&amp;\Sigma_{ab}\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\begin{pmatrix}\mathbb{I}\\mathbb{O}\end{pmatrix}=\Sigma_{aa}<br> $$</p><p> 所以 $x_a\sim\mathcal{N}(\mu_a,\Sigma_{aa})$。</p></li><li><p> 同样的，$x_b\sim\mathcal{N}(\mu_b,\Sigma_{bb})$。</p></li><li><p>对于两个条件概率，我们引入三个量：</p><p> $$<br> x_{b\cdot a}=x_b-\Sigma_{ba}\Sigma_{aa}^{-1}x_a\<br> \mu_{b\cdot a}=\mu_b-\Sigma_{ba}\Sigma_{aa}^{-1}\mu_a\<br> \Sigma_{bb\cdot a}=\Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab}<br> $$</p><p> 特别的，最后一个式子叫做 $\Sigma_{bb}$ 的 Schur Complementary。可以看到：</p><p> $$<br> x_{b\cdot a}=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbb{I}_{n\times n}\end{pmatrix}\begin{pmatrix}x_a\x_b\end{pmatrix}<br> $$</p><p> 所以：</p><p> $$<br> \mathbb{E}[x_{b\cdot a}]=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbb{I}<em>{n\times n}\end{pmatrix}\begin{pmatrix}\mu_a\\mu_b\end{pmatrix}=\mu</em>{b\cdot a}\<br> Var[x_{b\cdot a}]=\begin{pmatrix}-\Sigma_{ba}\Sigma_{aa}^{-1}&amp;\mathbb{I}<em>{n\times n}\end{pmatrix}\begin{pmatrix}\Sigma</em>{aa}&amp;\Sigma_{ab}\\Sigma_{ba}&amp;\Sigma_{bb}\end{pmatrix}\begin{pmatrix}-\Sigma_{aa}^{-1}\Sigma_{ba}^T\\mathbb{I}<em>{n\times n}\end{pmatrix}=\Sigma</em>{bb\cdot a}<br> $$</p><p> 利用这三个量可以得到 $x_b=x_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a$。因此：</p><p> $$<br> \mathbb{E}[x_b|x_a]=\mu_{b\cdot a}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a<br> $$</p><p> $$<br> Var[x_b|x_a]=\Sigma_{bb\cdot a}<br> $$</p><p> 这里同样用到了定理。</p></li><li><p>同样：</p><p> $$<br> x_{a\cdot b}=x_a-\Sigma_{ab}\Sigma_{bb}^{-1}x_b\<br> \mu_{a\cdot b}=\mu_a-\Sigma_{ab}\Sigma_{bb}^{-1}\mu_b\<br> \Sigma_{aa\cdot b}=\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba}<br> $$</p><p> 所以：</p><p> $$<br> \mathbb{E}[x_a|x_b]=\mu_{a\cdot b}+\Sigma_{ab}\Sigma_{bb}^{-1}x_b<br> $$</p><p> $$<br> Var[x_a|x_b]=\Sigma_{aa\cdot b}<br> $$</p></li></ol><p>下面利用上边四个量，求解线性模型：</p><blockquote><p>  已知：$p(x)=\mathcal{N}(\mu,\Lambda^{-1}),p(y|x)=\mathcal{N}(Ax+b,L^{-1})$，求解：$p(y),p(x|y)$。</p><p>  解：令 $y=Ax+b+\epsilon,\epsilon\sim\mathcal{N}(0,L^{-1})$，所以 $\mathbb{E}[y]=\mathbb{E}[Ax+b+\epsilon]=A\mu+b$，$Var[y]=A \Lambda^{-1}A^T+L^{-1}$，因此：<br>  $$<br>  p(y)=\mathcal{N}(A\mu+b,L^{-1}+A\Lambda^{-1}A^T)<br>  $$<br>  引入 $z=\begin{pmatrix}x\y\end{pmatrix}$，我们可以得到 $Cov[x,y]=\mathbb{E}[(x-\mathbb{E}[x])(y-\mathbb{E}[y])^T]$。对于这个协方差可以直接计算：<br>  $$<br>  \begin{align}<br>  Cov(x,y)&amp;=\mathbb{E}[(x-\mu)(Ax-A\mu+\epsilon)^T]=\mathbb{E}[(x-\mu)(x-\mu)^TA^T]=Var[x]A^T=\Lambda^{-1}A^T<br>  \end{align}<br>  $$<br>  注意到协方差矩阵的对称性，所以 $p(z)=\mathcal{N}\begin{pmatrix}\mu\A\mu+b\end{pmatrix},\begin{pmatrix}\Lambda^{-1}&amp;\Lambda^{-1}A^T\A\Lambda^{-1}&amp;L^{-1}+A\Lambda^{-1}A^T\end{pmatrix})$。根据之前的公式，我们可以得到：<br>  $$<br>  \mathbb{E}[x|y]=\mu+\Lambda^{-1}A^T(L^{-1}+A\Lambda^{-1}A^T)^{-1}(y-A\mu-b)<br>  $$</p><p>  $$<br>  Var[x|y]=\Lambda^{-1}-\Lambda^{-1}A^T(L^{-1}+A\Lambda^{-1}A^T)^{-1}A\Lambda^{-1}<br>  $$</p></blockquote>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;p&gt;对概率的诠释有两大学派，一种是频率派另一种是贝叶斯派。后面我们对观测集采用下面记号：&lt;br&gt;$$&lt;br&gt;X_{N\times p}=(x_{1},x_{2},\cdots,x_{N})^{T},x_{i}=(x_{i1},x_{i2},\cdots,x_{ip})^{T}&lt;br&gt;$$&lt;br&gt;这个记号表示有 $N$ 个样本，每个样本都是 $p$ 维向量。其中每个观测都是由 $p(x|\theta)$ 生成的。&lt;/p&gt;</summary>
    
    
    
    
    <category term="统计学习" scheme="https://jpccc.github.io/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>BN算法</title>
    <link href="https://jpccc.github.io/2021/10/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/BN%E7%AE%97%E6%B3%95/"/>
    <id>https://jpccc.github.io/2021/10/25/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/BN%E7%AE%97%E6%B3%95/</id>
    <published>2021-10-25T12:20:14.000Z</published>
    <updated>2022-05-12T12:25:58.882Z</updated>
    
    <content type="html"><![CDATA[<h1 id="BN算法"><a href="#BN算法" class="headerlink" title="BN算法"></a>BN算法</h1><h2 id="BN算法概要"><a href="#BN算法概要" class="headerlink" title="BN算法概要"></a>BN算法概要</h2><p>传统的神经网络，只是在将样本x输入到输入层之前对x进行标准化处理，以降低样本间的差异性。BN是在此基础上，不仅仅只对输入层的输入数据x进行标准化，还对每个隐藏层的输入进行标准化。</p><p>Batch Normalization是2015年一篇论文中提出的数据归一化方法，<strong>往往用在深度神经网络中激活层之前</strong>。其作用可以加快模型训练时的收敛速度，使得模型训练过程更加稳定，避免梯度爆炸或者梯度消失。（怎么避免梯度爆炸或梯度消失的？）</p><p>那为什么需要对每个隐藏层的输入进行标准化呢？或者说这样做有什么好处呢？这就牵涉到一个Covariate Shift问题。</p><p>并且起到一定的正则化作用，几乎代替了Dropout。</p><span id="more"></span><h2 id="Covariate-Shift问题"><a href="#Covariate-Shift问题" class="headerlink" title="Covariate Shift问题"></a>Covariate Shift问题</h2><p>Convariate shift是BN论文作者提出来的概念，指的是具有不同分布的输入值对深度网络学习的影响。当神经网络的输入值的分布不同时，我们可以理解为输入特征值的scale差异较大，与权重进行矩阵相乘后，会产生一些偏离较大的差异值；而深度学习网络需要通过训练不断更新完善，那么差异值产生的些许变化都会深深影响后层，偏离越大表现越为明显；因此，对于反向传播来说，这些现象都会导致梯度发散，从而需要更多的训练步骤来抵消scale不同带来的影响，也就是说，<strong>这种分布不一致将减缓训练速度。</strong></p><p>而BN的作用就是将这些输入值进行标准化，降低scale的差异至同一个范围内。这样做的好处在于一方面提高梯度的收敛程度，加快模型的训练速度；另一方面使得每一层可以尽量面对同一特征分布的输入值，减少了变化带来的不确定性，也降低了对后层网络的影响，各层网络变得相对独立，缓解了训练中的梯度消失问题。</p><h2 id="BN算法产生的背景"><a href="#BN算法产生的背景" class="headerlink" title="BN算法产生的背景"></a>BN算法产生的背景</h2><p>做深度学习大家应该都知道，我们在数据处理部分，我们为了加速训练首先会对数据进行处理的。其中我们最常用的是零均值和PCA（白话）。首先我们进行简单介绍零均值带来的效果：</p><p><img src="E:\笔记\markdown\picture\bn01.png" alt="img"></p><p>首先对第一张图进行分析。</p><p>由于我们通常使用采用零均值化对网络进行参数初始化，我们初始的拟合直线也就是红色部分。另外的一条绿色直线，是我们的目标直线。从图能够直观看出，我们应该需要多次迭代才能得到我们的需要的目标直线。</p><p><img src="E:\笔记\markdown\picture\bn02.png" alt="img"></p><p>我们再看第二张图</p><p>假设我们还是和第一张图有相同的分布，只是我们做了减均值，让数据均值为零。能够直观的发现可能只进行简单的微调就能够实现拟合（理想）。大大提高了我们的训练速度。因此，在训练开始前，对数据进行零均值是一个必要的操作。</p><p>但是，随着网络层次加深参数对分布的影响不定，导致网络每层间以及不同迭代轮次的相同层的输入分布发生改变，导致网络需要重新适应新的分布，迫使我们降低学习率降低影响。在这个背景下BN算法开始出现。       有些人首先提出在每层增加PCA白化(先对数据进行去相关然后再进行归一化)，这样基本满足了数据的0均值、单位方差、弱相关性。但是这样是不可取的，因为在白化过程中会计算协方差矩阵、求逆等操作，计算量会很大，另外，在反向传播时，白化的操作不一定可微。因此，在此背景下BN算法开始出现。</p><h2 id="BN算法的实现和优点"><a href="#BN算法的实现和优点" class="headerlink" title="BN算法的实现和优点"></a>BN算法的实现和优点</h2><h3 id="BN算法的实现"><a href="#BN算法的实现" class="headerlink" title="BN算法的实现"></a>BN算法的实现</h3><p>上面提到了PCA白化优点，能够去相关和数据均值，标准值归一化等优点。但是当数据量比较大的情况下去相关的话需要大量的计算，因此有些人提出了只对数据进行均值和标准差归一化。叫做近似白化预处理。</p><p>$$\hat{x}^k=\frac{X^k-E(X^k)}{\sqrt{Var[(x^k)}]}$$</p><p>由于训练过程采用了batch随机梯度下降，因此$E(X^k)$指的是一批训练数据时，各神经元输入值的平均值；$\sqrt{Var[(x^k)}]$指的是一批训练数据时各神经元输入值的标准差。</p><p><strong>但是，这些应用到深度学习网络还远远不够，因为可能由于这种的强制转化导致数据的分布发生破坏</strong>。因此需要对公式的鲁棒性进行优化，就有人提出了<strong>变换重构</strong>的概念。就是在基础公式的基础之上加上了两个参数γ、β。<strong>这样在训练过程中就可以学习这两个参数，采用适合自己网络的BN公式。</strong></p><p>公式如下：<br>    $$y^k=\gamma^k\hat x^k+\beta^k$$<br>    每一个神经元都会有一对这样的参数γ、β。这样其实当$\beta^k=E[x^k],\gamma^k=\sqrt{var[x^k]}$时，是可以恢复出原始的某一层所学到的特征的。引入可学习重构参数γ、β，让网络可以学习恢复出原始网络所要学习的特征分布。</p><p>总结上面我们会得到BN的向前传导公式：</p><p>$\mu_\beta\leftarrow\frac{1}{m}\sum_{i=1}^nx_i$<br>        $\delta_\beta^2\leftarrow\frac{1}{m}\sum_{i=1}^n(x_i-\mu_\beta)^2$<br>        $\hat{x}\leftarrow\frac{x_i-\mu_\beta}{\sqrt{\delta_\beta^2+\epsilon}}$<br>        $y_i\leftarrow\gamma\hat{x_i}+\beta\equiv BN_{\gamma,\beta}(x_i)$ </p><h3 id="BN算法在网络中的作用"><a href="#BN算法在网络中的作用" class="headerlink" title="BN算法在网络中的作用"></a>BN算法在网络中的作用</h3><p>BN算法像卷积层，池化层、激活层一样也作为一层。BN层添加在激活函数前，对输入激活函数的输入进行归一化。这样解决了输入数据发生偏移和增大的影响。</p><p>优点：</p><ol><li>可以增加训练速度，防止过拟合：如果没有归一化，每一层训练后的数据分布都不同，网络需要更大的开销去学习新的分布，造成网络模型更加复杂，因此容易发生过拟合，网络收敛也比较慢。即使小的学习率也能够有快速的学习速率;</li><li>可以避免激活函数进入非线性饱和区，从而造成梯度弥散问题。不用理会拟合中的droupout、L2 正则化项的参数选择，采用BN算法可以省去这两项或者只需要小的L2正则化约束。原因，BN算法后，参数进行了归一化，原本经过激活函数没有太大影响的神经元，分布变得明显，经过一个激活函数以后，神经元会自动削弱或者去除一些神经元，就不用再对其进行dropout。另外就是L2正则化，由于每次训练都进行了归一化，就很少发生由于数据分布不同导致的参数变动过大，带来的参数不断增大。</li><li>可以把训练数据集打乱，防止训练发生偏移。</li></ol><p>使用： 在卷积中，会出现每层卷积层中有（L）多个特征图。AxAxL特征矩阵。我们只需要以每个特征图为单元求取一对γ、β。</p><p>在测试时，由于不再有mini-batch的概念，无法计算均值和方差，因此用训练时所有mini-batch的均值和方差求期望，将其作为测试时的均值和方差。训练（train）过程和测试（inference）过程合并，算法总流程如下：</p><p><img src="E:\笔记\markdown\picture\bn03.png" alt="img"></p><h1 id="归一化和标准化"><a href="#归一化和标准化" class="headerlink" title="归一化和标准化"></a>归一化和标准化</h1><h2 id="问题引入"><a href="#问题引入" class="headerlink" title="问题引入"></a><strong>问题引入</strong></h2><p>在我们做机器学习相关的研究时，尤其在进行数据处理的过程中，大大小小都会遇到标准化和归一化的相关概念和处理。</p><h2 id="问题解答"><a href="#问题解答" class="headerlink" title="问题解答"></a><strong>问题解答</strong></h2><p>首先看一下标准化和归一化的公式：</p><p>归一化 ：$x^{\prime}=\frac{x-\min(x)}{\max(x)-\min(x)}$</p><p>标准化 ：$x^{\prime}=\frac{x-\overline{x}}{\delta}$</p><h2 id="归一化（Normalization）"><a href="#归一化（Normalization）" class="headerlink" title="归一化（Normalization）"></a>归一化（Normalization）</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>将一列数据变化到某个固定区间(范围)中，通常，这个区间是[0, 1]，广义的讲，可以是各种区间，比如映射到[0，1]一样可以继续映射到其他范围，图像中可能会映射到[0,255]，其他情况可能映射到[-1,1]；归一化不止上面公式给出的一种形式，还有范数（比如torch.nn.normalize()就是一种归一化）</p><h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><p>归一化后的数据有助于在求解是缓解求解过程中的参数寻优的动荡，以加快收敛。对于不归一化的收敛，可以发现其参数更新、收敛如左图，归一化后的收敛如右图。可以看到在左边是呈现出之字形的寻优路线，在右边则是呈现较快的梯度下降。</p><p><img src="E:\笔记\markdown\picture\640.jpeg" alt="图片"></p><h2 id="标准化（Standardization）"><a href="#标准化（Standardization）" class="headerlink" title="标准化（Standardization）"></a>标准化（Standardization）</h2><h3 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h3><p>将数据变换为均值为0，标准差为1的分布切记，并非一定是正态的。</p><h2 id="归一化和标准化的区别"><a href="#归一化和标准化的区别" class="headerlink" title="归一化和标准化的区别"></a>归一化和标准化的区别</h2><p>归一化是将样本的特征值转换到同一量纲下把数据映射到[0,1]或者[-1, 1]区间内，仅由变量的极值决定，因区间放缩法是归一化的一种。标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，转换为标准正态分布，和整体样本分布相关，每个样本点都能对标准化产生影响。它们的相同点在于都能取消由于量纲不同引起的误差；都是一种线性变换，都是对向量X按照比例压缩再进行平移。</p><p>很多博客甚至书中说，Standardization是改变数据分布，将其变换为服从N(0,1)的标准正态分布，这点是错的，Standardization会改变数据的均值、标准差(当然，严格的说，均值和标准差变了，分布也是变了，但分布种类依然没变，原来是啥类型，现在就是啥类型)，但本质上的分布并不一定是标准正态，完全取决于原始数据是什么分布。我个举例子，我生成了100万个服从beta(0.5,0.5)的样本点(你可以替换成任意非正态分布，比如卡方等等，beta(1,1)是一个服从U(0,1)的均匀分布，所以我选了beta(0.5,0.5))，称这个原始数据为$b_0$分布如下图所示：</p><p><img src="E:\笔记\markdown\picture\bn004.png" alt="img"></p><p>通过计算机计算，样本$b_0$的均值和方差分别为0.49982和0.12497(约为0.5和0.125)<br>对这个数据做Standardization，称这个标准化后的数据为$b_1$,分布如下：</p><p><img src="E:\笔记\markdown\picture\bn005.png" alt="图片"></p><p>可以看到数据形态完全不是正态分布，但是数学期望和方差已经变了。beta分布的数学期望为$\frac{a}{a+b}$，方差为$\frac{ab}{(a+b)^2(a+b+1)} $<br> ，所以$E(b_0) =\frac{0.5}{0.5+0.5}=\frac{1}{2}$, $Var(b_0)=\frac{1}{8}$,这也和我们上文所计算的样本均值和方差一致，而$b_1$ 的均值和方差分别为：-1.184190523417783e-1和1，均值和方差已经不再是0.5和0.125，分布改变，但绝不是一个正态分布。之所以大家会把标准化和正态分布联系起来，是因为实际数据中大部分都是正态分布，起码近似正态，另外，我看到很多人说标准化的基本假设是对正态数据，我并没有从哪些知名度较高的课本教材中查询到依据，如果有知道的同学也可以给我普及。</p><h3 id="什么时候Standardization，什么时候Normalization"><a href="#什么时候Standardization，什么时候Normalization" class="headerlink" title="什么时候Standardization，什么时候Normalization"></a>什么时候Standardization，什么时候Normalization</h3><p>如果你对处理后的数据范围有严格要求，那肯定是归一化，个人经验，标准化是ML中更通用的手段，如果你无从下手，可以直接使用标准化；如果数据不为稳定，存在极端的最大最小值，不要用归一化。在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候，标准化表现更好；在不涉及距离度量、协方差计算的时候，可以使用归一化方法。<br>PS：PCA中标准化表现更好的原因可以参考(<a href="https://blog.csdn.net/young951023/article/details/78389445">PCA标准化</a>)</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;BN算法&quot;&gt;&lt;a href=&quot;#BN算法&quot; class=&quot;headerlink&quot; title=&quot;BN算法&quot;&gt;&lt;/a&gt;BN算法&lt;/h1&gt;&lt;h2 id=&quot;BN算法概要&quot;&gt;&lt;a href=&quot;#BN算法概要&quot; class=&quot;headerlink&quot; title=&quot;BN算法概要&quot;&gt;&lt;/a&gt;BN算法概要&lt;/h2&gt;&lt;p&gt;传统的神经网络，只是在将样本x输入到输入层之前对x进行标准化处理，以降低样本间的差异性。BN是在此基础上，不仅仅只对输入层的输入数据x进行标准化，还对每个隐藏层的输入进行标准化。&lt;/p&gt;
&lt;p&gt;Batch Normalization是2015年一篇论文中提出的数据归一化方法，&lt;strong&gt;往往用在深度神经网络中激活层之前&lt;/strong&gt;。其作用可以加快模型训练时的收敛速度，使得模型训练过程更加稳定，避免梯度爆炸或者梯度消失。（怎么避免梯度爆炸或梯度消失的？）&lt;/p&gt;
&lt;p&gt;那为什么需要对每个隐藏层的输入进行标准化呢？或者说这样做有什么好处呢？这就牵涉到一个Covariate Shift问题。&lt;/p&gt;
&lt;p&gt;并且起到一定的正则化作用，几乎代替了Dropout。&lt;/p&gt;</summary>
    
    
    
    
    <category term="deepLearning" scheme="https://jpccc.github.io/tags/deepLearning/"/>
    
  </entry>
  
  <entry>
    <title>互信息</title>
    <link href="https://jpccc.github.io/2021/10/25/%E4%BA%92%E4%BF%A1%E6%81%AF/"/>
    <id>https://jpccc.github.io/2021/10/25/%E4%BA%92%E4%BF%A1%E6%81%AF/</id>
    <published>2021-10-25T04:44:13.000Z</published>
    <updated>2022-05-10T15:46:47.021Z</updated>
    
    <content type="html"><![CDATA[<h1 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h1><p>机器学习中很多地方都要根据目前的信息做出决策，信息熵主要是反应信息的不确定性；它的一个很重要的作用，就是做决策时提供一定的判断依据，比如决策树根据熵来往下设置分枝(branch)。</p><ol><li>计算方法 $$H(X)=-\sum_{i=1}^nP(x_i)logP(x_i)$$<br> 其中$P(x_i)$代表随机事件X为$x_i$的概率。</li></ol><span id="more"></span><ol start="2"><li>信息量</li></ol><p>信息量是对信息的度量，就跟时间的度量是秒一样；我们考虑一个离散的随机变量x，当我们观察到的这个变量的一个具体值的时候，我们接收到了多少信息呢？</p><p>多少信息用信息量来衡量，我们接受到的信息量信息的大小跟随机事件的概率分布有关，越小概率的事情发生了产生的信息量越大，如湖南产生的地震了；越大概率的事情发生了产生的信息量越小，如太阳从东边升起来了（肯定发生嘛，没什么信息量）。这很好理解！ <strong>所以描述信息量的函数应该是一个与随机变量的发生概率成负相关的函数，且不能为负数。</strong></p><p>例子:</p><blockquote><p>如果我们有俩个不相关的事件x和y，那么我们观察到的俩个事件同时发生时获得的信息应该等于观察到的事件各自发生时获得的信息之和，即：h(x,y) = h(x) + h(y)。 由于x，y是俩个不相关的事件，那么满足p(x,y) = p(x)*p(y)。<br>根据上面推导，我们很容易看出h(x)一定与p(x)的对数有关（因为只有对数形式的真数相乘之后，能够对应对数的相加形式，可以试试）。因此我们有信息量公式如下：<br>$$h(x)=-log_2p(x)$$<br>下面解决两个疑问:<br>(1). 为什么有一个负号?<br>信息量取概率的负对数，其实是因为信息量的定义是概率的倒数的对数。而用概率的倒数，是为了使概率越大，信息量越小，同时因为概率的倒数大于1，其对数自然大于0了。<br>(2). 为什么底数为2?<br>这是因为，我们只需要信息量满足低概率事件x对应于高的信息量，那么对数的选择是任意的，我们只是遵循信息论的普遍传统，使用2作为对数的底！</p></blockquote><ol start="3"><li>信息熵</li></ol><p>信息量度量的是一个具体事件发生了所带来的信息，而熵则是在结果出来之前对可能产生的信息量的期望——考虑该随机变量的所有可能取值，<strong>即所有可能发生事件所带来的信息量的期望</strong>。即<br>          $$H(X)=-\sum_{i=1}^np(x_i)logp(x_i)$$<br>信息熵还可以作为一个系统复杂程度的度量，如果系统越复杂，出现不同情况的种类越多，那么他的信息熵是比较大的。如果一个系统越简单，出现情况种类很少（极端情况为1种情况，那么对应概率为1，那么对应的信息熵为0），此时的信息熵较小。</p><h1 id="互信息"><a href="#互信息" class="headerlink" title="互信息"></a>互信息</h1><ol><li>定义<br> 互信息(Mutual Information)是衡量随机变量之间相互依赖程度的度量。</li><li>它的形象化解释是，假如明天下雨是个随机事件，假如今晚有晚霞同样是个随机事件，那么这两个随机事件互相依赖的程度是：</li></ol><p>​        当<b>不知道</b>”今晚有晚霞“情况下，”明天下雨“带来的不确定性<b>与我们已知</b>“今晚有晚霞“情况下，”明天下雨”带来的不确定性之差。</p><ol start="3"><li>解释<br> 假设存在一个随机变量$X$ ，和另外一个随机变量$Y$ ，那么它们的互信息是：<br> $$I(X;Y)=H(X)-H(X|Y)$$</li></ol><p>$H(X)$是$X$的信息熵,$H(X|Y)$是已知$Y$情况下，X带来的信息熵（条件熵）。</p><blockquote><p>直观理解是，我们知道存在两个随机事件X,Y，其中一个随机事件X 给我们带来了一些不确定性H(X)，我们想衡量Y,X 之间的关系。那么，如果X,Y 存在关联，当Y已知时，X给我们的不确定性会变化，这个变化值就是X的信息熵减去当已知 Y时，X的条件熵，就是互信息。</p></blockquote><p> 从概率角度，互信息是由随机变量 $X,Y$ 的联合概率分布 p(x,y) 和边缘概率分布 $p(x),p(y)$ 得出。</p><p> $$I(X;Y)=\sum_{y \in \cal Y}\sum_{x \in \cal X}p(x,y)log(\frac{p(x,y)}{p(x)p(y)})$$<br>  互信息和信息熵的关系是：</p> <p align="center">             <img src="https://jpccc.github.io/resource/Mutual_%20Information/001.jpg"> </p> 通常我们使用的最大化互信息条件，就是最大化两个随机事件的相关性。在数据集里，就是最大化两个数据集所拟合出的概率分布的相关性。当两个随机变量相同时,互信息最大，如下:$$I(X;Y)=H(X)-H(X|X)=H(X)$$<blockquote><p>在机器学习中，理想情况下，当互信息最大，可以认为从数据集中拟合出来的随机变量的概率分布与真实分布相同。</p></blockquote><p>到这里，应该足够大家日常理解使用了，以下是性质，应用和变形，几乎都是数学。</p><ol start="2"><li>The most common lower bound is InfoNCE [35] whose formula is given by: <p align="center">             <img src="https://jpccc.github.io/resource/Mutual_%20Information/002.PNG"> </p></li></ol>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;信息熵&quot;&gt;&lt;a href=&quot;#信息熵&quot; class=&quot;headerlink&quot; title=&quot;信息熵&quot;&gt;&lt;/a&gt;信息熵&lt;/h1&gt;&lt;p&gt;机器学习中很多地方都要根据目前的信息做出决策，信息熵主要是反应信息的不确定性；它的一个很重要的作用，就是做决策时提供一定的判断依据，比如决策树根据熵来往下设置分枝(branch)。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;计算方法 $$H(X)=-\sum_{i=1}^nP(x_i)logP(x_i)$$&lt;br&gt; 其中$P(x_i)$代表随机事件X为$x_i$的概率。&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    
    <category term="math" scheme="https://jpccc.github.io/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>algebra</title>
    <link href="https://jpccc.github.io/2021/10/22/algebra/"/>
    <id>https://jpccc.github.io/2021/10/22/algebra/</id>
    <published>2021-10-22T08:26:34.000Z</published>
    <updated>2021-11-18T05:23:04.062Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一-向量"><a href="#一-向量" class="headerlink" title="一. 向量"></a>一. 向量</h1><ol><li>向量对于不同的学科有不一样的定义，我们将从三个角度对向量进行解释。<ul><li>物理学<br> 物理中的向量由长度和方向决定，长度和方向不变的情况下随意移动，表示的都是同一个向量。</li><li>计算机<br> 计算机中的向量更多的是对数据的抽象，可以是根据面积和价格定义的一个房子特征$\begin{bmatrix}100m^2\\700000￥\end{bmatrix}$或是通过神经网络得到的图象的的一个向量。</li><li>数学<br> 数学中的向量可以是任意东西，只要保证两个向量的相加$\vec v + \vec w$以及数字和向量相乘$2\vec v$是有意义的即可。<span id="more"></span></li></ul></li><li>线性代数中的向量可以理解为一个空间中的箭头，这个箭头起点落在原点。如果空间中有许多的向量，可以用点表示一个向量，即向量头的坐标。</li></ol><h1 id="二-向量的基本运算"><a href="#二-向量的基本运算" class="headerlink" title="二. 向量的基本运算"></a>二. 向量的基本运算</h1><ol><li><p>向量的加法<br> 可以理解为在坐标系中两个向量的移动。</p> <p align="center">             <img src="https://jpccc.github.io/resource/algebra/001.png"> </p></li><li><p>向量的乘法</p> <p align="center">             <img src="https://jpccc.github.io/resource/algebra/002.png"> </p></li></ol><h1 id="三-线性组合、张成空间、基"><a href="#三-线性组合、张成空间、基" class="headerlink" title="三. 线性组合、张成空间、基"></a>三. 线性组合、张成空间、基</h1><blockquote><p> 线性组合</p></blockquote><p>两个数乘向量相加称为两个向量的线性组合$a\vec v+ b\vec w$。<br>两个不共线的向量通过不同的线性组合可以得到二维平面中的所有向量。<br>两个共线的向量通过线程组合只能得到一个直线的所有向量。<br>如果两个向量都是零向量那么它只能在原点。</p><blockquote><p>张成向量</p></blockquote><p>所有可以表示给定向量线性组合的向量的集合称为给定向量的张成空间（span）。<br>一般来说两个向量张成空间可以是直线、平面。<br>三个向量张成空间可以是平面、空间。<br>如果多个向量，并且可以移除其中一个而不减小张成空间，那么它们是线性相关的，也可以说一个向量可以表示为其他向量的线性组合$\vec u = a \vec v + b\vec w$。<br>如果所有的向量都给张成的空间增加了新的维度，它们就成为线性无关的$\vec u \neq a \vec v + b\vec w$。</p><blockquote><p>基</p></blockquote><p>向量空间的一组基是张成该空间的一个线性无关向量集。</p><h1 id="四-矩阵与线性变换"><a href="#四-矩阵与线性变换" class="headerlink" title="四. 矩阵与线性变换"></a>四. 矩阵与线性变换</h1><p>(向量的基默认为(0,1)(1,0)正交基。左乘向量可以看作是向量对基向量进行操作。同一向量乘以不同的基表示对不同的基做相同的运算。)</p><p>严格意义上来说，线性变换是将向量作为输入和输出的一类函数。<br>变化可以多种多样，线性变化将变化限制在一个特殊类型的变换上，可以简单的理解为网格线保持平行且等距分布。<br>线性变化满足一下两个性质：</p><ul><li>线性变化前后直线依旧是直线不能弯曲。</li><li>原点必须保持固定</li></ul><p align="center">    <img src="https://jpccc.github.io/resource/algebra/003.png"></p><p>可以使用<strong>基向量来描述线性变化：</strong><br>通过记录两个基向量$\hat{i}$,$\hat{j}$的变换，就可以得到其他变化后的向量。<br>已知向量$\vec v = \begin{bmatrix}-1\\2\end{bmatrix}$<br>变换之前的$\hat i$和$\hat j$：</p><p>$$<br>\hat{i} = \begin{bmatrix}<br>1 \<br>   0<br>  \end{bmatrix} \<br>  \hat{j} = \begin{bmatrix}<br>0 \<br>   1<br>  \end{bmatrix} \<br>\vec{v} = -1\hat{i} + 2 \hat{j} = \begin{bmatrix}<br>-1 \<br>   2<br>  \end{bmatrix}<br>$$</p><p>变换之后的$\hat i$和$\hat j$：</p><p>$$<br>\begin{aligned}<br>\hat{i} = \begin{bmatrix}<br>1 \<br>   -2<br>  \end{bmatrix} \<br>  \hat{j} = \begin{bmatrix}<br>3 \<br>   0<br>  \end{bmatrix} \<br>\vec{v} = -1\hat{i} + 2 \hat{j} &amp;= \begin{bmatrix}<br>5 \<br>   2<br>  \end{bmatrix} \<br>  &amp;= -1\begin{bmatrix}<br>1 \<br>   -2<br>  \end{bmatrix}  + 2 \begin{bmatrix}<br>3 \<br>  0<br>  \end{bmatrix} \<br>  &amp;= \begin{bmatrix}<br>1 &amp; 3 \<br>   -2 &amp; 0<br>  \end{bmatrix}  \begin{bmatrix}<br>-1 \<br>   2<br>  \end{bmatrix} \<br>  &amp;= \begin{bmatrix}<br>5 \<br>   2<br>  \end{bmatrix} \<br>\end{aligned}<br>$$<br><strong>将对基向量的变换记录下来，对其作用于其它向量，就可以得到其它向量在变换后的空间中的值</strong></p><p>我们可以将变换后的$\hat i$和$\hat j$写成矩阵的形式：$\begin{bmatrix}<br>1 &amp; 3 \<br>   -2 &amp; 0<br>  \end{bmatrix}  \begin{bmatrix}<br>-1 \<br>   2<br>  \end{bmatrix}$，通过矩阵的乘法得到变化后的向量。(左侧矩阵是基向量的线性变换矩阵)</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/004.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/005.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/006.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/007.png"></p><p>如果变化后的$\hat{i}$和$\hat{j}$是线性相关的，变化后向量的张量就是一维空间：</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/008.png"></p><h1 id="五-矩阵乘法与线性变换复合的联系"><a href="#五-矩阵乘法与线性变换复合的联系" class="headerlink" title="五. 矩阵乘法与线性变换复合的联系"></a>五. 矩阵乘法与线性变换复合的联系</h1><blockquote><p>线性变化的复合</p></blockquote><p>如何描述先旋转再剪切的操作呢？</p><p>一个通俗的方法是首先左乘旋转矩阵然后左乘剪切矩阵。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/009.png"></p>   <p>两个矩阵的乘积需要从右向左读，类似函数的复合。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/010.png"></p><p>这样两个矩阵的乘积就对应了一个复合的线性变换，最终得到对应变换后的$\hat{i}$和$\hat{j}$</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/011.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/012.png"></p>这一过程具有普适性：<p align="center">            <img src="https://jpccc.github.io/resource/algebra/013.png"></p><blockquote><p>矩阵乘法的顺序</p></blockquote><p align="center">            <img src="https://jpccc.github.io/resource/algebra/014.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/015.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/016.png"></p><blockquote><p>如何证明矩阵乘法的结合性？</p></blockquote><p>$(AB)C = A(BC)$<br>根据线性变化我们可以得出，矩阵的乘法都是以CBA的顺序变换得到，所以他们本质上相同，通过变化的形式解释比代数计算更加容易理解。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/017.png"></p><h1 id="六-三维空间的线性变化"><a href="#六-三维空间的线性变化" class="headerlink" title="六. 三维空间的线性变化"></a>六. 三维空间的线性变化</h1><p>三维的空间变化和二维的类似。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/018.png"></p><p>同样跟踪基向量的变换，能很好的解释变换后的向量，同样两个矩阵相乘的复合变换也是。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/019.png"></p><h1 id="七-行列式"><a href="#七-行列式" class="headerlink" title="七. 行列式"></a>七. 行列式</h1><blockquote><p>行列式的本质</p></blockquote><p>行列式的本质是计算线性变化对空间的缩放比例，具体一点就是，测量一个给定区域面积增大或减小的比例。<strong>注意，面积的变化比例是对原空间中的网格来说的。也可以说成是原基向量组成的区域面积的变化。</strong><br>单位面积的变换代表任意区域的面积变换比例。<br>值得注意的是：</p><ul><li>如果一个二维线性变换的行列式为0，说明其将说明起其将整个平面压缩成一条线甚至一个点上。</li><li>所以只要检测一个矩阵的行列式是否为0，我们就可以知道矩阵所代表的变换是否将空间压缩到更小的维度上。</li></ul><p align="center">            <img src="https://jpccc.github.io/resource/algebra/020.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/021.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/022.png"></p>行列式的值表示缩放比例。<p align="center">            <img src="https://jpccc.github.io/resource/algebra/023.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/024.png"></p><p>行列式为什么有负值呢？可以从两个角度考虑：</p><ul><li>一是变换将平面进行了反转。就好像将一张纸的正面通过变换将这张纸进行了翻面。</li><li>二是考虑的$\hat i$和$\hat j$的相对位置。如$\hat i$在$\hat j$的左边，通过变换将$\hat i$变换到了$\hat j$的右边，那么这个变换所对应矩阵的行列式值为负。<p align="center">          <img src="https://jpccc.github.io/resource/algebra/001.gif"></p></li></ul><p>三维空间的行列式类似，它的单位是一个单位1的立方体。</p><p>三位空间的线性变换，可以使用右手定则判断三维空间的定向。如果变换前后都可以通过右手定则得到，那么他的行列式就是正值，否则为负值.</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/025.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/026.png"></p><h1 id="八-逆矩阵、列空间、秩、零空间"><a href="#八-逆矩阵、列空间、秩、零空间" class="headerlink" title="八. 逆矩阵、列空间、秩、零空间"></a>八. 逆矩阵、列空间、秩、零空间</h1><blockquote><p>线性方程组</p></blockquote><p align="center">            <img src="https://jpccc.github.io/resource/algebra/027.png"></p><p>从几何的角度来思考，矩阵A表示一个线性变换，我们需要找到一个$\vec x$使得它在变换后和$\vec v$重合。</p><blockquote><p>逆矩阵</p></blockquote><p>矩阵的逆运算，记为$\vec A = \begin{bmatrix}3&amp;1 \0&amp;2\end{bmatrix}^{-1}$，对于线程方程$A \vec x = \vec v $来说，找到$A^{-1}$就得到解$\vec x = A^{-1} \vec v$。<br>$A^{-1}A=\begin{bmatrix}1&amp;0 \0&amp;1\end{bmatrix}$，什么都不做称为恒等变换。</p><blockquote><p>线性方程组的解</p></blockquote><p>对于方程组$A\vec x = \vec v$，线性变换A存在两种情况：</p><ul><li>$det(A) \neq0$：这时空间的维数并没有改变，有且只有一个向量经过线性变换后和$\vec v$重合。</li><li>$det(A) =0$：空间被压缩到更低的维度，这时不存在逆变换。因为不能将一个直线解压缩为一个平面(这会要求将一个单独的向量变换为一整条线的向量，函数多对一可以，一对多不行，即存在一个矩阵A将多个向量映射到一个点，但不可能存在一个矩阵A将一个点映射成一条线的向量)。但是即使不存在逆变换，解可能仍然存在，这时候目标$\vec v$必须刚好落在压缩后的空间上。(例如一个变换将空间压缩成了一条直线，而向量$\vec v$恰好在这条直线上。共线的情况下，则有无穷解，因为有无穷都被压缩到直线上的每一点)。</li></ul><blockquote><p>秩</p></blockquote><p>秩代表变换后空间的维度。<br>如果线性变化后将空间压缩成一条直线，那么称这个变化的秩为1；<br>如果线性变化后向量落在二维平面，那么称这个变化的秩为2。</p><blockquote><p>列空间</p></blockquote><p>所有可能的输出向量$A\vec v$构成的集合(A为基向量的集合，对其进行任意的组合(组合即乘以$\vec{v}$)，所得到的所有向量)，称为列空间，即所有列向量张成的空间。</p><ul><li>更精确的秩的定义就是列空间的维数。</li></ul><blockquote><p>零空间（Null space）</p></blockquote><p>所有的线性变化中，零向量一定包含在列空间中，因为线性变换原点保持不动。对于非满秩的情况来说，会有一系列的向量在变换后仍为零向量（二维空间压缩为一条直线，一条线上的向量都会落到原点。）</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/002.gif"></p><p>三维空间压缩为二维平面，一条线上的向量都会落到原点。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/003.gif"></p><p>三维空间压缩为一条直线，整个平面上的向量都会落到原点。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/004.gif"></p><p>当$A\vec x = \vec v$中的$\vec v$是一个零向量，即$A\vec x = \begin{bmatrix}0 \\0\end{bmatrix}$时，零空间就是它所有可能的解。</p><h1 id="非方阵、不同维度空间之间的线性变换"><a href="#非方阵、不同维度空间之间的线性变换" class="headerlink" title="非方阵、不同维度空间之间的线性变换"></a>非方阵、不同维度空间之间的线性变换</h1><p>不同维度的变换也是存在的。</p><p>一个$3\times2$的矩阵：$\begin{bmatrix}2&amp;0\\-1&amp;1\\-2&amp;1 \end{bmatrix}$它的几何意义是将一个二维空间映射到三维空间上，矩阵有两列表明输入空间有两个基向量，有三行表示每个向量在变换后用三个独立的坐标描述。(A是满秩的，因为其与输入空间x的维度相等)</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/028.png"></p><p>一个$2\times 3$的矩阵：$\begin{bmatrix}3&amp;1&amp;4\\1&amp;5&amp;9 \end{bmatrix}$则表示将一个三维空间映射到二维空间上。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/029.png"></p><p>一个$1\times 2$的矩阵：$\begin{bmatrix}1&amp;2 \end{bmatrix}$表示一个二维空间映射到一维空间。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/030.png"></p>(还可以从解方程的角度，通解+特解，来理解一维变二维。)# 点积与对偶性> 点积<p>对于两个维度相同的向量，他们的点积计算为：$\begin{bmatrix}1\\2\end{bmatrix}\cdot\begin{bmatrix} 3\\4\end{bmatrix}=1\cdot3+2\cdot4=11$。<br>点积的几何解释是将一个向量向一个向量投影，然后两个长度相乘，如果为负数则表示反向。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/031.png"></p><p>为什么点积和坐标相乘联系起来了？这和对偶性有关。</p><blockquote><p>对偶性</p></blockquote><p>对偶性的思想是：<strong>每当看到一个多维空间到数轴上的线性变换时</strong>，他都与空间中的唯一一个向量对应，也就是说使用线性变换和与这个向量点乘等价。这个向量也叫做线性变换的对偶向量。<br>当二维空间向一维空间映射时，如果在二维空间中等距分布的点在变换后还是等距分布的，那么这种变换就是线性的。</p><p>假设有一个线性变换A<br>$\begin{bmatrix}1&amp;-2\end{bmatrix}$<br>和一个向量<br>$\vec v=\begin{bmatrix} 4\\3 \end{bmatrix}$。</p><p>变换后的位置为$\begin{bmatrix}1&amp;-2\end{bmatrix}\begin{bmatrix}4\3\end{bmatrix}=4\cdot1+3\cdot-2=-2$，这个变换是一个二维空间向一维空间的变化，所以变换后的结果为一个坐标值。<br>我们可以看到线性变换的计算过程和向量的点积相同$\begin{bmatrix}1\-2\end{bmatrix}\cdot\begin{bmatrix}4\3\end{bmatrix}=4\cdot1+3\cdot-2=-2$，所以向量和一个线性变化有着微妙的联系。<br>假设有一个倾斜的数轴，上面有一个单位向量$\vec v$，对于任意一个向量它在数轴上的投影都是一个数字，这表示了一个二维向量到一位空间的一种线性变换，那么如何得到这个线性变化呢？</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/032.png">        </p><p>由之前的内容来说，我们可以观察基向量$\vec i$和$\vec j$的变化，从而得到对应的线性变化。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/033.png">        </p><p>因为$\vec i$、$\vec j$、$\vec u$都是单位向量，根据对称性可以得到$\vec i$和$\vec j$在$\vec u$上的投影长度刚好是$\vec u$的坐标。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/034.png">        </p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/035.png">        </p><p>这样空间中的所有向量都可以通过线性变化<br>$\begin{bmatrix} u_x&amp;u_y \end{bmatrix}$<br>得到，而这个计算过程刚好和单位向量的点积相同。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/036.png"></p><p>也就是为什么向量投影到直线的长度，刚好等于它与直线上单位向量的点积，对于非单位向量也是类似，只是将其扩大到对应倍数。</p><h1 id="叉积"><a href="#叉积" class="headerlink" title="叉积"></a>叉积</h1><p>对于两个向量所围成的面积来说，可以使用行列式计算，将两个向量看作是变换后的基向量，这样通过行列式就可以得到变换后面积缩放的比例，因为基向量的单位为1，所以就得到了对应的面积。<br>考虑到正向，这个面积的值存在负值，这是参照基向量$\vec i$和$\vec j$的相对位置来说的。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/037.png">        </p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/038.png">        </p><p>真正的叉积是通过两个三维向量$\vec v$和$\vec w$，生成一个新的三维向量$\vec u$，这个向量垂直于向量$\vec v$和$\vec w$所在的平面，长度等于它们围成的面积。<br>叉积的反向可以通过右手定则判断：</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/039.png">        </p>叉积的计算方法：<p align="center">            <img src="https://jpccc.github.io/resource/algebra/040.png">        </p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/041.png">        </p><h1 id="线性代数看叉积"><a href="#线性代数看叉积" class="headerlink" title="线性代数看叉积"></a>线性代数看叉积</h1><p>参考二维向量的叉积计算：</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/042.png"></p><p>三维的可以写成类似的形式，但是他并是真正的叉积，不过和真正的叉积已经很接近了。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/043.png"></p><p>我可以构造一个函数，它可以把一个三维空间映射到一维空间上。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/044.png"></p><p>右侧行列式是线性的，所以我们可以找到一个线性变换代替这个函数。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/045.png"></p><p>根据对偶性的思想，从多维空间到一维空间的线性变换，等于与对应向量的点积，这个特殊的向量$\vec p$就是我们要找的向量。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/046.png"></p><blockquote><p>从数值计算上:</p></blockquote><p align="center">            <img src="https://jpccc.github.io/resource/algebra/047.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/048.png"></p><p>向量$\vec p$的计算结果刚好和叉积计算的结果相同。</p><blockquote><p>从几何意义：</p></blockquote><p align="center">            <img src="https://jpccc.github.io/resource/algebra/049.png"></p><p>当向量$\vec p$和向量$\begin{bmatrix}x\\y\\z \end{bmatrix}$点乘时，得到一个$\begin{bmatrix}x\\y\\z \end{bmatrix}$与$\vec v$与$\vec w$确定的平行六面体的有向体积，什么样的向量满足这个性质呢？<br>点积的几何解释是，其他向量在$\vec p$上的投影的长度乘以$\vec p$的长度。<br>对于平行六面体的体积来说，它等于$\vec v$和$\vec w$所确定的面积乘以$\begin{bmatrix}x\\y\\z \end{bmatrix}$在垂线上的投影。<br>那么$\vec p$要想满足这一要求，那么它就刚好符合，长度等于$\vec v,\vec w$所围成的面积，且刚好垂直这个平面。</p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/050.png"></p><p align="center">            <img src="https://jpccc.github.io/resource/algebra/051.png"></p><h1 id="基变换"><a href="#基变换" class="headerlink" title="基变换"></a>基变换</h1><p>标准坐标系的基向量为$\vec {i}: \begin{bmatrix}1\0 \end{bmatrix}$和$\vec {j}: \begin{bmatrix}0\1 \end{bmatrix}$，假如詹妮弗有另一个坐标系：她的基向量为$\vec i \begin{bmatrix}2\1 \end{bmatrix}$和$\vec j \begin{bmatrix}-1\1 \end{bmatrix}$。<br>对于同一个点$\begin{bmatrix}3\2 \end{bmatrix}$来说他们所表示的形式不同，在詹妮弗的坐标系中表示为$\begin{bmatrix}\frac{5}{3}\\frac{1}{3} \end{bmatrix}$。（在不同基向量下，坐标同基相乘的结果是一样的。）<br>{詹尼弗的坐标乘以其基向量的结果(向量)是在我们坐标系中的表示。，即$\begin{bmatrix}3\2 \end{bmatrix}$}<br>从标准坐标到詹尼佛的坐标系，我能可以得到一个线性变换$A:\begin{bmatrix}2&amp;-1\1&amp;1 \end{bmatrix}$。（这个变换将詹尼佛的0，1变成我们语言表示的詹尼佛的0，1）</p><p>如果想知道詹妮弗的坐标系中点$\begin{bmatrix}3\2 \end{bmatrix}$在标准坐标系的位置，可以通过$\begin{bmatrix}2&amp;-1\1&amp;1 \end{bmatrix}\begin{bmatrix}3\2 \end{bmatrix}$得到。（基是我们的语言表示，而坐标是詹妮弗中的坐标，那么在我们的空间网格中，詹妮弗的坐标系中点$\begin{bmatrix}3\2 \end{bmatrix}$所代表的向量在我们的坐标系中的坐标为$\begin{bmatrix}4\5 \end{bmatrix}$）</p><p>如果想知道标准坐标系中点$\begin{bmatrix}3\2 \end{bmatrix}$在詹妮弗坐标系的位置，可以通过$\begin{bmatrix}2&amp;-1\1&amp;1 \end{bmatrix}^{-1}\begin{bmatrix}3\2 \end{bmatrix}$得到。<br>具体的例子，90°旋转。<br>在标准坐标系可以跟踪基向量的变化来体现：</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;一-向量&quot;&gt;&lt;a href=&quot;#一-向量&quot; class=&quot;headerlink&quot; title=&quot;一. 向量&quot;&gt;&lt;/a&gt;一. 向量&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;向量对于不同的学科有不一样的定义，我们将从三个角度对向量进行解释。&lt;ul&gt;
&lt;li&gt;物理学&lt;br&gt; 物理中的向量由长度和方向决定，长度和方向不变的情况下随意移动，表示的都是同一个向量。&lt;/li&gt;
&lt;li&gt;计算机&lt;br&gt; 计算机中的向量更多的是对数据的抽象，可以是根据面积和价格定义的一个房子特征$\begin{bmatrix}100m^2\\700000￥\end{bmatrix}$或是通过神经网络得到的图象的的一个向量。&lt;/li&gt;
&lt;li&gt;数学&lt;br&gt; 数学中的向量可以是任意东西，只要保证两个向量的相加$\vec v + \vec w$以及数字和向量相乘$2\vec v$是有意义的即可。</summary>
    
    
    
    
    <category term="math" scheme="https://jpccc.github.io/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://jpccc.github.io/2021/10/17/hello-world/"/>
    <id>https://jpccc.github.io/2021/10/17/hello-world/</id>
    <published>2021-10-17T07:02:13.391Z</published>
    <updated>2022-05-12T08:24:47.124Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
